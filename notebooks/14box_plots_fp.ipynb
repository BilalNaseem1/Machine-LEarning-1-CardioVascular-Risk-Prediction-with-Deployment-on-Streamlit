{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-XnX66Kfsdo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "from scipy.stats import ttest_1samp, shapiro\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "from sklearn.metrics import recall_score, make_scorer, roc_auc_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "from graphviz import Source\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn import tree\n",
        "from IPython.display import SVG, display\n",
        "# import shap\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from scipy.stats.mstats import winsorize\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "# from pycaret.classification import *\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import recall_score\n",
        "import six\n",
        "import sys\n",
        "sys.modules['sklearn.externals.six'] = six\n",
        "# from skopt.space import Real, Integer, Categorical\n",
        "# from skopt import gp_minimize, space\n",
        "from functools import partial\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import numpy as np\n",
        "# from functools import partial\n",
        "# from skopt import gp_minimize\n",
        "# from skopt import space\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from mlxtend.classifier import EnsembleVoteClassifier\n",
        "from mlxtend.classifier import EnsembleVoteClassifier\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import numpy as np\n",
        "# from functools import partial\n",
        "# from skopt import gp_minimize\n",
        "# from skopt import space\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.classifier import EnsembleVoteClassifier\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.metrics import roc_curve, auc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/sample_data/Best_dataset_fp.csv')\n",
        "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7U0GpcsZfz48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df.drop(['TenYearCHD'], axis = 1)\n",
        "y = train_df['TenYearCHD']\n",
        "X = X.reset_index(drop=True)\n",
        "y = y.reset_index(drop=True)\n",
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']"
      ],
      "metadata": {
        "id": "tkK_AYNkgANd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.classifier import StackingClassifier\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from mlxtend.classifier import StackingCVClassifier\n",
        "\n",
        "\n",
        "\n",
        "# Define the wrapper class for Keras models\n",
        "class KerasWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(X, y)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        predictions = self.model.predict(X)\n",
        "        proba = predictions.flatten()\n",
        "        proba = np.vstack([1 - proba, proba]).T\n",
        "        return proba\n",
        "\n",
        "# Define the base models\n",
        "optimized_model = Sequential()\n",
        "optimized_model.add(Dense(units=298, activation='relu', input_dim=X.shape[1]))\n",
        "optimized_model.add(Dense(units=1, activation='sigmoid'))\n",
        "optimizer = Adam(learning_rate=0.007119418600172993)\n",
        "optimized_model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "\n",
        "neural_network_model = KerasWrapper(optimized_model)\n",
        "\n",
        "xgboost_model = XGBClassifier(max_depth = 4, n_estimators = 399, learning_rate = 0.14376998191774137,\n",
        "                              subsample = 0.646680904808694, colsample_bytree = 0.6279281044874794, min_child_weight = 7.707299055505381)\n",
        "\n",
        "gb_model = GradientBoostingClassifier(learning_rate = 0.024998785901829815, max_depth = 5, subsample = 0.6593086507635105,\n",
        "                          max_features = 0.5352246704995934, n_estimators = 62, min_samples_split = 34, min_samples_leaf = 11)\n",
        "\n",
        "random_forest_model = RandomForestClassifier(n_estimators=39, max_depth=6, criterion='entropy')\n",
        "lr_model = LogisticRegression(C = 100.0, penalty = 'l2', solver = 'saga')\n",
        "\n",
        "# Define meta model\n",
        "meta_model = LogisticRegression()\n",
        "\n",
        "# Define stratified k-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define stacking classifier with base models and meta model\n",
        "stacking_clf_xgb4 = StackingCVClassifier(classifiers = [neural_network_model, xgboost_model, gb_model, random_forest_model, lr_model],\n",
        "                                    meta_classifier=meta_model,\n",
        "                                    cv=5,\n",
        "                                    stratify=True,\n",
        "                                    shuffle=True,\n",
        "                                    use_probas=True,\n",
        "                                    use_features_in_secondary=True,\n",
        "                                    verbose=2)\n",
        "\n",
        "# Define metric as ROC\n",
        "metric = roc_auc_score\n",
        "\n",
        "# Fit the stacking classifier\n",
        "stacking_clf_xgb4.fit(X.values, y.values)\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "y_pred = stacking_clf_xgb4.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUC\n",
        "auprc = average_precision_score(y_test, y_pred)\n",
        "print(\"AUPRC:\", auprc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nChZv4cPgErX",
        "outputId": "4e8df604-c473-4d24-9739-4d77bd33fc9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f011042d7b0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "67/67 [==============================] - 1s 2ms/step - loss: 0.4355\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3881\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3912\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3925\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3838\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.3831\n",
            "21/21 [==============================] - 0s 2ms/step\n",
            "AUPRC: 0.31593226195469837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# params = {'n_estimators': 39, 'max_depth': 6, 'criterion': 'entropy'}\n",
        "\n",
        "# # Define the KNeighborsClassifier model\n",
        "# xgb_model = RandomForestClassifier(**params)\n",
        "\n",
        "# Define the RepeatedStratifiedKFold cross-validator\n",
        "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
        "\n",
        "# List to store the roc auc scores\n",
        "roc_auc_scores = []\n",
        "\n",
        "# Loop through each cross-validation fold\n",
        "for train_idx, test_idx in cv.split(X, y):\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
        "    \n",
        "    # Fit the KNeighborsClassifier model to the training data\n",
        "    stacking_clf_xgb4.fit(X_train.values, y_train.values)\n",
        "    \n",
        "    # Predict probabilities for the positive class on the test data\n",
        "    model_probs = stacking_clf_xgb4.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate ROC AUC score\n",
        "    roc_auc = average_precision_score(y_test, model_probs)\n",
        "    \n",
        "    # Append ROC AUC score to the list\n",
        "    roc_auc_scores.append(roc_auc)\n",
        "\n",
        "# Calculate the average ROC AUC score across all folds\n",
        "avg_roc_auc = sum(roc_auc_scores) / len(roc_auc_scores)\n",
        "\n",
        "print(\"PR AUC scores:\", roc_auc_scores)\n",
        "print(\"Average PR AUC score:\", avg_roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBG1UbVcgQJU",
        "outputId": "c6c43c95-738d-4c6b-cd44-606ff2f5293b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f0110c93640>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 1s 4ms/step - loss: 0.4417\n",
            "14/14 [==============================] - 0s 5ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.4028\n",
            "14/14 [==============================] - 0s 6ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 4ms/step - loss: 0.3881\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3877\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3806\n",
            "14/14 [==============================] - 0s 1ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3832\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f0101b6ff70>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4546\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4125\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3972\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3906\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3921\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3901\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f01102e8070>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4536\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4055\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3956\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3838\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3836\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3838\n",
            "17/17 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f0110d0ead0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4475\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3933\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3942\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3880\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3853\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3840\n",
            "17/17 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f01012d64a0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4396\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4005\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3995\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3905\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3932\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3857\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f010116dcc0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4516\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4032\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4022\n",
            "14/14 [==============================] - 0s 1ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3921\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3864\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3882\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f010116db40>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4533\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4070\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3865\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3798\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3995\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3889\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fafbd690>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4495\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3962\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3831\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3846\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3791\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3786\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa64da50>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4575\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4040\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3919\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3937\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3882\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3896\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa50b2b0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4590\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4087\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3947\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3883\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3818\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3843\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa5bcd30>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4461\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3995\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3862\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3864\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3816\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3860\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa5bee00>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4509\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4094\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3890\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3888\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3866\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3851\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa27bbb0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4694\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4013\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3911\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3865\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3876\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3843\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa2d8c40>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4434\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4000\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3978\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3920\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3973\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3918\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00f9ff72b0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4457\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3989\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3918\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3830\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3840\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3803\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa0b2980>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4454\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4022\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3909\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3937\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 4ms/step - loss: 0.3868\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3818\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00ebe11150>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.4537\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3996\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3915\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3935\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3825\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3820\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00ebdff820>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4456\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3988\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3911\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3968\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3854\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3852\n",
            "17/17 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00eadb22f0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4379\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3965\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3930\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3767\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3688\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3803\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00e84845e0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4543\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4107\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3940\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3917\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3938\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3882\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00e737d270>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4395\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4038\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3959\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3958\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3849\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3878\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00e5a9df30>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 2s 2ms/step - loss: 0.4762\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.4002\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 4ms/step - loss: 0.3879\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3832\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3883\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.3834\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa3fdde0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4461\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4042\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3811\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3879\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3860\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3796\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f01104dca60>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4594\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4050\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3920\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3873\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3854\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3871\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f0110c63850>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4580\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4015\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3921\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3912\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3877\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3885\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00e59dd2a0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4388\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3985\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3968\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3874\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3894\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3859\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00eacf7280>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4473\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4040\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3937\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3867\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3870\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3838\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00ebe2a5c0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4484\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4080\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3835\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3886\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3915\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3834\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00ebe2bcd0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4565\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4064\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3862\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3944\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3894\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3823\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa49d300>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4333\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3950\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3892\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3869\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3871\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3841\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa58a6e0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4412\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4003\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3848\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3818\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3803\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3799\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f0101199c30>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4460\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4007\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3955\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3950\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3884\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3862\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f0101bdb0d0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4501\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4037\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3917\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3895\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3875\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3865\n",
            "17/17 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f0110d2b370>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4635\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4038\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3942\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3823\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3907\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3882\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f0101bfb190>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4528\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3935\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3808\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3910\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3872\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3834\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f01102740d0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4495\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.4008\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3904\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3891\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3871\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3869\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f010117edd0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4423\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4014\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3902\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3827\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3838\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3824\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa467910>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4658\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3992\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3932\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3951\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3859\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3940\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa263d90>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4546\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4017\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3882\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3896\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3922\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3916\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa178400>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4452\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3894\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3900\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3836\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3816\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3795\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa17b2e0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4509\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4000\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3917\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3910\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3898\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3837\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00ebe1c250>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4475\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3973\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3894\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3899\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3856\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3801\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00e49a5a80>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4581\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4030\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3969\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 4ms/step - loss: 0.3888\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 4ms/step - loss: 0.3875\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3856\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f01012690f0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4540\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4018\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3792\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3855\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3887\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3812\n",
            "17/17 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00fa60a770>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4471\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3991\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3902\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3915\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3983\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3836\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00e844af80>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4444\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 4ms/step - loss: 0.3990\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 4ms/step - loss: 0.3943\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3847\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3851\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3861\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00edf76aa0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4484\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4062\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3900\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3879\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3802\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3827\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00edf77df0>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4501\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3915\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3854\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3871\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3883\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3810\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f00faefcb20>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4626\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4079\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4096\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3957\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 4ms/step - loss: 0.3890\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3916\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7f01012c1930>)\n",
            "Training and fitting fold 1 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4446\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.4082\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.3914\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3881\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.3854\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.6279281044874794, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.14376998191774137,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.707299055505381, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=399, n_jobs=None,\n",
            "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: gradientboostingclassifier (3/5)\n",
            "GradientBoostingClassifier(learning_rate=0.024998785901829815, max_depth=5,\n",
            "                           max_features=0.5352246704995934, min_samples_leaf=11,\n",
            "                           min_samples_split=34, n_estimators=62,\n",
            "                           subsample=0.6593086507635105)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=39)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=100.0, solver='saga')\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3790\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "PR AUC scores: [0.30828978069765367, 0.343798168302532, 0.3024990906503224, 0.37244324439592175, 0.3621060082745221, 0.3425173189704659, 0.3127579198901261, 0.29986619408057935, 0.3463951319013106, 0.36500482077668606, 0.28929539145182925, 0.301058334552206, 0.32098378336043165, 0.4119009424944663, 0.32387770287610806, 0.3617290208761753, 0.3650543504932151, 0.3410275420263037, 0.2776267533351995, 0.3421799223931475, 0.38316467223291417, 0.28279800080582157, 0.308678076620574, 0.367095089053494, 0.3212540629120771, 0.3554017057456056, 0.3352674243264728, 0.27504849945547355, 0.344531367537768, 0.3133587050705793, 0.3207262108710167, 0.34249867829291253, 0.33822082747321663, 0.3351299515696396, 0.2969763426522304, 0.3842545269114456, 0.29083871513175646, 0.3231439499667997, 0.3889253426872323, 0.3162829832155528, 0.32388126858928357, 0.32055482200037094, 0.35022520328595164, 0.3170043716048848, 0.3800600691292034, 0.38414906675811017, 0.30048979831673905, 0.30603348435800537, 0.3973794775606681, 0.2945457085793086]\n",
            "Average PR AUC score: 0.33376659649028634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Define the wrapper class for Keras models\n",
        "class KerasWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(X, y)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        predictions = self.model.predict(X)\n",
        "        proba = predictions.flatten()\n",
        "        proba = np.vstack([1 - proba, proba]).T\n",
        "        return proba\n",
        "\n",
        "# Define the base models\n",
        "# Define the base models\n",
        "optimized_model = Sequential()\n",
        "optimized_model.add(Dense(units=298, activation='relu', input_dim=X.shape[1]))\n",
        "optimized_model.add(Dense(units=1, activation='sigmoid'))\n",
        "optimizer = Adam(learning_rate=0.007119418600172993)\n",
        "optimized_model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "\n",
        "neural_network_model = KerasWrapper(optimized_model)\n",
        "\n",
        "xgboost_model = XGBClassifier(max_depth = 4, n_estimators = 399, learning_rate = 0.14376998191774137,\n",
        "                              subsample = 0.646680904808694, colsample_bytree = 0.6279281044874794, min_child_weight = 7.707299055505381)\n",
        "\n",
        "gb_model = GradientBoostingClassifier(learning_rate = 0.024998785901829815, max_depth = 5, subsample = 0.6593086507635105,\n",
        "                          max_features = 0.5352246704995934, n_estimators = 62, min_samples_split = 34, min_samples_leaf = 11)\n",
        "\n",
        "random_forest_model = RandomForestClassifier(n_estimators=39, max_depth=6, criterion='entropy')\n",
        "lr_model = LogisticRegression(C = 100.0, penalty = 'l2', solver = 'saga')\n",
        "\n",
        "\n",
        "# # set the weights for each classifier\n",
        "# weights = [0.99, 0.993, 1.004, 1.006, 1.007]\n",
        "weights = [1.5, 1, 1, 1, 1.5] # 0.318\n",
        "# weights = [1.3, 0.6, 1.4, 0.6, 1.4]\n",
        "\n",
        "\n",
        "classifiers = [neural_network_model, xgboost_model, gb_model, random_forest_model, lr_model]\n",
        "\n",
        "# create the ensemble classifier\n",
        "ensemble_SVW_O = EnsembleVoteClassifier(clfs=classifiers, voting='soft', weights = weights, verbose=1)\n",
        "\n",
        "# fit the ensemble classifier on the training data\n",
        "ensemble_SVW_O.fit(X, y)\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ensemble_SVW_O.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUC\n",
        "auprc = average_precision_score(y_test, y_pred)\n",
        "print(\"AUPRC:\", auprc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFcuZrMIgiai",
        "outputId": "ef31d561-b171-493e-f4c5-07bc760f30f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "84/84 [==============================] - 1s 2ms/step - loss: 0.4395\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "21/21 [==============================] - 0s 2ms/step\n",
            "AUPRC: 0.3112937899728293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# params = {'n_estimators': 39, 'max_depth': 6, 'criterion': 'entropy'}\n",
        "\n",
        "# # Define the KNeighborsClassifier model\n",
        "# xgb_model = RandomForestClassifier(**params)\n",
        "\n",
        "# Define the RepeatedStratifiedKFold cross-validator\n",
        "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
        "\n",
        "# List to store the roc auc scores\n",
        "roc_auc_scores = []\n",
        "\n",
        "# Loop through each cross-validation fold\n",
        "for train_idx, test_idx in cv.split(X, y):\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
        "    \n",
        "    # Fit the KNeighborsClassifier model to the training data\n",
        "    ensemble_SVW_O.fit(X_train.values, y_train.values)\n",
        "    \n",
        "    # Predict probabilities for the positive class on the test data\n",
        "    model_probs = ensemble_SVW_O.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate ROC AUC score\n",
        "    roc_auc = average_precision_score(y_test, model_probs)\n",
        "    \n",
        "    # Append ROC AUC score to the list\n",
        "    roc_auc_scores.append(roc_auc)\n",
        "\n",
        "# Calculate the average ROC AUC score across all folds\n",
        "avg_roc_auc = sum(roc_auc_scores) / len(roc_auc_scores)\n",
        "\n",
        "print(\"PR AUC scores:\", roc_auc_scores)\n",
        "print(\"Average PR AUC score:\", avg_roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni5DzqbQiroQ",
        "outputId": "8d5728d3-df2d-46f0-d4ea-58b3884bf12e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 2ms/step - loss: 0.4465\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 2ms/step - loss: 0.4338\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4402\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4366\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 2ms/step - loss: 0.4451\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4426\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 3ms/step - loss: 0.4390\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4489\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4354\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 2ms/step - loss: 0.4314\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4326\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4497\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 3ms/step - loss: 0.4374\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 2ms/step - loss: 0.4343\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4404\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4521\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4492\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4464\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 4ms/step - loss: 0.4342\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 2ms/step - loss: 0.4364\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 3ms/step - loss: 0.4442\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 2ms/step - loss: 0.4423\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 2ms/step - loss: 0.4293\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 2ms/step - loss: 0.4421\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 2ms/step - loss: 0.4549\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 2ms/step - loss: 0.4424\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4466\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 3ms/step - loss: 0.4391\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4406\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4418\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4367\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4415\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4503\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4404\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4422\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4337\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4434\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4405\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4315\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 3ms/step - loss: 0.4350\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 3ms/step - loss: 0.4447\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 6ms/step - loss: 0.4409\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4472\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4400\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4539\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 2ms/step - loss: 0.4405\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4573\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 3ms/step - loss: 0.4406\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 2s 3ms/step - loss: 0.4491\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "67/67 [==============================] - 1s 2ms/step - loss: 0.4414\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: gradientboostingclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "PR AUC scores: [0.28913093260615685, 0.3176392415776459, 0.33865640267979735, 0.38350516559564346, 0.3857002764741272, 0.39379700845431376, 0.31470323312356385, 0.2879355761533104, 0.33547206642472743, 0.3344013534856112, 0.2705129486604148, 0.28773323020229313, 0.3504469287760851, 0.4030543023766945, 0.32169239158211976, 0.3483622754206326, 0.3539517636180863, 0.32892435489692484, 0.28284584967135606, 0.3277611575624709, 0.37953559113300583, 0.30169382778315956, 0.2904659844348636, 0.3388783709726533, 0.3019426227684737, 0.34289550623237774, 0.3358715011723919, 0.303327666686701, 0.33280683340687056, 0.303804208534957, 0.3258949936627553, 0.3496926826396013, 0.3329609191267726, 0.3240683263723104, 0.31113223384292676, 0.3600743843944028, 0.28433189597397934, 0.32044593262653875, 0.3675332173052174, 0.300449168031108, 0.3531954176252953, 0.30274433958501346, 0.36102014915628533, 0.31076155770972214, 0.34933288806449797, 0.39377620827673354, 0.29555373770865767, 0.27163339396214164, 0.39132555760528165, 0.2841578656492323]\n",
            "Average PR AUC score: 0.3295506688357181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Assuming you have your training and evaluation data as X_train, y_train, X_eval, y_eval, and test data as X_test, y_test\n",
        "\n",
        "# Define the parameters\n",
        "params = {'learning_rate': 0.024998785901829815, 'max_depth': 5, 'subsample': 0.6593086507635105,\n",
        "          'max_features': 0.5352246704995934, 'n_estimators': 62, 'min_samples_split': 34, 'min_samples_leaf': 11}\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model = GradientBoostingClassifier(**params)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUC\n",
        "auprc = average_precision_score(y_test, y_pred)\n",
        "print(\"AUPRC:\", auprc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6R6boeYjUlP",
        "outputId": "12b8da9a-d890-4dbf-c95b-68dcfb6af4d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUPRC: 0.32135003263280887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# params = {'n_estimators': 39, 'max_depth': 6, 'criterion': 'entropy'}\n",
        "\n",
        "# # Define the KNeighborsClassifier model\n",
        "# xgb_model = RandomForestClassifier(**params)\n",
        "\n",
        "# Define the RepeatedStratifiedKFold cross-validator\n",
        "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
        "\n",
        "# List to store the roc auc scores\n",
        "roc_auc_scores = []\n",
        "\n",
        "# Loop through each cross-validation fold\n",
        "for train_idx, test_idx in cv.split(X, y):\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
        "    \n",
        "    # Fit the KNeighborsClassifier model to the training data\n",
        "    model.fit(X_train.values, y_train.values)\n",
        "    \n",
        "    # Predict probabilities for the positive class on the test data\n",
        "    model_probs = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate ROC AUC score\n",
        "    roc_auc = average_precision_score(y_test, model_probs)\n",
        "    \n",
        "    # Append ROC AUC score to the list\n",
        "    roc_auc_scores.append(roc_auc)\n",
        "\n",
        "# Calculate the average ROC AUC score across all folds\n",
        "avg_roc_auc = sum(roc_auc_scores) / len(roc_auc_scores)\n",
        "\n",
        "print(\"PR AUC scores:\", roc_auc_scores)\n",
        "print(\"Average PR AUC score:\", avg_roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HnWN-N9jUgw",
        "outputId": "3a84b893-88be-49c0-9aa7-d9345a91b42c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PR AUC scores: [0.27978480974743847, 0.28803176010421166, 0.33964565190370616, 0.3964323980514589, 0.34466250828463874, 0.33950958771381623, 0.34391780312756864, 0.262621457665221, 0.3288197358720021, 0.33822540613888186, 0.2662778120712578, 0.2760835723493187, 0.3367146073363239, 0.45001217890703077, 0.273839066035762, 0.32037877615255134, 0.35095539530864683, 0.2980041536755427, 0.2801692939127861, 0.3208265174885092, 0.36991112857280195, 0.30089751967719297, 0.27405683896174576, 0.3287502684267688, 0.29243508401421636, 0.31903299393412504, 0.3613333007071185, 0.2821320887455845, 0.2962773783691569, 0.2958148094669302, 0.2876159348531316, 0.35830727137751084, 0.2868711710242126, 0.31510627402514785, 0.3181949778551382, 0.36457694594408374, 0.28802476293332135, 0.3005076991744001, 0.35865079921937226, 0.2704934187308193, 0.31061118199611076, 0.2727124144648201, 0.32336441771145813, 0.31226537545229993, 0.32931232876511635, 0.3762192059531769, 0.2764638898713364, 0.28537190143066743, 0.36617914703496196, 0.2723663108633346]\n",
            "Average PR AUC score: 0.31657538662805473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "optimized_model = Sequential()\n",
        "optimized_model.add(Dense(units=298, activation='relu', input_dim=X.shape[1]))\n",
        "optimized_model.add(Dense(units=1, activation='sigmoid'))\n",
        "optimizer = Adam(learning_rate=0.007119418600172993)\n",
        "optimized_model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "\n",
        "\n",
        "# Train the model on the training data\n",
        "optimized_model.fit(X, y, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Make predictions on X_test\n",
        "y_test_pred_probs = optimized_model.predict(X_test)[:, 0]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "auprc = average_precision_score(y_test, y_test_pred_probs)\n",
        "print(\"AUPRC:\", auprc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGipDcXIj136",
        "outputId": "959ce49b-05b3-4c3a-e990-cbfa9cfbfb94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "84/84 [==============================] - 1s 3ms/step - loss: 0.4300\n",
            "Epoch 2/10\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.3959\n",
            "Epoch 3/10\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.3879\n",
            "Epoch 4/10\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.3857\n",
            "Epoch 5/10\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.3867\n",
            "Epoch 6/10\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.3831\n",
            "Epoch 7/10\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.3840\n",
            "Epoch 8/10\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.3828\n",
            "Epoch 9/10\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.3858\n",
            "Epoch 10/10\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.3811\n",
            "21/21 [==============================] - 0s 2ms/step\n",
            "AUPRC: 0.3030604707156078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# params = {'n_estimators': 39, 'max_depth': 6, 'criterion': 'entropy'}\n",
        "\n",
        "# # Define the KNeighborsClassifier model\n",
        "# xgb_model = RandomForestClassifier(**params)\n",
        "\n",
        "# Define the RepeatedStratifiedKFold cross-validator\n",
        "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
        "\n",
        "# List to store the roc auc scores\n",
        "roc_auc_scores = []\n",
        "\n",
        "# Loop through each cross-validation fold\n",
        "for train_idx, test_idx in cv.split(X, y):\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
        "    \n",
        "    # Fit the KNeighborsClassifier model to the training data\n",
        "    optimized_model.fit(X_train.values, y_train.values)\n",
        "    \n",
        "    # Predict probabilities for the positive class on the test data\n",
        "    model_probs = optimized_model.predict(X_test)[:, 0]\n",
        "    \n",
        "    # Calculate ROC AUC score\n",
        "    roc_auc = average_precision_score(y_test, model_probs)\n",
        "    \n",
        "    # Append ROC AUC score to the list\n",
        "    roc_auc_scores.append(roc_auc)\n",
        "\n",
        "# Calculate the average ROC AUC score across all folds\n",
        "avg_roc_auc = sum(roc_auc_scores) / len(roc_auc_scores)\n",
        "\n",
        "print(\"PR AUC scores:\", roc_auc_scores)\n",
        "print(\"Average PR AUC score:\", avg_roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTHIvyz_j10c",
        "outputId": "4387ee9a-fb29-4044-a86f-1367b7eba2c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67/67 [==============================] - 1s 8ms/step - loss: 0.3754\n",
            "17/17 [==============================] - 0s 4ms/step\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.3837\n",
            "17/17 [==============================] - 0s 4ms/step\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3776\n",
            "17/17 [==============================] - 0s 6ms/step\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3785\n",
            "17/17 [==============================] - 0s 4ms/step\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3788\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.3787\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3768\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3771\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3810\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3789\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3760\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3745\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3725\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3825\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3739\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3769\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3749\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3731\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3692\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3778\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3780\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3728\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3695\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3753\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3762\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3718\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3708\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3695\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3738\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3747\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3705\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3728\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3727\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3702\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3678\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3730\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3691\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3644\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3685\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3667\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3675\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3646\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3658\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3622\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3691\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3706\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3640\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.3625\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3681\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3573\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "PR AUC scores: [0.3265165604669005, 0.349318002819048, 0.35308235030885077, 0.3859078638739704, 0.4155849905074006, 0.3931695468922759, 0.33127584294659645, 0.33964329880785427, 0.38590822322119434, 0.37347556762735146, 0.3291022378879673, 0.3599114288219383, 0.3225221410219269, 0.47409886646026866, 0.34956036667942386, 0.38462849165026414, 0.41297221684859264, 0.417507841619825, 0.3055156967536457, 0.4021957106628148, 0.398942141865847, 0.32679437431649133, 0.3555179380393303, 0.39882547235599575, 0.36413638315430047, 0.40533562367555304, 0.3691123556070365, 0.31091108389838534, 0.4111406081794032, 0.39448414017733385, 0.34701660835982223, 0.4332495257668757, 0.37022498233848444, 0.4041105606099708, 0.329772676311478, 0.38603568755817286, 0.4128101459628857, 0.35799141334237194, 0.434326066466938, 0.36265593425026404, 0.4182922480750557, 0.3600570577175596, 0.3917951182151185, 0.39920752948076993, 0.4622061210188649, 0.48450510831318455, 0.3679786556796524, 0.3658539783048681, 0.4507557962953375, 0.3505220794454118]\n",
            "Average PR AUC score: 0.3807292932132175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Assuming you have your training and evaluation data as X_train, y_train, X_eval, y_eval, and test data as X_test, y_test\n",
        "\n",
        "# Define the parameters\n",
        "params = {'C': 100.0, 'penalty': 'l2', 'solver': 'saga'}\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(**params)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUC\n",
        "auprc = average_precision_score(y_test, y_pred)\n",
        "print(\"AUPRC:\", auprc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ7yDTyej1yq",
        "outputId": "a4c39fb2-fb1d-4476-9076-e48feb327d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUPRC: 0.31531295361823947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# params = {'n_estimators': 39, 'max_depth': 6, 'criterion': 'entropy'}\n",
        "\n",
        "# # Define the KNeighborsClassifier model\n",
        "# xgb_model = RandomForestClassifier(**params)\n",
        "\n",
        "# Define the RepeatedStratifiedKFold cross-validator\n",
        "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
        "\n",
        "# List to store the roc auc scores\n",
        "roc_auc_scores = []\n",
        "\n",
        "# Loop through each cross-validation fold\n",
        "for train_idx, test_idx in cv.split(X, y):\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
        "    \n",
        "    # Fit the KNeighborsClassifier model to the training data\n",
        "    model.fit(X_train.values, y_train.values)\n",
        "    \n",
        "    # Predict probabilities for the positive class on the test data\n",
        "    model_probs = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate ROC AUC score\n",
        "    roc_auc = average_precision_score(y_test, model_probs)\n",
        "    \n",
        "    # Append ROC AUC score to the list\n",
        "    roc_auc_scores.append(roc_auc)\n",
        "\n",
        "# Calculate the average ROC AUC score across all folds\n",
        "avg_roc_auc = sum(roc_auc_scores) / len(roc_auc_scores)\n",
        "\n",
        "print(\"PR AUC scores:\", roc_auc_scores)\n",
        "print(\"Average PR AUC score:\", avg_roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX05zYqNj1ws",
        "outputId": "2664d055-15ce-44eb-f7a3-1896569255bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PR AUC scores: [0.304359059858775, 0.34998340220480634, 0.3112523517663998, 0.3700941970540661, 0.3708721335637062, 0.36319050643758155, 0.33173677050032135, 0.2997856451372715, 0.3453155984052022, 0.3596324371741583, 0.30707902442166163, 0.3019802696082036, 0.3182239529338129, 0.42300713837760234, 0.33224761721088325, 0.35670833080954994, 0.3679990784655324, 0.35315374872055694, 0.263622085442966, 0.34714860741621245, 0.38762491729892457, 0.29453835557656105, 0.2936095972183018, 0.3726277343077078, 0.32686409464140315, 0.3770118017591457, 0.3415073536681109, 0.2738635585509745, 0.34095352901533876, 0.32141098892174474, 0.3258095773870866, 0.3379423184341059, 0.3538957148106596, 0.3501790686427871, 0.3043577582158818, 0.3774677689441678, 0.30902055880069823, 0.3241049954239581, 0.3895900664992838, 0.31403484866513626, 0.33123923385552917, 0.34039080779222686, 0.3388858850123608, 0.3182177058673879, 0.38517888336506145, 0.38543278504624273, 0.3075619249941497, 0.30150382941540527, 0.39265601781410553, 0.3239948306006542]\n",
            "Average PR AUC score: 0.3383773693210875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Assuming you have your training and evaluation data as X_train, y_train, X_eval, y_eval, and test data as X_test, y_test\n",
        "\n",
        "# Define the parameters\n",
        "params = {'max_depth': 4, 'n_estimators': 399, 'learning_rate': 0.14376998191774137, \n",
        "          'subsample': 0.646680904808694, 'colsample_bytree': 0.6279281044874794, 'min_child_weight': 7.707299055505381}\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model = xgb.XGBClassifier(**params)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUPRC\n",
        "auprc = average_precision_score(y_test, y_pred)\n",
        "print(\"AUPRC:\", auprc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGR1Tep1j1uF",
        "outputId": "b25c8932-5c91-47fa-ead7-66242a9222ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUPRC: 0.2705901706175817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# params = {'n_estimators': 39, 'max_depth': 6, 'criterion': 'entropy'}\n",
        "\n",
        "# # Define the KNeighborsClassifier model\n",
        "# xgb_model = RandomForestClassifier(**params)\n",
        "\n",
        "# Define the RepeatedStratifiedKFold cross-validator\n",
        "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
        "\n",
        "# List to store the roc auc scores\n",
        "roc_auc_scores = []\n",
        "\n",
        "# Loop through each cross-validation fold\n",
        "for train_idx, test_idx in cv.split(X, y):\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
        "    \n",
        "    # Fit the KNeighborsClassifier model to the training data\n",
        "    model.fit(X_train.values, y_train.values)\n",
        "    \n",
        "    # Predict probabilities for the positive class on the test data\n",
        "    model_probs = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate ROC AUC score\n",
        "    roc_auc = average_precision_score(y_test, model_probs)\n",
        "    \n",
        "    # Append ROC AUC score to the list\n",
        "    roc_auc_scores.append(roc_auc)\n",
        "\n",
        "# Calculate the average ROC AUC score across all folds\n",
        "avg_roc_auc = sum(roc_auc_scores) / len(roc_auc_scores)\n",
        "\n",
        "print(\"PR AUC scores:\", roc_auc_scores)\n",
        "print(\"Average PR AUC score:\", avg_roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UxEmBHjj1sM",
        "outputId": "4b2aeae7-da01-43e4-baa0-da8cacd3f33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PR AUC scores: [0.23373006710125366, 0.22002889379346846, 0.3043283332792035, 0.30991719172599597, 0.32976350639569174, 0.3336817965265315, 0.2628736688675108, 0.21381241645412907, 0.2615883241363813, 0.2573725662020155, 0.207862923619631, 0.2504866070515816, 0.33634040325061604, 0.28112430884577866, 0.25880510050833017, 0.28292310813254634, 0.27462096743350234, 0.24200474364758007, 0.2403862684682892, 0.25636997417391166, 0.31221340129393294, 0.24767604857738962, 0.25062562400135485, 0.26181838340464586, 0.2308976862283468, 0.2600283293654226, 0.2832024294170507, 0.2902278659613563, 0.30992939402108666, 0.24296939479800114, 0.26234120657068133, 0.2811437184747529, 0.25544921373259816, 0.2302041527765677, 0.2993455903154899, 0.28286131736215614, 0.22060074628563953, 0.28099509684914975, 0.3081955782396014, 0.2508088366783739, 0.3072468465938481, 0.22708137489036162, 0.30122103365578307, 0.2697833370931263, 0.2545874472768205, 0.297260429189029, 0.25370774973298593, 0.25314404386064776, 0.33324817369596554, 0.22291268941342648]\n",
            "Average PR AUC score: 0.26879496618739085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Assuming you have your training and evaluation data as X_train, y_train, X_eval, y_eval, and test data as X_test, y_test\n",
        "\n",
        "# Define the parameters\n",
        "params = {'n_neighbors': 19, 'weights': 'distance'}\n",
        "\n",
        "# Train the model\n",
        "model = KNeighborsClassifier(**params)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUC\n",
        "auprc = average_precision_score(y_test, y_pred)\n",
        "print(\"AUPRC:\", auprc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX_VIx2lj1qo",
        "outputId": "ca00deac-92e2-42da-fc14-ebc3e17ad331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUPRC: 0.2773693107662063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# params = {'n_estimators': 39, 'max_depth': 6, 'criterion': 'entropy'}\n",
        "\n",
        "# # Define the KNeighborsClassifier model\n",
        "# xgb_model = RandomForestClassifier(**params)\n",
        "\n",
        "# Define the RepeatedStratifiedKFold cross-validator\n",
        "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
        "\n",
        "# List to store the roc auc scores\n",
        "roc_auc_scores = []\n",
        "\n",
        "# Loop through each cross-validation fold\n",
        "for train_idx, test_idx in cv.split(X, y):\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
        "    \n",
        "    # Fit the KNeighborsClassifier model to the training data\n",
        "    model.fit(X_train.values, y_train.values)\n",
        "    \n",
        "    # Predict probabilities for the positive class on the test data\n",
        "    model_probs = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate ROC AUC score\n",
        "    roc_auc = average_precision_score(y_test, model_probs)\n",
        "    \n",
        "    # Append ROC AUC score to the list\n",
        "    roc_auc_scores.append(roc_auc)\n",
        "\n",
        "# Calculate the average ROC AUC score across all folds\n",
        "avg_roc_auc = sum(roc_auc_scores) / len(roc_auc_scores)\n",
        "\n",
        "print(\"PR AUC scores:\", roc_auc_scores)\n",
        "print(\"Average PR AUC score:\", avg_roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60viniwyj1on",
        "outputId": "2a9eb81c-ae8d-4480-8bbb-b787a1f7da04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PR AUC scores: [0.26188199393130035, 0.27004573810817434, 0.2694992503225729, 0.2551630012661197, 0.2586012674971601, 0.30116475570060836, 0.2695056704580086, 0.26687923295230975, 0.2891477661740872, 0.2312093444700976, 0.26693843786378674, 0.2243157546890571, 0.2880455746519299, 0.332803318268606, 0.2471272060717694, 0.3104133429109068, 0.27665347482970853, 0.2813438023061431, 0.26793530768898155, 0.2853232549658606, 0.30950119495236417, 0.2096862474338987, 0.26821789717977823, 0.2586134340096478, 0.2542360747102117, 0.26238178477789087, 0.25556025828588, 0.27526725217883014, 0.25744035758765893, 0.2338477855456168, 0.2597963709245301, 0.29632507480079073, 0.2647555826439088, 0.2592128054849234, 0.27371522437355045, 0.286044237626881, 0.30470901942606715, 0.2516053955790023, 0.3468028761961107, 0.19806854803603727, 0.29091838924780056, 0.2841855203394199, 0.26594437911779045, 0.24131470840439448, 0.30203897937164714, 0.3326597138452949, 0.2313513982048057, 0.27551309884376385, 0.2625559088665985, 0.2510347598050032]\n",
            "Average PR AUC score: 0.27034603545854574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GybSlIcbjT7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "stacking\n",
        "\n",
        "PR AUC scores: [0.30828978069765367, 0.343798168302532, 0.3024990906503224, 0.37244324439592175, 0.3621060082745221, 0.3425173189704659, 0.3127579198901261, 0.29986619408057935, 0.3463951319013106, 0.36500482077668606, 0.28929539145182925, 0.301058334552206, 0.32098378336043165, 0.4119009424944663, 0.32387770287610806, 0.3617290208761753, 0.3650543504932151, 0.3410275420263037, 0.2776267533351995, 0.3421799223931475, 0.38316467223291417, 0.28279800080582157, 0.308678076620574, 0.367095089053494, 0.3212540629120771, 0.3554017057456056, 0.3352674243264728, 0.27504849945547355, 0.344531367537768, 0.3133587050705793, 0.3207262108710167, 0.34249867829291253, 0.33822082747321663, 0.3351299515696396, 0.2969763426522304, 0.3842545269114456, 0.29083871513175646, 0.3231439499667997, 0.3889253426872323, 0.3162829832155528, 0.32388126858928357, 0.32055482200037094, 0.35022520328595164, 0.3170043716048848, 0.3800600691292034, 0.38414906675811017, 0.30048979831673905, 0.30603348435800537, 0.3973794775606681, 0.2945457085793086]\n",
        "Average PR AUC score: 0.33376659649028634"
      ],
      "metadata": {
        "id": "XnP1WUQXidmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "soft voting\n",
        "\n",
        "PR AUC scores: [0.28913093260615685, 0.3176392415776459, 0.33865640267979735, 0.38350516559564346, 0.3857002764741272, 0.39379700845431376, 0.31470323312356385, 0.2879355761533104, 0.33547206642472743, 0.3344013534856112, 0.2705129486604148, 0.28773323020229313, 0.3504469287760851, 0.4030543023766945, 0.32169239158211976, 0.3483622754206326, 0.3539517636180863, 0.32892435489692484, 0.28284584967135606, 0.3277611575624709, 0.37953559113300583, 0.30169382778315956, 0.2904659844348636, 0.3388783709726533, 0.3019426227684737, 0.34289550623237774, 0.3358715011723919, 0.303327666686701, 0.33280683340687056, 0.303804208534957, 0.3258949936627553, 0.3496926826396013, 0.3329609191267726, 0.3240683263723104, 0.31113223384292676, 0.3600743843944028, 0.28433189597397934, 0.32044593262653875, 0.3675332173052174, 0.300449168031108, 0.3531954176252953, 0.30274433958501346, 0.36102014915628533, 0.31076155770972214, 0.34933288806449797, 0.39377620827673354, 0.29555373770865767, 0.27163339396214164, 0.39132555760528165, 0.2841578656492323]\n",
        "Average PR AUC score: 0.3295506688357181"
      ],
      "metadata": {
        "id": "KZaDKrYpjRFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "gb\n",
        "\n",
        "PR AUC scores: [0.27978480974743847, 0.28803176010421166, 0.33964565190370616, 0.3964323980514589, 0.34466250828463874, 0.33950958771381623, 0.34391780312756864, 0.262621457665221, 0.3288197358720021, 0.33822540613888186, 0.2662778120712578, 0.2760835723493187, 0.3367146073363239, 0.45001217890703077, 0.273839066035762, 0.32037877615255134, 0.35095539530864683, 0.2980041536755427, 0.2801692939127861, 0.3208265174885092, 0.36991112857280195, 0.30089751967719297, 0.27405683896174576, 0.3287502684267688, 0.29243508401421636, 0.31903299393412504, 0.3613333007071185, 0.2821320887455845, 0.2962773783691569, 0.2958148094669302, 0.2876159348531316, 0.35830727137751084, 0.2868711710242126, 0.31510627402514785, 0.3181949778551382, 0.36457694594408374, 0.28802476293332135, 0.3005076991744001, 0.35865079921937226, 0.2704934187308193, 0.31061118199611076, 0.2727124144648201, 0.32336441771145813, 0.31226537545229993, 0.32931232876511635, 0.3762192059531769, 0.2764638898713364, 0.28537190143066743, 0.36617914703496196, 0.2723663108633346]\n",
        "Average PR AUC score: 0.31657538662805473"
      ],
      "metadata": {
        "id": "Kf6KTpYjjv2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NN\n",
        "\n",
        "PR AUC scores: [0.3265165604669005, 0.349318002819048, 0.35308235030885077, 0.3859078638739704, 0.4155849905074006, 0.3931695468922759, 0.33127584294659645, 0.33964329880785427, 0.38590822322119434, 0.37347556762735146, 0.3291022378879673, 0.3599114288219383, 0.3225221410219269, 0.47409886646026866, 0.34956036667942386, 0.38462849165026414, 0.41297221684859264, 0.417507841619825, 0.3055156967536457, 0.4021957106628148, 0.398942141865847, 0.32679437431649133, 0.3555179380393303, 0.39882547235599575, 0.36413638315430047, 0.40533562367555304, 0.3691123556070365, 0.31091108389838534, 0.4111406081794032, 0.39448414017733385, 0.34701660835982223, 0.4332495257668757, 0.37022498233848444, 0.4041105606099708, 0.329772676311478, 0.38603568755817286, 0.4128101459628857, 0.35799141334237194, 0.434326066466938, 0.36265593425026404, 0.4182922480750557, 0.3600570577175596, 0.3917951182151185, 0.39920752948076993, 0.4622061210188649, 0.48450510831318455, 0.3679786556796524, 0.3658539783048681, 0.4507557962953375, 0.3505220794454118]\n",
        "Average PR AUC score: 0.3807292932132175"
      ],
      "metadata": {
        "id": "qWgR4n7VkQaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "PR AUC scores: [0.304359059858775, 0.34998340220480634, 0.3112523517663998, 0.3700941970540661, 0.3708721335637062, 0.36319050643758155, 0.33173677050032135, 0.2997856451372715, 0.3453155984052022, 0.3596324371741583, 0.30707902442166163, 0.3019802696082036, 0.3182239529338129, 0.42300713837760234, 0.33224761721088325, 0.35670833080954994, 0.3679990784655324, 0.35315374872055694, 0.263622085442966, 0.34714860741621245, 0.38762491729892457, 0.29453835557656105, 0.2936095972183018, 0.3726277343077078, 0.32686409464140315, 0.3770118017591457, 0.3415073536681109, 0.2738635585509745, 0.34095352901533876, 0.32141098892174474, 0.3258095773870866, 0.3379423184341059, 0.3538957148106596, 0.3501790686427871, 0.3043577582158818, 0.3774677689441678, 0.30902055880069823, 0.3241049954239581, 0.3895900664992838, 0.31403484866513626, 0.33123923385552917, 0.34039080779222686, 0.3388858850123608, 0.3182177058673879, 0.38517888336506145, 0.38543278504624273, 0.3075619249941497, 0.30150382941540527, 0.39265601781410553, 0.3239948306006542]\n",
        "Average PR AUC score: 0.3383773693210875"
      ],
      "metadata": {
        "id": "2StNC2UqkiO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XG\n",
        "\n",
        "PR AUC scores: [0.23373006710125366, 0.22002889379346846, 0.3043283332792035, 0.30991719172599597, 0.32976350639569174, 0.3336817965265315, 0.2628736688675108, 0.21381241645412907, 0.2615883241363813, 0.2573725662020155, 0.207862923619631, 0.2504866070515816, 0.33634040325061604, 0.28112430884577866, 0.25880510050833017, 0.28292310813254634, 0.27462096743350234, 0.24200474364758007, 0.2403862684682892, 0.25636997417391166, 0.31221340129393294, 0.24767604857738962, 0.25062562400135485, 0.26181838340464586, 0.2308976862283468, 0.2600283293654226, 0.2832024294170507, 0.2902278659613563, 0.30992939402108666, 0.24296939479800114, 0.26234120657068133, 0.2811437184747529, 0.25544921373259816, 0.2302041527765677, 0.2993455903154899, 0.28286131736215614, 0.22060074628563953, 0.28099509684914975, 0.3081955782396014, 0.2508088366783739, 0.3072468465938481, 0.22708137489036162, 0.30122103365578307, 0.2697833370931263, 0.2545874472768205, 0.297260429189029, 0.25370774973298593, 0.25314404386064776, 0.33324817369596554, 0.22291268941342648]\n",
        "Average PR AUC score: 0.26879496618739085"
      ],
      "metadata": {
        "id": "HMbIYscrlKjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN\n",
        "PR AUC scores: [0.26188199393130035, 0.27004573810817434, 0.2694992503225729, 0.2551630012661197, 0.2586012674971601, 0.30116475570060836, 0.2695056704580086, 0.26687923295230975, 0.2891477661740872, 0.2312093444700976, 0.26693843786378674, 0.2243157546890571, 0.2880455746519299, 0.332803318268606, 0.2471272060717694, 0.3104133429109068, 0.27665347482970853, 0.2813438023061431, 0.26793530768898155, 0.2853232549658606, 0.30950119495236417, 0.2096862474338987, 0.26821789717977823, 0.2586134340096478, 0.2542360747102117, 0.26238178477789087, 0.25556025828588, 0.27526725217883014, 0.25744035758765893, 0.2338477855456168, 0.2597963709245301, 0.29632507480079073, 0.2647555826439088, 0.2592128054849234, 0.27371522437355045, 0.286044237626881, 0.30470901942606715, 0.2516053955790023, 0.3468028761961107, 0.19806854803603727, 0.29091838924780056, 0.2841855203394199, 0.26594437911779045, 0.24131470840439448, 0.30203897937164714, 0.3326597138452949, 0.2313513982048057, 0.27551309884376385, 0.2625559088665985, 0.2510347598050032]\n",
        "Average PR AUC score: 0.27034603545854574\n"
      ],
      "metadata": {
        "id": "QwVsMFY7lXi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "data = [\n",
        "    #knn \n",
        "    [0.26188199393130035, 0.27004573810817434, 0.2694992503225729, 0.2551630012661197, 0.2586012674971601, 0.30116475570060836, 0.2695056704580086, 0.26687923295230975, 0.2891477661740872, 0.2312093444700976, 0.26693843786378674, 0.2243157546890571, 0.2880455746519299, 0.332803318268606, 0.2471272060717694, 0.3104133429109068, 0.27665347482970853, 0.2813438023061431, 0.26793530768898155, 0.2853232549658606, 0.30950119495236417, 0.2096862474338987, 0.26821789717977823, 0.2586134340096478, 0.2542360747102117, 0.26238178477789087, 0.25556025828588, 0.27526725217883014, 0.25744035758765893, 0.2338477855456168, 0.2597963709245301, 0.29632507480079073, 0.2647555826439088, 0.2592128054849234, 0.27371522437355045, 0.286044237626881, 0.30470901942606715, 0.2516053955790023, 0.3468028761961107, 0.19806854803603727, 0.29091838924780056, 0.2841855203394199, 0.26594437911779045, 0.24131470840439448, 0.30203897937164714, 0.3326597138452949, 0.2313513982048057, 0.27551309884376385, 0.2625559088665985, 0.2510347598050032],\n",
        "    #XG\n",
        "    [0.23373006710125366, 0.22002889379346846, 0.3043283332792035, 0.30991719172599597, 0.32976350639569174, 0.3336817965265315, 0.2628736688675108, 0.21381241645412907, 0.2615883241363813, 0.2573725662020155, 0.207862923619631, 0.2504866070515816, 0.33634040325061604, 0.28112430884577866, 0.25880510050833017, 0.28292310813254634, 0.27462096743350234, 0.24200474364758007, 0.2403862684682892, 0.25636997417391166, 0.31221340129393294, 0.24767604857738962, 0.25062562400135485, 0.26181838340464586, 0.2308976862283468, 0.2600283293654226, 0.2832024294170507, 0.2902278659613563, 0.30992939402108666, 0.24296939479800114, 0.26234120657068133, 0.2811437184747529, 0.25544921373259816, 0.2302041527765677, 0.2993455903154899, 0.28286131736215614, 0.22060074628563953, 0.28099509684914975, 0.3081955782396014, 0.2508088366783739, 0.3072468465938481, 0.22708137489036162, 0.30122103365578307, 0.2697833370931263, 0.2545874472768205, 0.297260429189029, 0.25370774973298593, 0.25314404386064776, 0.33324817369596554, 0.22291268941342648],\n",
        "    #LR\n",
        "    [0.304359059858775, 0.34998340220480634, 0.3112523517663998, 0.3700941970540661, 0.3708721335637062, 0.36319050643758155, 0.33173677050032135, 0.2997856451372715, 0.3453155984052022, 0.3596324371741583, 0.30707902442166163, 0.3019802696082036, 0.3182239529338129, 0.42300713837760234, 0.33224761721088325, 0.35670833080954994, 0.3679990784655324, 0.35315374872055694, 0.263622085442966, 0.34714860741621245, 0.38762491729892457, 0.29453835557656105, 0.2936095972183018, 0.3726277343077078, 0.32686409464140315, 0.3770118017591457, 0.3415073536681109, 0.2738635585509745, 0.34095352901533876, 0.32141098892174474, 0.3258095773870866, 0.3379423184341059, 0.3538957148106596, 0.3501790686427871, 0.3043577582158818, 0.3774677689441678, 0.30902055880069823, 0.3241049954239581, 0.3895900664992838, 0.31403484866513626, 0.33123923385552917, 0.34039080779222686, 0.3388858850123608, 0.3182177058673879, 0.38517888336506145, 0.38543278504624273, 0.3075619249941497, 0.30150382941540527, 0.39265601781410553, 0.3239948306006542],\n",
        "    #NN\n",
        "    [0.3265165604669005, 0.349318002819048, 0.35308235030885077, 0.3859078638739704, 0.4155849905074006, 0.3931695468922759, 0.33127584294659645, 0.33964329880785427, 0.38590822322119434, 0.37347556762735146, 0.3291022378879673, 0.3599114288219383, 0.3225221410219269, 0.47409886646026866, 0.34956036667942386, 0.38462849165026414, 0.41297221684859264, 0.417507841619825, 0.3055156967536457, 0.4021957106628148, 0.398942141865847, 0.32679437431649133, 0.3555179380393303, 0.39882547235599575, 0.36413638315430047, 0.40533562367555304, 0.3691123556070365, 0.31091108389838534, 0.4111406081794032, 0.39448414017733385, 0.34701660835982223, 0.4332495257668757, 0.37022498233848444, 0.4041105606099708, 0.329772676311478, 0.38603568755817286, 0.4128101459628857, 0.35799141334237194, 0.434326066466938, 0.36265593425026404, 0.4182922480750557, 0.3600570577175596, 0.3917951182151185, 0.39920752948076993, 0.4622061210188649, 0.48450510831318455, 0.3679786556796524, 0.3658539783048681, 0.4507557962953375, 0.3505220794454118],\n",
        "    #GB\n",
        "    [0.27978480974743847, 0.28803176010421166, 0.33964565190370616, 0.3964323980514589, 0.34466250828463874, 0.33950958771381623, 0.34391780312756864, 0.262621457665221, 0.3288197358720021, 0.33822540613888186, 0.2662778120712578, 0.2760835723493187, 0.3367146073363239, 0.45001217890703077, 0.273839066035762, 0.32037877615255134, 0.35095539530864683, 0.2980041536755427, 0.2801692939127861, 0.3208265174885092, 0.36991112857280195, 0.30089751967719297, 0.27405683896174576, 0.3287502684267688, 0.29243508401421636, 0.31903299393412504, 0.3613333007071185, 0.2821320887455845, 0.2962773783691569, 0.2958148094669302, 0.2876159348531316, 0.35830727137751084, 0.2868711710242126, 0.31510627402514785, 0.3181949778551382, 0.36457694594408374, 0.28802476293332135, 0.3005076991744001, 0.35865079921937226, 0.2704934187308193, 0.31061118199611076, 0.2727124144648201, 0.32336441771145813, 0.31226537545229993, 0.32931232876511635, 0.3762192059531769, 0.2764638898713364, 0.28537190143066743, 0.36617914703496196, 0.2723663108633346],\n",
        "    #SV\n",
        "    [0.28913093260615685, 0.3176392415776459, 0.33865640267979735, 0.38350516559564346, 0.3857002764741272, 0.39379700845431376, 0.31470323312356385, 0.2879355761533104, 0.33547206642472743, 0.3344013534856112, 0.2705129486604148, 0.28773323020229313, 0.3504469287760851, 0.4030543023766945, 0.32169239158211976, 0.3483622754206326, 0.3539517636180863, 0.32892435489692484, 0.28284584967135606, 0.3277611575624709, 0.37953559113300583, 0.30169382778315956, 0.2904659844348636, 0.3388783709726533, 0.3019426227684737, 0.34289550623237774, 0.3358715011723919, 0.303327666686701, 0.33280683340687056, 0.303804208534957, 0.3258949936627553, 0.3496926826396013, 0.3329609191267726, 0.3240683263723104, 0.31113223384292676, 0.3600743843944028, 0.28433189597397934, 0.32044593262653875, 0.3675332173052174, 0.300449168031108, 0.3531954176252953, 0.30274433958501346, 0.36102014915628533, 0.31076155770972214, 0.34933288806449797, 0.39377620827673354, 0.29555373770865767, 0.27163339396214164, 0.39132555760528165, 0.2841578656492323],\n",
        "    #Stacking\n",
        "    [0.30828978069765367, 0.343798168302532, 0.3024990906503224, 0.37244324439592175, 0.3621060082745221, 0.3425173189704659, 0.3127579198901261, 0.29986619408057935, 0.3463951319013106, 0.36500482077668606, 0.28929539145182925, 0.301058334552206, 0.32098378336043165, 0.4119009424944663, 0.32387770287610806, 0.3617290208761753, 0.3650543504932151, 0.3410275420263037, 0.2776267533351995, 0.3421799223931475, 0.38316467223291417, 0.28279800080582157, 0.308678076620574, 0.367095089053494, 0.3212540629120771, 0.3554017057456056, 0.3352674243264728, 0.27504849945547355, 0.344531367537768, 0.3133587050705793, 0.3207262108710167, 0.34249867829291253, 0.33822082747321663, 0.3351299515696396, 0.2969763426522304, 0.3842545269114456, 0.29083871513175646, 0.3231439499667997, 0.3889253426872323, 0.3162829832155528, 0.32388126858928357, 0.32055482200037094, 0.35022520328595164, 0.3170043716048848, 0.3800600691292034, 0.38414906675811017, 0.30048979831673905, 0.30603348435800537, 0.3973794775606681, 0.2945457085793086]\n",
        "]\n",
        "\n",
        "categories = ['KNN', 'XG Boost', 'Logistic Regression', 'Neural Network', 'Gradient Boost',  'Soft Voting', 'Stacking']\n",
        "\n",
        "# custom values for each category\n",
        "custom_values = [0.2773693107662063, 0.2705901706175817, 0.31532339789895136, 0.29301497674638266, 0.31967277698702457, 0.32002242661014807, 0.3128332853598304]\n",
        "\n",
        "# create a list of values and categories for the boxplot\n",
        "values = [item for sublist in data for item in sublist]\n",
        "cats = [cat for cat in categories for i in range(len(data[0]))]\n",
        "\n",
        "# create a dataframe for the data\n",
        "df = pd.DataFrame({'Values': values, 'Category': cats})\n",
        "\n",
        "# set seaborn style\n",
        "sns.set(style='whitegrid', font_scale=1.2)\n",
        "\n",
        "# create the boxplot with custom styling\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.boxplot(x='Category', y='Values', data=df, width=0.5, palette='pastel')\n",
        "sns.stripplot(x='Category', y='Values', data=df, jitter=True, size=5, alpha=0.5, color='black')\n",
        "sns.stripplot(x=categories, y=custom_values, marker='D', size=7, color='red', edgecolor='black', alpha = 0.6)\n",
        "\n",
        "# set plot labels and title\n",
        "ax.set_xlabel('Model', fontsize=10, labelpad=8)\n",
        "ax.set_ylabel('Precision Recall AUC', fontsize=10, labelpad=8)\n",
        "ax.set_title('Model Comparision', fontsize=10, pad=10)\n",
        "\n",
        "# set font size for tick labels\n",
        "ax.tick_params(axis='both', labelsize=8)\n",
        "\n",
        "# remove spines\n",
        "sns.despine()\n",
        "\n",
        "# adjust plot layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "RlVRpnqYie_Y",
        "outputId": "27c2aa00-4378-4da3-99bd-55a82e61c7f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WrAStIOUlVQP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}