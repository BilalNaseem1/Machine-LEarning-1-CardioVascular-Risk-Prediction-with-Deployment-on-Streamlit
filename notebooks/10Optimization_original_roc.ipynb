{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_xG_BDDJAXR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3805a7cc-a651-4bcd-8669-b635962c3afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.0)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-23.5.9-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.1.0)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-23.5.9 scikit-optimize-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9dy60Xb_kp8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "from scipy.stats import ttest_1samp, shapiro\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "from sklearn.metrics import recall_score, make_scorer, roc_auc_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "from graphviz import Source\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn import tree\n",
        "from IPython.display import SVG, display\n",
        "# import shap\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from scipy.stats.mstats import winsorize\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "# from pycaret.classification import *\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import recall_score\n",
        "import six\n",
        "import sys\n",
        "sys.modules['sklearn.externals.six'] = six\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "from skopt import gp_minimize, space\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v_DgDORG2e6"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv('/content/sample_data/Best_dataset_fp.csv')\n",
        "# df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "df = pd.read_csv('/content/sample_data/Best_dataset2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgQz7MNIG5kg"
      },
      "outputs": [],
      "source": [
        "# Split the DataFrame into training set (90%) and test set (10%)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKhr1NrTIAtQ"
      },
      "outputs": [],
      "source": [
        "X = train_df.drop(['TenYearCHD_1'], axis = 1)\n",
        "y = train_df['TenYearCHD_1']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNptMI6GIeOm"
      },
      "source": [
        "### LighGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c2eVkMoIKUz",
        "outputId": "954447b2-990a-4f75-e7cc-dfb55be11a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 1 started. Evaluating function at random point.\n",
            "Iteration No: 1 ended. Evaluation done at random point.\n",
            "Time taken: 5.0826\n",
            "Function value obtained: -0.6771\n",
            "Current minimum: -0.6771\n",
            "Iteration No: 2 started. Evaluating function at random point.\n",
            "Iteration No: 2 ended. Evaluation done at random point.\n",
            "Time taken: 0.4856\n",
            "Function value obtained: -0.6860\n",
            "Current minimum: -0.6860\n",
            "Iteration No: 3 started. Evaluating function at random point.\n",
            "Iteration No: 3 ended. Evaluation done at random point.\n",
            "Time taken: 0.3294\n",
            "Function value obtained: -0.6350\n",
            "Current minimum: -0.6860\n",
            "Iteration No: 4 started. Evaluating function at random point.\n",
            "Iteration No: 4 ended. Evaluation done at random point.\n",
            "Time taken: 0.3133\n",
            "Function value obtained: -0.6924\n",
            "Current minimum: -0.6924\n",
            "Iteration No: 5 started. Evaluating function at random point.\n",
            "Iteration No: 5 ended. Evaluation done at random point.\n",
            "Time taken: 0.2762\n",
            "Function value obtained: -0.6835\n",
            "Current minimum: -0.6924\n",
            "Iteration No: 6 started. Evaluating function at random point.\n",
            "Iteration No: 6 ended. Evaluation done at random point.\n",
            "Time taken: 0.6198\n",
            "Function value obtained: -0.6933\n",
            "Current minimum: -0.6933\n",
            "Iteration No: 7 started. Evaluating function at random point.\n",
            "Iteration No: 7 ended. Evaluation done at random point.\n",
            "Time taken: 0.3232\n",
            "Function value obtained: -0.6352\n",
            "Current minimum: -0.6933\n",
            "Iteration No: 8 started. Evaluating function at random point.\n",
            "Iteration No: 8 ended. Evaluation done at random point.\n",
            "Time taken: 0.3255\n",
            "Function value obtained: -0.6383\n",
            "Current minimum: -0.6933\n",
            "Iteration No: 9 started. Evaluating function at random point.\n",
            "Iteration No: 9 ended. Evaluation done at random point.\n",
            "Time taken: 0.2652\n",
            "Function value obtained: -0.6703\n",
            "Current minimum: -0.6933\n",
            "Iteration No: 10 started. Evaluating function at random point.\n",
            "Iteration No: 10 ended. Evaluation done at random point.\n",
            "Time taken: 0.3143\n",
            "Function value obtained: -0.6358\n",
            "Current minimum: -0.6933\n",
            "Iteration No: 11 started. Evaluating function at random point.\n",
            "Iteration No: 11 ended. Evaluation done at random point.\n",
            "Time taken: 0.3776\n",
            "Function value obtained: -0.6698\n",
            "Current minimum: -0.6933\n",
            "Iteration No: 12 started. Evaluating function at random point.\n",
            "Iteration No: 12 ended. Evaluation done at random point.\n",
            "Time taken: 0.2694\n",
            "Function value obtained: -0.6639\n",
            "Current minimum: -0.6933\n",
            "Iteration No: 13 started. Evaluating function at random point.\n",
            "Iteration No: 13 ended. Evaluation done at random point.\n",
            "Time taken: 0.3603\n",
            "Function value obtained: -0.6618\n",
            "Current minimum: -0.6933\n",
            "Iteration No: 14 started. Evaluating function at random point.\n",
            "Iteration No: 14 ended. Evaluation done at random point.\n",
            "Time taken: 0.2596\n",
            "Function value obtained: -0.6837\n",
            "Current minimum: -0.6933\n",
            "Iteration No: 15 started. Evaluating function at random point.\n",
            "Iteration No: 15 ended. Evaluation done at random point.\n",
            "Time taken: 0.2907\n",
            "Function value obtained: -0.6801\n",
            "Current minimum: -0.6933\n",
            "Iteration No: 16 started. Evaluating function at random point.\n",
            "Iteration No: 16 ended. Evaluation done at random point.\n",
            "Time taken: 0.4465\n",
            "Function value obtained: -0.6713\n",
            "Current minimum: -0.6933\n",
            "Iteration No: 17 started. Evaluating function at random point.\n",
            "Iteration No: 17 ended. Evaluation done at random point.\n",
            "Time taken: 0.5778\n",
            "Function value obtained: -0.6856\n",
            "Current minimum: -0.6933\n",
            "Iteration No: 18 started. Evaluating function at random point.\n",
            "Iteration No: 18 ended. Evaluation done at random point.\n",
            "Time taken: 0.3768\n",
            "Function value obtained: -0.6619\n",
            "Current minimum: -0.6933\n",
            "Iteration No: 19 started. Evaluating function at random point.\n",
            "Iteration No: 19 ended. Evaluation done at random point.\n",
            "Time taken: 0.3118\n",
            "Function value obtained: -0.6723\n",
            "Current minimum: -0.6933\n",
            "Iteration No: 20 started. Evaluating function at random point.\n",
            "Iteration No: 20 ended. Evaluation done at random point.\n",
            "Time taken: 0.8902\n",
            "Function value obtained: -0.6977\n",
            "Current minimum: -0.6977\n",
            "Iteration No: 21 started. Evaluating function at random point.\n",
            "Iteration No: 21 ended. Evaluation done at random point.\n",
            "Time taken: 0.2822\n",
            "Function value obtained: -0.6681\n",
            "Current minimum: -0.6977\n",
            "Iteration No: 22 started. Evaluating function at random point.\n",
            "Iteration No: 22 ended. Evaluation done at random point.\n",
            "Time taken: 0.2639\n",
            "Function value obtained: -0.6836\n",
            "Current minimum: -0.6977\n",
            "Iteration No: 23 started. Evaluating function at random point.\n",
            "Iteration No: 23 ended. Evaluation done at random point.\n",
            "Time taken: 2.5930\n",
            "Function value obtained: -0.6726\n",
            "Current minimum: -0.6977\n",
            "Iteration No: 24 started. Evaluating function at random point.\n",
            "Iteration No: 24 ended. Evaluation done at random point.\n",
            "Time taken: 0.7754\n",
            "Function value obtained: -0.6999\n",
            "Current minimum: -0.6999\n",
            "Iteration No: 25 started. Evaluating function at random point.\n",
            "Iteration No: 25 ended. Evaluation done at random point.\n",
            "Time taken: 0.3127\n",
            "Function value obtained: -0.6517\n",
            "Current minimum: -0.6999\n",
            "Iteration No: 26 started. Evaluating function at random point.\n",
            "Iteration No: 26 ended. Evaluation done at random point.\n",
            "Time taken: 0.6350\n",
            "Function value obtained: -0.6840\n",
            "Current minimum: -0.6999\n",
            "Iteration No: 27 started. Evaluating function at random point.\n",
            "Iteration No: 27 ended. Evaluation done at random point.\n",
            "Time taken: 0.4534\n",
            "Function value obtained: -0.6660\n",
            "Current minimum: -0.6999\n",
            "Iteration No: 28 started. Evaluating function at random point.\n",
            "Iteration No: 28 ended. Evaluation done at random point.\n",
            "Time taken: 0.3436\n",
            "Function value obtained: -0.6714\n",
            "Current minimum: -0.6999\n",
            "Iteration No: 29 started. Evaluating function at random point.\n",
            "Iteration No: 29 ended. Evaluation done at random point.\n",
            "Time taken: 0.5493\n",
            "Function value obtained: -0.6904\n",
            "Current minimum: -0.6999\n",
            "Iteration No: 30 started. Evaluating function at random point.\n",
            "Iteration No: 30 ended. Evaluation done at random point.\n",
            "Time taken: 0.2806\n",
            "Function value obtained: -0.7005\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 31 started. Evaluating function at random point.\n",
            "Iteration No: 31 ended. Evaluation done at random point.\n",
            "Time taken: 0.3596\n",
            "Function value obtained: -0.6293\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 32 started. Evaluating function at random point.\n",
            "Iteration No: 32 ended. Evaluation done at random point.\n",
            "Time taken: 0.3369\n",
            "Function value obtained: -0.6589\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 33 started. Evaluating function at random point.\n",
            "Iteration No: 33 ended. Evaluation done at random point.\n",
            "Time taken: 0.2979\n",
            "Function value obtained: -0.6829\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 34 started. Evaluating function at random point.\n",
            "Iteration No: 34 ended. Evaluation done at random point.\n",
            "Time taken: 0.3692\n",
            "Function value obtained: -0.6103\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 35 started. Evaluating function at random point.\n",
            "Iteration No: 35 ended. Evaluation done at random point.\n",
            "Time taken: 0.2985\n",
            "Function value obtained: -0.6229\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 36 started. Evaluating function at random point.\n",
            "Iteration No: 36 ended. Evaluation done at random point.\n",
            "Time taken: 0.2650\n",
            "Function value obtained: -0.6788\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 37 started. Evaluating function at random point.\n",
            "Iteration No: 37 ended. Evaluation done at random point.\n",
            "Time taken: 0.2929\n",
            "Function value obtained: -0.6641\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 38 started. Evaluating function at random point.\n",
            "Iteration No: 38 ended. Evaluation done at random point.\n",
            "Time taken: 0.5110\n",
            "Function value obtained: -0.6861\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 39 started. Evaluating function at random point.\n",
            "Iteration No: 39 ended. Evaluation done at random point.\n",
            "Time taken: 0.3188\n",
            "Function value obtained: -0.6393\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 40 started. Evaluating function at random point.\n",
            "Iteration No: 40 ended. Evaluation done at random point.\n",
            "Time taken: 0.2928\n",
            "Function value obtained: -0.6777\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 41 started. Evaluating function at random point.\n",
            "Iteration No: 41 ended. Evaluation done at random point.\n",
            "Time taken: 0.4346\n",
            "Function value obtained: -0.6790\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 42 started. Evaluating function at random point.\n",
            "Iteration No: 42 ended. Evaluation done at random point.\n",
            "Time taken: 0.2794\n",
            "Function value obtained: -0.6959\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 43 started. Evaluating function at random point.\n",
            "Iteration No: 43 ended. Evaluation done at random point.\n",
            "Time taken: 0.3177\n",
            "Function value obtained: -0.6827\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 44 started. Evaluating function at random point.\n",
            "Iteration No: 44 ended. Evaluation done at random point.\n",
            "Time taken: 1.0246\n",
            "Function value obtained: -0.6651\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 45 started. Evaluating function at random point.\n",
            "Iteration No: 45 ended. Evaluation done at random point.\n",
            "Time taken: 0.4982\n",
            "Function value obtained: -0.6816\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 46 started. Evaluating function at random point.\n",
            "Iteration No: 46 ended. Evaluation done at random point.\n",
            "Time taken: 0.3814\n",
            "Function value obtained: -0.6951\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 47 started. Evaluating function at random point.\n",
            "Iteration No: 47 ended. Evaluation done at random point.\n",
            "Time taken: 2.5203\n",
            "Function value obtained: -0.6904\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 48 started. Evaluating function at random point.\n",
            "Iteration No: 48 ended. Evaluation done at random point.\n",
            "Time taken: 0.3308\n",
            "Function value obtained: -0.6568\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 49 started. Evaluating function at random point.\n",
            "Iteration No: 49 ended. Evaluation done at random point.\n",
            "Time taken: 0.7276\n",
            "Function value obtained: -0.6836\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 50 started. Evaluating function at random point.\n",
            "Iteration No: 50 ended. Evaluation done at random point.\n",
            "Time taken: 0.3469\n",
            "Function value obtained: -0.6537\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 51 started. Evaluating function at random point.\n",
            "Iteration No: 51 ended. Evaluation done at random point.\n",
            "Time taken: 1.1862\n",
            "Function value obtained: -0.6825\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 52 started. Evaluating function at random point.\n",
            "Iteration No: 52 ended. Evaluation done at random point.\n",
            "Time taken: 0.3254\n",
            "Function value obtained: -0.6727\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 53 started. Evaluating function at random point.\n",
            "Iteration No: 53 ended. Evaluation done at random point.\n",
            "Time taken: 0.5968\n",
            "Function value obtained: -0.6921\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 54 started. Evaluating function at random point.\n",
            "Iteration No: 54 ended. Evaluation done at random point.\n",
            "Time taken: 0.3583\n",
            "Function value obtained: -0.6652\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 55 started. Evaluating function at random point.\n",
            "Iteration No: 55 ended. Evaluation done at random point.\n",
            "Time taken: 0.2681\n",
            "Function value obtained: -0.6562\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 56 started. Evaluating function at random point.\n",
            "Iteration No: 56 ended. Evaluation done at random point.\n",
            "Time taken: 0.3170\n",
            "Function value obtained: -0.6699\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 57 started. Evaluating function at random point.\n",
            "Iteration No: 57 ended. Evaluation done at random point.\n",
            "Time taken: 0.2868\n",
            "Function value obtained: -0.6715\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 58 started. Evaluating function at random point.\n",
            "Iteration No: 58 ended. Evaluation done at random point.\n",
            "Time taken: 0.3706\n",
            "Function value obtained: -0.6804\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 59 started. Evaluating function at random point.\n",
            "Iteration No: 59 ended. Evaluation done at random point.\n",
            "Time taken: 0.3386\n",
            "Function value obtained: -0.6384\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 60 started. Evaluating function at random point.\n",
            "Iteration No: 60 ended. Evaluation done at random point.\n",
            "Time taken: 0.4422\n",
            "Function value obtained: -0.6935\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 61 started. Evaluating function at random point.\n",
            "Iteration No: 61 ended. Evaluation done at random point.\n",
            "Time taken: 0.3207\n",
            "Function value obtained: -0.6535\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 62 started. Evaluating function at random point.\n",
            "Iteration No: 62 ended. Evaluation done at random point.\n",
            "Time taken: 0.3328\n",
            "Function value obtained: -0.6496\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 63 started. Evaluating function at random point.\n",
            "Iteration No: 63 ended. Evaluation done at random point.\n",
            "Time taken: 0.3719\n",
            "Function value obtained: -0.6661\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 64 started. Evaluating function at random point.\n",
            "Iteration No: 64 ended. Evaluation done at random point.\n",
            "Time taken: 0.3504\n",
            "Function value obtained: -0.6534\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 65 started. Evaluating function at random point.\n",
            "Iteration No: 65 ended. Evaluation done at random point.\n",
            "Time taken: 0.3259\n",
            "Function value obtained: -0.6690\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 66 started. Evaluating function at random point.\n",
            "Iteration No: 66 ended. Evaluation done at random point.\n",
            "Time taken: 0.3356\n",
            "Function value obtained: -0.6506\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 67 started. Evaluating function at random point.\n",
            "Iteration No: 67 ended. Evaluation done at random point.\n",
            "Time taken: 0.2880\n",
            "Function value obtained: -0.6558\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 68 started. Evaluating function at random point.\n",
            "Iteration No: 68 ended. Evaluation done at random point.\n",
            "Time taken: 0.2886\n",
            "Function value obtained: -0.6807\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 69 started. Evaluating function at random point.\n",
            "Iteration No: 69 ended. Evaluation done at random point.\n",
            "Time taken: 0.4846\n",
            "Function value obtained: -0.6876\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 70 started. Evaluating function at random point.\n",
            "Iteration No: 70 ended. Evaluation done at random point.\n",
            "Time taken: 0.3404\n",
            "Function value obtained: -0.6586\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 71 started. Evaluating function at random point.\n",
            "Iteration No: 71 ended. Evaluation done at random point.\n",
            "Time taken: 0.3039\n",
            "Function value obtained: -0.6690\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 72 started. Evaluating function at random point.\n",
            "Iteration No: 72 ended. Evaluation done at random point.\n",
            "Time taken: 2.4316\n",
            "Function value obtained: -0.6867\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 73 started. Evaluating function at random point.\n",
            "Iteration No: 73 ended. Evaluation done at random point.\n",
            "Time taken: 0.3416\n",
            "Function value obtained: -0.6679\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 74 started. Evaluating function at random point.\n",
            "Iteration No: 74 ended. Evaluation done at random point.\n",
            "Time taken: 0.2797\n",
            "Function value obtained: -0.6330\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 75 started. Evaluating function at random point.\n",
            "Iteration No: 75 ended. Evaluation done at random point.\n",
            "Time taken: 0.3027\n",
            "Function value obtained: -0.6565\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 76 started. Evaluating function at random point.\n",
            "Iteration No: 76 ended. Evaluation done at random point.\n",
            "Time taken: 0.3393\n",
            "Function value obtained: -0.6494\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 77 started. Evaluating function at random point.\n",
            "Iteration No: 77 ended. Evaluation done at random point.\n",
            "Time taken: 0.7133\n",
            "Function value obtained: -0.6555\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 78 started. Evaluating function at random point.\n",
            "Iteration No: 78 ended. Evaluation done at random point.\n",
            "Time taken: 0.4834\n",
            "Function value obtained: -0.6880\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 79 started. Evaluating function at random point.\n",
            "Iteration No: 79 ended. Evaluation done at random point.\n",
            "Time taken: 0.3707\n",
            "Function value obtained: -0.6712\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 80 started. Evaluating function at random point.\n",
            "Iteration No: 80 ended. Evaluation done at random point.\n",
            "Time taken: 0.3184\n",
            "Function value obtained: -0.6915\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 81 started. Evaluating function at random point.\n",
            "Iteration No: 81 ended. Evaluation done at random point.\n",
            "Time taken: 0.2982\n",
            "Function value obtained: -0.6902\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 82 started. Evaluating function at random point.\n",
            "Iteration No: 82 ended. Evaluation done at random point.\n",
            "Time taken: 0.3235\n",
            "Function value obtained: -0.6638\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 83 started. Evaluating function at random point.\n",
            "Iteration No: 83 ended. Evaluation done at random point.\n",
            "Time taken: 0.4045\n",
            "Function value obtained: -0.6767\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 84 started. Evaluating function at random point.\n",
            "Iteration No: 84 ended. Evaluation done at random point.\n",
            "Time taken: 0.3140\n",
            "Function value obtained: -0.6424\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 85 started. Evaluating function at random point.\n",
            "Iteration No: 85 ended. Evaluation done at random point.\n",
            "Time taken: 0.3893\n",
            "Function value obtained: -0.6546\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 86 started. Evaluating function at random point.\n",
            "Iteration No: 86 ended. Evaluation done at random point.\n",
            "Time taken: 0.3762\n",
            "Function value obtained: -0.6682\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 87 started. Evaluating function at random point.\n",
            "Iteration No: 87 ended. Evaluation done at random point.\n",
            "Time taken: 0.3406\n",
            "Function value obtained: -0.6952\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 88 started. Evaluating function at random point.\n",
            "Iteration No: 88 ended. Evaluation done at random point.\n",
            "Time taken: 0.2750\n",
            "Function value obtained: -0.6918\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 89 started. Evaluating function at random point.\n",
            "Iteration No: 89 ended. Evaluation done at random point.\n",
            "Time taken: 0.3401\n",
            "Function value obtained: -0.6618\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 90 started. Evaluating function at random point.\n",
            "Iteration No: 90 ended. Evaluation done at random point.\n",
            "Time taken: 0.9567\n",
            "Function value obtained: -0.6938\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 91 started. Evaluating function at random point.\n",
            "Iteration No: 91 ended. Evaluation done at random point.\n",
            "Time taken: 0.4899\n",
            "Function value obtained: -0.6912\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 92 started. Evaluating function at random point.\n",
            "Iteration No: 92 ended. Evaluation done at random point.\n",
            "Time taken: 0.3360\n",
            "Function value obtained: -0.6564\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 93 started. Evaluating function at random point.\n",
            "Iteration No: 93 ended. Evaluation done at random point.\n",
            "Time taken: 0.3500\n",
            "Function value obtained: -0.6819\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 94 started. Evaluating function at random point.\n",
            "Iteration No: 94 ended. Evaluation done at random point.\n",
            "Time taken: 0.3482\n",
            "Function value obtained: -0.6737\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 95 started. Evaluating function at random point.\n",
            "Iteration No: 95 ended. Evaluation done at random point.\n",
            "Time taken: 0.3312\n",
            "Function value obtained: -0.6667\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 96 started. Evaluating function at random point.\n",
            "Iteration No: 96 ended. Evaluation done at random point.\n",
            "Time taken: 2.9862\n",
            "Function value obtained: -0.6856\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 97 started. Evaluating function at random point.\n",
            "Iteration No: 97 ended. Evaluation done at random point.\n",
            "Time taken: 0.3472\n",
            "Function value obtained: -0.6410\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 98 started. Evaluating function at random point.\n",
            "Iteration No: 98 ended. Evaluation done at random point.\n",
            "Time taken: 0.3092\n",
            "Function value obtained: -0.6510\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 99 started. Evaluating function at random point.\n",
            "Iteration No: 99 ended. Evaluation done at random point.\n",
            "Time taken: 0.3228\n",
            "Function value obtained: -0.6677\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 100 started. Evaluating function at random point.\n",
            "Iteration No: 100 ended. Evaluation done at random point.\n",
            "Time taken: 0.2688\n",
            "Function value obtained: -0.6918\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 101 started. Evaluating function at random point.\n",
            "Iteration No: 101 ended. Evaluation done at random point.\n",
            "Time taken: 0.3115\n",
            "Function value obtained: -0.6119\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 102 started. Evaluating function at random point.\n",
            "Iteration No: 102 ended. Evaluation done at random point.\n",
            "Time taken: 0.2853\n",
            "Function value obtained: -0.6682\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 103 started. Evaluating function at random point.\n",
            "Iteration No: 103 ended. Evaluation done at random point.\n",
            "Time taken: 0.3112\n",
            "Function value obtained: -0.6590\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 104 started. Evaluating function at random point.\n",
            "Iteration No: 104 ended. Evaluation done at random point.\n",
            "Time taken: 0.2943\n",
            "Function value obtained: -0.6844\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 105 started. Evaluating function at random point.\n",
            "Iteration No: 105 ended. Evaluation done at random point.\n",
            "Time taken: 0.4366\n",
            "Function value obtained: -0.6728\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 106 started. Evaluating function at random point.\n",
            "Iteration No: 106 ended. Evaluation done at random point.\n",
            "Time taken: 0.3179\n",
            "Function value obtained: -0.6576\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 107 started. Evaluating function at random point.\n",
            "Iteration No: 107 ended. Evaluation done at random point.\n",
            "Time taken: 0.3427\n",
            "Function value obtained: -0.6515\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 108 started. Evaluating function at random point.\n",
            "Iteration No: 108 ended. Evaluation done at random point.\n",
            "Time taken: 0.2878\n",
            "Function value obtained: -0.6716\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 109 started. Evaluating function at random point.\n",
            "Iteration No: 109 ended. Evaluation done at random point.\n",
            "Time taken: 0.5224\n",
            "Function value obtained: -0.6815\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 110 started. Evaluating function at random point.\n",
            "Iteration No: 110 ended. Evaluation done at random point.\n",
            "Time taken: 0.3193\n",
            "Function value obtained: -0.6097\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 111 started. Evaluating function at random point.\n",
            "Iteration No: 111 ended. Evaluation done at random point.\n",
            "Time taken: 0.4498\n",
            "Function value obtained: -0.6802\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 112 started. Evaluating function at random point.\n",
            "Iteration No: 112 ended. Evaluation done at random point.\n",
            "Time taken: 0.7840\n",
            "Function value obtained: -0.6834\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 113 started. Evaluating function at random point.\n",
            "Iteration No: 113 ended. Evaluation done at random point.\n",
            "Time taken: 0.5958\n",
            "Function value obtained: -0.6806\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 114 started. Evaluating function at random point.\n",
            "Iteration No: 114 ended. Evaluation done at random point.\n",
            "Time taken: 0.3117\n",
            "Function value obtained: -0.6914\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 115 started. Evaluating function at random point.\n",
            "Iteration No: 115 ended. Evaluation done at random point.\n",
            "Time taken: 0.3164\n",
            "Function value obtained: -0.6548\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 116 started. Evaluating function at random point.\n",
            "Iteration No: 116 ended. Evaluation done at random point.\n",
            "Time taken: 0.3638\n",
            "Function value obtained: -0.6470\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 117 started. Evaluating function at random point.\n",
            "Iteration No: 117 ended. Evaluation done at random point.\n",
            "Time taken: 0.4522\n",
            "Function value obtained: -0.6716\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 118 started. Evaluating function at random point.\n",
            "Iteration No: 118 ended. Evaluation done at random point.\n",
            "Time taken: 0.3191\n",
            "Function value obtained: -0.6385\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 119 started. Evaluating function at random point.\n",
            "Iteration No: 119 ended. Evaluation done at random point.\n",
            "Time taken: 0.3178\n",
            "Function value obtained: -0.6636\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 120 started. Evaluating function at random point.\n",
            "Iteration No: 120 ended. Evaluation done at random point.\n",
            "Time taken: 0.2781\n",
            "Function value obtained: -0.6692\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 121 started. Evaluating function at random point.\n",
            "Iteration No: 121 ended. Evaluation done at random point.\n",
            "Time taken: 0.5290\n",
            "Function value obtained: -0.6854\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 122 started. Evaluating function at random point.\n",
            "Iteration No: 122 ended. Evaluation done at random point.\n",
            "Time taken: 0.3968\n",
            "Function value obtained: -0.6662\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 123 started. Evaluating function at random point.\n",
            "Iteration No: 123 ended. Evaluation done at random point.\n",
            "Time taken: 2.2807\n",
            "Function value obtained: -0.6815\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 124 started. Evaluating function at random point.\n",
            "Iteration No: 124 ended. Evaluation done at random point.\n",
            "Time taken: 0.2672\n",
            "Function value obtained: -0.6714\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 125 started. Evaluating function at random point.\n",
            "Iteration No: 125 ended. Evaluation done at random point.\n",
            "Time taken: 0.3590\n",
            "Function value obtained: -0.6750\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 126 started. Evaluating function at random point.\n",
            "Iteration No: 126 ended. Evaluation done at random point.\n",
            "Time taken: 0.4239\n",
            "Function value obtained: -0.6758\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 127 started. Evaluating function at random point.\n",
            "Iteration No: 127 ended. Evaluation done at random point.\n",
            "Time taken: 0.3125\n",
            "Function value obtained: -0.6803\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 128 started. Evaluating function at random point.\n",
            "Iteration No: 128 ended. Evaluation done at random point.\n",
            "Time taken: 0.3864\n",
            "Function value obtained: -0.6600\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 129 started. Evaluating function at random point.\n",
            "Iteration No: 129 ended. Evaluation done at random point.\n",
            "Time taken: 0.3271\n",
            "Function value obtained: -0.6737\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 130 started. Evaluating function at random point.\n",
            "Iteration No: 130 ended. Evaluation done at random point.\n",
            "Time taken: 0.3449\n",
            "Function value obtained: -0.6283\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 131 started. Evaluating function at random point.\n",
            "Iteration No: 131 ended. Evaluation done at random point.\n",
            "Time taken: 0.2891\n",
            "Function value obtained: -0.6694\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 132 started. Evaluating function at random point.\n",
            "Iteration No: 132 ended. Evaluation done at random point.\n",
            "Time taken: 0.3624\n",
            "Function value obtained: -0.6809\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 133 started. Evaluating function at random point.\n",
            "Iteration No: 133 ended. Evaluation done at random point.\n",
            "Time taken: 0.2793\n",
            "Function value obtained: -0.6168\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 134 started. Evaluating function at random point.\n",
            "Iteration No: 134 ended. Evaluation done at random point.\n",
            "Time taken: 0.3580\n",
            "Function value obtained: -0.6915\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 135 started. Evaluating function at random point.\n",
            "Iteration No: 135 ended. Evaluation done at random point.\n",
            "Time taken: 0.2963\n",
            "Function value obtained: -0.6618\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 136 started. Evaluating function at random point.\n",
            "Iteration No: 136 ended. Evaluation done at random point.\n",
            "Time taken: 0.3338\n",
            "Function value obtained: -0.6496\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 137 started. Evaluating function at random point.\n",
            "Iteration No: 137 ended. Evaluation done at random point.\n",
            "Time taken: 0.3108\n",
            "Function value obtained: -0.6578\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 138 started. Evaluating function at random point.\n",
            "Iteration No: 138 ended. Evaluation done at random point.\n",
            "Time taken: 0.5713\n",
            "Function value obtained: -0.6908\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 139 started. Evaluating function at random point.\n",
            "Iteration No: 139 ended. Evaluation done at random point.\n",
            "Time taken: 0.5511\n",
            "Function value obtained: -0.6558\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 140 started. Evaluating function at random point.\n",
            "Iteration No: 140 ended. Evaluation done at random point.\n",
            "Time taken: 1.6809\n",
            "Function value obtained: -0.6911\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 141 started. Evaluating function at random point.\n",
            "Iteration No: 141 ended. Evaluation done at random point.\n",
            "Time taken: 0.3886\n",
            "Function value obtained: -0.6928\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 142 started. Evaluating function at random point.\n",
            "Iteration No: 142 ended. Evaluation done at random point.\n",
            "Time taken: 0.3565\n",
            "Function value obtained: -0.6626\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 143 started. Evaluating function at random point.\n",
            "Iteration No: 143 ended. Evaluation done at random point.\n",
            "Time taken: 0.3798\n",
            "Function value obtained: -0.6869\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 144 started. Evaluating function at random point.\n",
            "Iteration No: 144 ended. Evaluation done at random point.\n",
            "Time taken: 0.4148\n",
            "Function value obtained: -0.6695\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 145 started. Evaluating function at random point.\n",
            "Iteration No: 145 ended. Evaluation done at random point.\n",
            "Time taken: 0.3408\n",
            "Function value obtained: -0.6612\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 146 started. Evaluating function at random point.\n",
            "Iteration No: 146 ended. Evaluation done at random point.\n",
            "Time taken: 0.2918\n",
            "Function value obtained: -0.6651\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 147 started. Evaluating function at random point.\n",
            "Iteration No: 147 ended. Evaluation done at random point.\n",
            "Time taken: 0.4119\n",
            "Function value obtained: -0.6926\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 148 started. Evaluating function at random point.\n",
            "Iteration No: 148 ended. Evaluation done at random point.\n",
            "Time taken: 2.2650\n",
            "Function value obtained: -0.6788\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 149 started. Evaluating function at random point.\n",
            "Iteration No: 149 ended. Evaluation done at random point.\n",
            "Time taken: 0.3363\n",
            "Function value obtained: -0.6924\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 150 started. Evaluating function at random point.\n",
            "Iteration No: 150 ended. Evaluation done at random point.\n",
            "Time taken: 0.3048\n",
            "Function value obtained: -0.6634\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 151 started. Evaluating function at random point.\n",
            "Iteration No: 151 ended. Evaluation done at random point.\n",
            "Time taken: 0.3136\n",
            "Function value obtained: -0.6853\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 152 started. Evaluating function at random point.\n",
            "Iteration No: 152 ended. Evaluation done at random point.\n",
            "Time taken: 0.5178\n",
            "Function value obtained: -0.6988\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 153 started. Evaluating function at random point.\n",
            "Iteration No: 153 ended. Evaluation done at random point.\n",
            "Time taken: 0.5569\n",
            "Function value obtained: -0.6836\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 154 started. Evaluating function at random point.\n",
            "Iteration No: 154 ended. Evaluation done at random point.\n",
            "Time taken: 0.2882\n",
            "Function value obtained: -0.6589\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 155 started. Evaluating function at random point.\n",
            "Iteration No: 155 ended. Evaluation done at random point.\n",
            "Time taken: 0.7481\n",
            "Function value obtained: -0.6805\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 156 started. Evaluating function at random point.\n",
            "Iteration No: 156 ended. Evaluation done at random point.\n",
            "Time taken: 0.4327\n",
            "Function value obtained: -0.6656\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 157 started. Evaluating function at random point.\n",
            "Iteration No: 157 ended. Evaluation done at random point.\n",
            "Time taken: 0.3508\n",
            "Function value obtained: -0.6477\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 158 started. Evaluating function at random point.\n",
            "Iteration No: 158 ended. Evaluation done at random point.\n",
            "Time taken: 0.4488\n",
            "Function value obtained: -0.6801\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 159 started. Evaluating function at random point.\n",
            "Iteration No: 159 ended. Evaluation done at random point.\n",
            "Time taken: 0.3647\n",
            "Function value obtained: -0.6734\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 160 started. Evaluating function at random point.\n",
            "Iteration No: 160 ended. Evaluation done at random point.\n",
            "Time taken: 0.2981\n",
            "Function value obtained: -0.6101\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 161 started. Evaluating function at random point.\n",
            "Iteration No: 161 ended. Evaluation done at random point.\n",
            "Time taken: 0.2991\n",
            "Function value obtained: -0.6470\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 162 started. Evaluating function at random point.\n",
            "Iteration No: 162 ended. Evaluation done at random point.\n",
            "Time taken: 0.3358\n",
            "Function value obtained: -0.6510\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 163 started. Evaluating function at random point.\n",
            "Iteration No: 163 ended. Evaluation done at random point.\n",
            "Time taken: 0.3363\n",
            "Function value obtained: -0.6928\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 164 started. Evaluating function at random point.\n",
            "Iteration No: 164 ended. Evaluation done at random point.\n",
            "Time taken: 0.3446\n",
            "Function value obtained: -0.6416\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 165 started. Evaluating function at random point.\n",
            "Iteration No: 165 ended. Evaluation done at random point.\n",
            "Time taken: 0.4380\n",
            "Function value obtained: -0.6855\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 166 started. Evaluating function at random point.\n",
            "Iteration No: 166 ended. Evaluation done at random point.\n",
            "Time taken: 0.3548\n",
            "Function value obtained: -0.6321\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 167 started. Evaluating function at random point.\n",
            "Iteration No: 167 ended. Evaluation done at random point.\n",
            "Time taken: 0.3092\n",
            "Function value obtained: -0.6566\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 168 started. Evaluating function at random point.\n",
            "Iteration No: 168 ended. Evaluation done at random point.\n",
            "Time taken: 0.2927\n",
            "Function value obtained: -0.6652\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 169 started. Evaluating function at random point.\n",
            "Iteration No: 169 ended. Evaluation done at random point.\n",
            "Time taken: 0.3234\n",
            "Function value obtained: -0.6440\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 170 started. Evaluating function at random point.\n",
            "Iteration No: 170 ended. Evaluation done at random point.\n",
            "Time taken: 0.3430\n",
            "Function value obtained: -0.6817\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 171 started. Evaluating function at random point.\n",
            "Iteration No: 171 ended. Evaluation done at random point.\n",
            "Time taken: 0.3603\n",
            "Function value obtained: -0.6916\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 172 started. Evaluating function at random point.\n",
            "Iteration No: 172 ended. Evaluation done at random point.\n",
            "Time taken: 0.3393\n",
            "Function value obtained: -0.6665\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 173 started. Evaluating function at random point.\n",
            "Iteration No: 173 ended. Evaluation done at random point.\n",
            "Time taken: 0.3068\n",
            "Function value obtained: -0.6649\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 174 started. Evaluating function at random point.\n",
            "Iteration No: 174 ended. Evaluation done at random point.\n",
            "Time taken: 0.3831\n",
            "Function value obtained: -0.6485\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 175 started. Evaluating function at random point.\n",
            "Iteration No: 175 ended. Evaluation done at random point.\n",
            "Time taken: 2.3757\n",
            "Function value obtained: -0.6768\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 176 started. Evaluating function at random point.\n",
            "Iteration No: 176 ended. Evaluation done at random point.\n",
            "Time taken: 0.3181\n",
            "Function value obtained: -0.6439\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 177 started. Evaluating function at random point.\n",
            "Iteration No: 177 ended. Evaluation done at random point.\n",
            "Time taken: 0.3527\n",
            "Function value obtained: -0.6743\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 178 started. Evaluating function at random point.\n",
            "Iteration No: 178 ended. Evaluation done at random point.\n",
            "Time taken: 0.3051\n",
            "Function value obtained: -0.6770\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 179 started. Evaluating function at random point.\n",
            "Iteration No: 179 ended. Evaluation done at random point.\n",
            "Time taken: 0.3078\n",
            "Function value obtained: -0.6513\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 180 started. Evaluating function at random point.\n",
            "Iteration No: 180 ended. Evaluation done at random point.\n",
            "Time taken: 0.4198\n",
            "Function value obtained: -0.6758\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 181 started. Evaluating function at random point.\n",
            "Iteration No: 181 ended. Evaluation done at random point.\n",
            "Time taken: 0.3701\n",
            "Function value obtained: -0.6669\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 182 started. Evaluating function at random point.\n",
            "Iteration No: 182 ended. Evaluation done at random point.\n",
            "Time taken: 0.3087\n",
            "Function value obtained: -0.6400\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 183 started. Evaluating function at random point.\n",
            "Iteration No: 183 ended. Evaluation done at random point.\n",
            "Time taken: 0.3061\n",
            "Function value obtained: -0.6334\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 184 started. Evaluating function at random point.\n",
            "Iteration No: 184 ended. Evaluation done at random point.\n",
            "Time taken: 0.3396\n",
            "Function value obtained: -0.6551\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 185 started. Evaluating function at random point.\n",
            "Iteration No: 185 ended. Evaluation done at random point.\n",
            "Time taken: 0.3391\n",
            "Function value obtained: -0.6775\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 186 started. Evaluating function at random point.\n",
            "Iteration No: 186 ended. Evaluation done at random point.\n",
            "Time taken: 0.3988\n",
            "Function value obtained: -0.6813\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 187 started. Evaluating function at random point.\n",
            "Iteration No: 187 ended. Evaluation done at random point.\n",
            "Time taken: 0.3869\n",
            "Function value obtained: -0.6982\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 188 started. Evaluating function at random point.\n",
            "Iteration No: 188 ended. Evaluation done at random point.\n",
            "Time taken: 0.3505\n",
            "Function value obtained: -0.6567\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 189 started. Evaluating function at random point.\n",
            "Iteration No: 189 ended. Evaluation done at random point.\n",
            "Time taken: 0.4481\n",
            "Function value obtained: -0.6957\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 190 started. Evaluating function at random point.\n",
            "Iteration No: 190 ended. Evaluation done at random point.\n",
            "Time taken: 0.3306\n",
            "Function value obtained: -0.6403\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 191 started. Evaluating function at random point.\n",
            "Iteration No: 191 ended. Evaluation done at random point.\n",
            "Time taken: 0.3487\n",
            "Function value obtained: -0.6637\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 192 started. Evaluating function at random point.\n",
            "Iteration No: 192 ended. Evaluation done at random point.\n",
            "Time taken: 0.3497\n",
            "Function value obtained: -0.6484\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 193 started. Evaluating function at random point.\n",
            "Iteration No: 193 ended. Evaluation done at random point.\n",
            "Time taken: 0.2893\n",
            "Function value obtained: -0.6731\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 194 started. Evaluating function at random point.\n",
            "Iteration No: 194 ended. Evaluation done at random point.\n",
            "Time taken: 0.3738\n",
            "Function value obtained: -0.6768\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 195 started. Evaluating function at random point.\n",
            "Iteration No: 195 ended. Evaluation done at random point.\n",
            "Time taken: 0.3258\n",
            "Function value obtained: -0.6589\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 196 started. Evaluating function at random point.\n",
            "Iteration No: 196 ended. Evaluation done at random point.\n",
            "Time taken: 0.2854\n",
            "Function value obtained: -0.6888\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 197 started. Evaluating function at random point.\n",
            "Iteration No: 197 ended. Evaluation done at random point.\n",
            "Time taken: 0.2954\n",
            "Function value obtained: -0.6685\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 198 started. Evaluating function at random point.\n",
            "Iteration No: 198 ended. Evaluation done at random point.\n",
            "Time taken: 0.5523\n",
            "Function value obtained: -0.6832\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 199 started. Evaluating function at random point.\n",
            "Iteration No: 199 ended. Evaluation done at random point.\n",
            "Time taken: 0.3377\n",
            "Function value obtained: -0.6672\n",
            "Current minimum: -0.7005\n",
            "Iteration No: 200 started. Evaluating function at random point.\n",
            "Iteration No: 200 ended. Evaluation done at random point.\n",
            "Time taken: 4.1246\n",
            "Function value obtained: -0.6736\n",
            "Current minimum: -0.7005\n",
            "{'max_depth': 3, 'n_estimators': 338, 'learning_rate': 0.5439505741205511, 'subsample': 0.602219936251845, 'colsample_bytree': 0.44007150842579157, 'min_child_weight': 6.672622110826326}\n",
            "Best ROC-AUC Score: 0.7005147947982265\n"
          ]
        }
      ],
      "source": [
        "X = train_df.drop(['TenYearCHD_1'], axis = 1)\n",
        "y = train_df['TenYearCHD_1']\n",
        "X = X.reset_index(drop=True)\n",
        "\n",
        "def optimize(params, X, y):\n",
        "  params = dict(zip(param_names, params))\n",
        "  model = lgb.LGBMClassifier(**params)\n",
        "  cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
        "  scores = []\n",
        "  for train_idx, val_idx in cv.split(X, y):\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "    model.fit(X_train, y_train, early_stopping_rounds=10,\n",
        "              eval_set=[(X_val, y_val)], verbose=False)\n",
        "    y_pred = model.predict_proba(X_val)[:, 1]\n",
        "    score = roc_auc_score(y_val, y_pred)\n",
        "    scores.append(score)\n",
        "  return -1.0 * np.mean(scores)\n",
        "# Define the parameter space and names\n",
        "param_space = [\n",
        "    space.Integer(3, 15, name=\"max_depth\"),\n",
        "    space.Integer(100, 600, name=\"n_estimators\"),\n",
        "    space.Real(0.01, 1, prior=\"uniform\", name=\"learning_rate\"),\n",
        "    space.Real(0.1, 1, prior=\"uniform\", name=\"subsample\"),\n",
        "    space.Real(0.1, 1, prior=\"uniform\", name=\"colsample_bytree\"),\n",
        "    space.Real(0.1, 10, prior=\"uniform\", name=\"min_child_weight\")\n",
        "    ]\n",
        "param_names = [\"max_depth\", \"n_estimators\", \"learning_rate\", \"subsample\", \"colsample_bytree\", \"min_child_weight\"]\n",
        "\n",
        "# Define the optimization function with partial arguments\n",
        "optimization_function = partial(optimize, X=X, y=y)\n",
        "\n",
        "# Run the optimization using gp_minimize\n",
        "result = gp_minimize(\n",
        "    optimization_function,\n",
        "    dimensions=param_space,\n",
        "    n_calls=200,\n",
        "    n_random_starts=200,\n",
        "    verbose=10,\n",
        "    acq_func='EI'\n",
        ")\n",
        "# Print the best hyperparameters found\n",
        "print(dict(zip(param_names, result.x)))\n",
        "print(f'Best ROC-AUC Score: {-1.0 * result.fun}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePgQ5wtZJOLO",
        "outputId": "8449972c-fb37-4a62-e5eb-bc09ba47ee81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.6730429464528404\n"
          ]
        }
      ],
      "source": [
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Assuming you have your training and evaluation data as X_train, y_train, X_eval, y_eval, and test data as X_test, y_test\n",
        "\n",
        "# Define the parameters\n",
        "params = {'max_depth': 3,\n",
        "          'n_estimators': 173,\n",
        "          'learning_rate': 0.26658391864937014,\n",
        "          'subsample': 0.27417395433428393,\n",
        "          'colsample_bytree': 0.37678704205444047,\n",
        "          'min_child_weight': 8.827872937189104}\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model = lgb.LGBMClassifier(**params)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66__fQVANOQB"
      },
      "source": [
        "Optuna TPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbZhbTEwNQph"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OvrGjrDLhi9",
        "outputId": "f3377244-d318-4b12-ae50-1d379e28dc38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-25 13:59:57,570]\u001b[0m A new study created in memory with name: no-name-c48fd018-681c-416b-abb7-3fe0a251acf4\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 13:59:59,531]\u001b[0m Trial 1 finished with value: -0.6601684318970503 and parameters: {'max_depth': 8, 'n_estimators': 174, 'learning_rate': 0.7878747640239373, 'subsample': 0.2592273280165252, 'colsample_bytree': 0.8806143626087539, 'min_child_weight': 2.2382901801725468}. Best is trial 1 with value: -0.6601684318970503.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 13:59:59,961]\u001b[0m Trial 0 finished with value: -0.6653024990360518 and parameters: {'max_depth': 9, 'n_estimators': 470, 'learning_rate': 0.48493139233540294, 'subsample': 0.8314171009100851, 'colsample_bytree': 0.2605551341091043, 'min_child_weight': 8.29175647100937}. Best is trial 1 with value: -0.6601684318970503.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:01,207]\u001b[0m Trial 2 finished with value: -0.606153344097423 and parameters: {'max_depth': 7, 'n_estimators': 476, 'learning_rate': 0.8653694779858707, 'subsample': 0.4874937843909558, 'colsample_bytree': 0.30160130416767483, 'min_child_weight': 2.512908512656384}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:01,488]\u001b[0m Trial 3 finished with value: -0.6705345876711009 and parameters: {'max_depth': 6, 'n_estimators': 177, 'learning_rate': 0.28433606386207827, 'subsample': 0.23100824359582828, 'colsample_bytree': 0.16221279984006548, 'min_child_weight': 1.7096491939653253}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:02,721]\u001b[0m Trial 4 finished with value: -0.6637283834586467 and parameters: {'max_depth': 10, 'n_estimators': 255, 'learning_rate': 0.3811011205636433, 'subsample': 0.5114159428233332, 'colsample_bytree': 0.8205810645567347, 'min_child_weight': 4.653815776289996}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:02,927]\u001b[0m Trial 5 finished with value: -0.6427290521978022 and parameters: {'max_depth': 13, 'n_estimators': 396, 'learning_rate': 0.7718232968256058, 'subsample': 0.4749476633175279, 'colsample_bytree': 0.6885146925927513, 'min_child_weight': 0.8107430710050887}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:04,106]\u001b[0m Trial 6 finished with value: -0.6559730415783047 and parameters: {'max_depth': 11, 'n_estimators': 125, 'learning_rate': 0.9292623539510895, 'subsample': 0.902812766307889, 'colsample_bytree': 0.3965748566739913, 'min_child_weight': 8.102669235513213}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:04,497]\u001b[0m Trial 7 finished with value: -0.7009083104395605 and parameters: {'max_depth': 4, 'n_estimators': 346, 'learning_rate': 0.15567864891248673, 'subsample': 0.4242133946276142, 'colsample_bytree': 0.9419914036065283, 'min_child_weight': 4.335186119159153}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:05,858]\u001b[0m Trial 8 finished with value: -0.6560134089229483 and parameters: {'max_depth': 9, 'n_estimators': 365, 'learning_rate': 0.6897363291601222, 'subsample': 0.27683405782246573, 'colsample_bytree': 0.4599587069916137, 'min_child_weight': 4.762877523219783}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:07,191]\u001b[0m Trial 10 finished with value: -0.6857284015326779 and parameters: {'max_depth': 8, 'n_estimators': 316, 'learning_rate': 0.39250503723772284, 'subsample': 0.3066150643956454, 'colsample_bytree': 0.997807818395732, 'min_child_weight': 6.956239826426006}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:07,604]\u001b[0m Trial 9 finished with value: -0.6813214888181993 and parameters: {'max_depth': 14, 'n_estimators': 367, 'learning_rate': 0.1268728764433749, 'subsample': 0.6088950824793922, 'colsample_bytree': 0.34468875662400944, 'min_child_weight': 3.1435828818239897}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:08,453]\u001b[0m Trial 11 finished with value: -0.6823908167855536 and parameters: {'max_depth': 3, 'n_estimators': 581, 'learning_rate': 0.9777119722594815, 'subsample': 0.6810898340671395, 'colsample_bytree': 0.5913837933762606, 'min_child_weight': 9.987658559551726}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:10,085]\u001b[0m Trial 12 finished with value: -0.6293238384422594 and parameters: {'max_depth': 15, 'n_estimators': 590, 'learning_rate': 0.9868809553782574, 'subsample': 0.6525743276920785, 'colsample_bytree': 0.5936767095295298, 'min_child_weight': 0.5752978442451306}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:11,302]\u001b[0m Trial 13 finished with value: -0.6454233721322536 and parameters: {'max_depth': 15, 'n_estimators': 494, 'learning_rate': 0.6924779094755973, 'subsample': 0.4282835135219308, 'colsample_bytree': 0.6233470209924832, 'min_child_weight': 0.1253579630030366}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:12,925]\u001b[0m Trial 14 finished with value: -0.6316801458775141 and parameters: {'max_depth': 15, 'n_estimators': 593, 'learning_rate': 0.9961993689621702, 'subsample': 0.7432286192999533, 'colsample_bytree': 0.5738072942328302, 'min_child_weight': 0.20776974838964074}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:13,652]\u001b[0m Trial 15 finished with value: -0.6414094792269135 and parameters: {'max_depth': 6, 'n_estimators': 576, 'learning_rate': 0.9997741236843922, 'subsample': 0.12552573052115357, 'colsample_bytree': 0.4740248700407915, 'min_child_weight': 2.7018773797745244}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:14,878]\u001b[0m Trial 16 finished with value: -0.6708180868838765 and parameters: {'max_depth': 6, 'n_estimators': 503, 'learning_rate': 0.8615950072624128, 'subsample': 0.5726621479878267, 'colsample_bytree': 0.140247309488773, 'min_child_weight': 2.3718285492340043}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:15,456]\u001b[0m Trial 17 finished with value: -0.6660788931463274 and parameters: {'max_depth': 12, 'n_estimators': 481, 'learning_rate': 0.8581713691962011, 'subsample': 0.6054513605961733, 'colsample_bytree': 0.11186650896939118, 'min_child_weight': 1.316228314365317}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:16,726]\u001b[0m Trial 18 finished with value: -0.6091064178876678 and parameters: {'max_depth': 12, 'n_estimators': 442, 'learning_rate': 0.5863758491281139, 'subsample': 0.9973252161776793, 'colsample_bytree': 0.2596847823686792, 'min_child_weight': 1.335071360163127}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:17,312]\u001b[0m Trial 19 finished with value: -0.6541131635177687 and parameters: {'max_depth': 5, 'n_estimators': 436, 'learning_rate': 0.6300453825353948, 'subsample': 0.9685489255282134, 'colsample_bytree': 0.28585364249192813, 'min_child_weight': 3.1194407205298766}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:18,820]\u001b[0m Trial 20 finished with value: -0.6375398813540262 and parameters: {'max_depth': 12, 'n_estimators': 427, 'learning_rate': 0.6161001202843128, 'subsample': 0.7839072299025269, 'colsample_bytree': 0.26175008172115866, 'min_child_weight': 3.6422610669541733}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:19,118]\u001b[0m Trial 21 finished with value: -0.6522553077405052 and parameters: {'max_depth': 11, 'n_estimators': 293, 'learning_rate': 0.5930313849533037, 'subsample': 0.7964389400164673, 'colsample_bytree': 0.22831409270956327, 'min_child_weight': 3.7377421242559716}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:20,543]\u001b[0m Trial 22 finished with value: -0.6294605182828867 and parameters: {'max_depth': 14, 'n_estimators': 541, 'learning_rate': 0.8910377087680518, 'subsample': 0.9760609141618425, 'colsample_bytree': 0.49636325858734676, 'min_child_weight': 1.2734255587849033}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:20,920]\u001b[0m Trial 23 finished with value: -0.6355235042735043 and parameters: {'max_depth': 13, 'n_estimators': 551, 'learning_rate': 0.8708616166858139, 'subsample': 0.9693748465299116, 'colsample_bytree': 0.3914680081704728, 'min_child_weight': 1.9146479491636972}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:22,186]\u001b[0m Trial 25 finished with value: -0.6383271881627145 and parameters: {'max_depth': 7, 'n_estimators': 527, 'learning_rate': 0.7743559778956157, 'subsample': 0.6753328556987281, 'colsample_bytree': 0.3648641399553749, 'min_child_weight': 0.9221559704790308}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:22,256]\u001b[0m Trial 24 finished with value: -0.6358110500610501 and parameters: {'max_depth': 13, 'n_estimators': 525, 'learning_rate': 0.7957621603133604, 'subsample': 0.8865699386360528, 'colsample_bytree': 0.38778208653321367, 'min_child_weight': 1.8737628932600146}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:23,641]\u001b[0m Trial 27 finished with value: -0.6415179173896279 and parameters: {'max_depth': 10, 'n_estimators': 432, 'learning_rate': 0.9289544700197829, 'subsample': 0.6477731481745871, 'colsample_bytree': 0.6770548326658024, 'min_child_weight': 0.7443071139719747}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:23,805]\u001b[0m Trial 26 finished with value: -0.6416019857335649 and parameters: {'max_depth': 14, 'n_estimators': 431, 'learning_rate': 0.9193983909087677, 'subsample': 0.6913592813162229, 'colsample_bytree': 0.19450085327145583, 'min_child_weight': 0.625045140566336}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:26,309]\u001b[0m Trial 28 finished with value: -0.6502154986826039 and parameters: {'max_depth': 15, 'n_estimators': 456, 'learning_rate': 0.5235621023499151, 'subsample': 0.5479438015023459, 'colsample_bytree': 0.19830724393098947, 'min_child_weight': 2.4099861941780634}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:26,772]\u001b[0m Trial 29 finished with value: -0.6328163999742948 and parameters: {'max_depth': 12, 'n_estimators': 463, 'learning_rate': 0.509228082016972, 'subsample': 0.5577072519577808, 'colsample_bytree': 0.30472224842198153, 'min_child_weight': 1.4368579579606542}. Best is trial 2 with value: -0.606153344097423.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:29,401]\u001b[0m Trial 30 finished with value: -0.603347360789795 and parameters: {'max_depth': 8, 'n_estimators': 470, 'learning_rate': 0.6958746433632244, 'subsample': 0.8319126547756472, 'colsample_bytree': 0.27759378254730477, 'min_child_weight': 1.513954125410232}. Best is trial 30 with value: -0.603347360789795.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:29,643]\u001b[0m Trial 31 finished with value: -0.6401000437793201 and parameters: {'max_depth': 8, 'n_estimators': 507, 'learning_rate': 0.6934324021245835, 'subsample': 0.8342454833151112, 'colsample_bytree': 0.23911981689689807, 'min_child_weight': 0.15952480773195443}. Best is trial 30 with value: -0.603347360789795.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:30,490]\u001b[0m Trial 32 finished with value: -0.6403738492866783 and parameters: {'max_depth': 8, 'n_estimators': 503, 'learning_rate': 0.6800417249561728, 'subsample': 0.8348761223249737, 'colsample_bytree': 0.24555821960752305, 'min_child_weight': 1.5475590019546748}. Best is trial 30 with value: -0.603347360789795.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:30,667]\u001b[0m Trial 33 finished with value: -0.599523152834008 and parameters: {'max_depth': 9, 'n_estimators': 405, 'learning_rate': 0.7384747734189614, 'subsample': 0.8459259369583418, 'colsample_bytree': 0.3084471400393822, 'min_child_weight': 1.9112764442848795}. Best is trial 33 with value: -0.599523152834008.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:31,393]\u001b[0m Trial 34 finished with value: -0.5960217410513463 and parameters: {'max_depth': 9, 'n_estimators': 388, 'learning_rate': 0.8093049167318908, 'subsample': 0.7346069038395057, 'colsample_bytree': 0.33112248677535816, 'min_child_weight': 1.9975612441248505}. Best is trial 34 with value: -0.5960217410513463.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:31,513]\u001b[0m Trial 35 finished with value: -0.5905105592506907 and parameters: {'max_depth': 9, 'n_estimators': 398, 'learning_rate': 0.8061480826906051, 'subsample': 0.9056574160809344, 'colsample_bytree': 0.31785460611329325, 'min_child_weight': 2.080661858251143}. Best is trial 35 with value: -0.5905105592506907.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:32,273]\u001b[0m Trial 36 finished with value: -0.590018059973652 and parameters: {'max_depth': 9, 'n_estimators': 396, 'learning_rate': 0.8090136279153792, 'subsample': 0.7377480343235443, 'colsample_bytree': 0.31831184050690886, 'min_child_weight': 2.0603118228575186}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:32,456]\u001b[0m Trial 37 finished with value: -0.6001129285553628 and parameters: {'max_depth': 9, 'n_estimators': 403, 'learning_rate': 0.7702810960612924, 'subsample': 0.7378485695096323, 'colsample_bytree': 0.31448401324815095, 'min_child_weight': 1.986537969171753}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:33,130]\u001b[0m Trial 38 finished with value: -0.6078355765214317 and parameters: {'max_depth': 9, 'n_estimators': 396, 'learning_rate': 0.8234507328819555, 'subsample': 0.744059062907755, 'colsample_bytree': 0.3287710476450284, 'min_child_weight': 2.8246982516158825}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:33,262]\u001b[0m Trial 39 finished with value: -0.6394309732664996 and parameters: {'max_depth': 10, 'n_estimators': 289, 'learning_rate': 0.8255197281072756, 'subsample': 0.8945409238888677, 'colsample_bytree': 0.4351173668643722, 'min_child_weight': 2.8605953484166524}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:34,004]\u001b[0m Trial 40 finished with value: -0.6356284180001285 and parameters: {'max_depth': 10, 'n_estimators': 328, 'learning_rate': 0.7423388884409593, 'subsample': 0.8959211089942962, 'colsample_bytree': 0.41752745220358645, 'min_child_weight': 2.5475936368891006}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:34,087]\u001b[0m Trial 41 finished with value: -0.640333654649444 and parameters: {'max_depth': 7, 'n_estimators': 348, 'learning_rate': 0.7471147924219053, 'subsample': 0.9228645605247824, 'colsample_bytree': 0.4278410875999027, 'min_child_weight': 2.278331656870645}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:34,930]\u001b[0m Trial 43 finished with value: -0.5961551474840949 and parameters: {'max_depth': 9, 'n_estimators': 403, 'learning_rate': 0.8107782552650606, 'subsample': 0.7173097159619768, 'colsample_bytree': 0.3324381070338717, 'min_child_weight': 1.9950506261040593}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:34,999]\u001b[0m Trial 42 finished with value: -0.6037686785071653 and parameters: {'max_depth': 9, 'n_estimators': 401, 'learning_rate': 0.7497988184554569, 'subsample': 0.7424419711260591, 'colsample_bytree': 0.33539100717772136, 'min_child_weight': 2.066992620163416}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:35,692]\u001b[0m Trial 44 finished with value: -0.5901350351037851 and parameters: {'max_depth': 9, 'n_estimators': 386, 'learning_rate': 0.8149688683464456, 'subsample': 0.7877927948660735, 'colsample_bytree': 0.345280891797617, 'min_child_weight': 2.03860781724542}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:35,946]\u001b[0m Trial 45 finished with value: -0.610952589807853 and parameters: {'max_depth': 11, 'n_estimators': 375, 'learning_rate': 0.8161511578152651, 'subsample': 0.8003089312802093, 'colsample_bytree': 0.33889810520246305, 'min_child_weight': 3.385789469758734}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:36,686]\u001b[0m Trial 46 finished with value: -0.640361850138166 and parameters: {'max_depth': 11, 'n_estimators': 368, 'learning_rate': 0.8263503062076638, 'subsample': 0.7834590070837252, 'colsample_bytree': 0.36800977464142126, 'min_child_weight': 3.4652655791660543}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:36,957]\u001b[0m Trial 47 finished with value: -0.6469871995694363 and parameters: {'max_depth': 10, 'n_estimators': 257, 'learning_rate': 0.9018718131562912, 'subsample': 0.7149813057967744, 'colsample_bytree': 0.3713214434903307, 'min_child_weight': 4.180978344661683}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:37,728]\u001b[0m Trial 48 finished with value: -0.6411121353062142 and parameters: {'max_depth': 7, 'n_estimators': 239, 'learning_rate': 0.9439996093322935, 'subsample': 0.8648114532451961, 'colsample_bytree': 0.5268453181330155, 'min_child_weight': 3.9762631372932216}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:37,938]\u001b[0m Trial 49 finished with value: -0.6342671542638648 and parameters: {'max_depth': 7, 'n_estimators': 315, 'learning_rate': 0.94651498977103, 'subsample': 0.7104462738372604, 'colsample_bytree': 0.5178040263074184, 'min_child_weight': 1.1043294869728095}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:38,467]\u001b[0m Trial 50 finished with value: -0.6600749570239702 and parameters: {'max_depth': 8, 'n_estimators': 381, 'learning_rate': 0.8594393909723916, 'subsample': 0.771289155869147, 'colsample_bytree': 0.1888757748627462, 'min_child_weight': 5.126383209966757}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:38,913]\u001b[0m Trial 51 finished with value: -0.6322901604973974 and parameters: {'max_depth': 8, 'n_estimators': 381, 'learning_rate': 0.8546631113665667, 'subsample': 0.7573601673454444, 'colsample_bytree': 0.46967448072676793, 'min_child_weight': 3.094834220659287}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:39,437]\u001b[0m Trial 52 finished with value: -0.5905964289730737 and parameters: {'max_depth': 9, 'n_estimators': 412, 'learning_rate': 0.8016412189193388, 'subsample': 0.8162564189210257, 'colsample_bytree': 0.29929954844369255, 'min_child_weight': 1.7094423437793846}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:39,864]\u001b[0m Trial 53 finished with value: -0.5905791200758306 and parameters: {'max_depth': 9, 'n_estimators': 417, 'learning_rate': 0.8044473570602975, 'subsample': 0.8483573615192853, 'colsample_bytree': 0.3071854748343974, 'min_child_weight': 1.738027056573094}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:40,459]\u001b[0m Trial 54 finished with value: -0.5927184126180837 and parameters: {'max_depth': 9, 'n_estimators': 340, 'learning_rate': 0.7902705353326668, 'subsample': 0.856033587249233, 'colsample_bytree': 0.3485362490623122, 'min_child_weight': 2.307767730121474}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:41,748]\u001b[0m Trial 55 finished with value: -0.6111928639708245 and parameters: {'max_depth': 10, 'n_estimators': 338, 'learning_rate': 0.786493003621594, 'subsample': 0.9289864185589762, 'colsample_bytree': 0.2877459137861807, 'min_child_weight': 1.637382417465576}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:42,414]\u001b[0m Trial 56 finished with value: -0.5998656999068184 and parameters: {'max_depth': 10, 'n_estimators': 334, 'learning_rate': 0.8883709065839481, 'subsample': 0.930561930607055, 'colsample_bytree': 0.2779813004109594, 'min_child_weight': 1.0971230782973522}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:43,581]\u001b[0m Trial 57 finished with value: -0.5951042791273055 and parameters: {'max_depth': 11, 'n_estimators': 357, 'learning_rate': 0.8704409886978338, 'subsample': 0.8737669476668436, 'colsample_bytree': 0.27508990821841195, 'min_child_weight': 0.903349622231062}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:43,980]\u001b[0m Trial 58 finished with value: -0.6456526954405243 and parameters: {'max_depth': 6, 'n_estimators': 357, 'learning_rate': 0.7250749636060707, 'subsample': 0.8693434191422875, 'colsample_bytree': 0.3927677390366167, 'min_child_weight': 2.497145257271081}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:44,790]\u001b[0m Trial 59 finished with value: -0.648528514796607 and parameters: {'max_depth': 9, 'n_estimators': 305, 'learning_rate': 0.7181428357585938, 'subsample': 0.788836450215366, 'colsample_bytree': 0.21595354911520526, 'min_child_weight': 2.531689356486539}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:45,241]\u001b[0m Trial 60 finished with value: -0.6462064516258594 and parameters: {'max_depth': 9, 'n_estimators': 116, 'learning_rate': 0.6475074295222663, 'subsample': 0.8062157603298294, 'colsample_bytree': 0.23499868820811873, 'min_child_weight': 2.317464693060411}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:45,681]\u001b[0m Trial 61 finished with value: -0.6434579959514171 and parameters: {'max_depth': 8, 'n_estimators': 413, 'learning_rate': 0.6550744402638031, 'subsample': 0.8112832125130014, 'colsample_bytree': 0.1705776795482898, 'min_child_weight': 0.5822793607888068}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:46,083]\u001b[0m Trial 62 finished with value: -0.6403020732922048 and parameters: {'max_depth': 8, 'n_estimators': 417, 'learning_rate': 0.771864171871883, 'subsample': 0.8822092199949481, 'colsample_bytree': 0.15686999859964235, 'min_child_weight': 0.9082423216145588}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:46,519]\u001b[0m Trial 63 finished with value: -0.5919602210654843 and parameters: {'max_depth': 10, 'n_estimators': 448, 'learning_rate': 0.8501994431289184, 'subsample': 0.8737939712024403, 'colsample_bytree': 0.25989679854004094, 'min_child_weight': 1.6939996196921558}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:46,848]\u001b[0m Trial 64 finished with value: -0.6057813986087013 and parameters: {'max_depth': 11, 'n_estimators': 452, 'learning_rate': 0.8952531283822046, 'subsample': 0.8498289251192358, 'colsample_bytree': 0.2615927731556653, 'min_child_weight': 1.6343023461882022}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:47,436]\u001b[0m Trial 65 finished with value: -0.6366074019182572 and parameters: {'max_depth': 10, 'n_estimators': 453, 'learning_rate': 0.8407782003410821, 'subsample': 0.8431127647985932, 'colsample_bytree': 0.36204000998677227, 'min_child_weight': 1.7349479891118087}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:47,712]\u001b[0m Trial 66 finished with value: -0.6365470105552343 and parameters: {'max_depth': 10, 'n_estimators': 447, 'learning_rate': 0.843214850230767, 'subsample': 0.9448357140087287, 'colsample_bytree': 0.35236337206898183, 'min_child_weight': 1.696371146838439}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:48,340]\u001b[0m Trial 67 finished with value: -0.6160010824336482 and parameters: {'max_depth': 10, 'n_estimators': 486, 'learning_rate': 0.7877330312092007, 'subsample': 0.9296383362315382, 'colsample_bytree': 0.3114455587871936, 'min_child_weight': 2.875998981140711}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:48,440]\u001b[0m Trial 68 finished with value: -0.604933869127948 and parameters: {'max_depth': 7, 'n_estimators': 485, 'learning_rate': 0.7888478583278904, 'subsample': 0.8977519109158317, 'colsample_bytree': 0.302190752251667, 'min_child_weight': 1.1969043199689788}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:49,055]\u001b[0m Trial 69 finished with value: -0.5998499835325494 and parameters: {'max_depth': 7, 'n_estimators': 427, 'learning_rate': 0.9584501736220188, 'subsample': 0.8172646305638608, 'colsample_bytree': 0.29793188515625463, 'min_child_weight': 1.3106298925995903}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:49,441]\u001b[0m Trial 70 finished with value: -0.641695870284686 and parameters: {'max_depth': 9, 'n_estimators': 426, 'learning_rate': 0.9563418793446115, 'subsample': 0.8171480008812609, 'colsample_bytree': 0.22914424934149893, 'min_child_weight': 0.40033072769025835}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:49,759]\u001b[0m Trial 71 finished with value: -0.6442810431527537 and parameters: {'max_depth': 9, 'n_estimators': 421, 'learning_rate': 0.8910805934623465, 'subsample': 0.9561043025154439, 'colsample_bytree': 0.2147133622376724, 'min_child_weight': 2.2158159011349974}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:50,453]\u001b[0m Trial 72 finished with value: -0.5951398889852837 and parameters: {'max_depth': 11, 'n_estimators': 360, 'learning_rate': 0.8714427689771285, 'subsample': 0.8724399908309705, 'colsample_bytree': 0.2676140314104938, 'min_child_weight': 0.8277379512815422}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:50,720]\u001b[0m Trial 73 finished with value: -0.5952614729933808 and parameters: {'max_depth': 11, 'n_estimators': 389, 'learning_rate': 0.8662981011225197, 'subsample': 0.8622868193441232, 'colsample_bytree': 0.2588690055415223, 'min_child_weight': 0.8125069970245886}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:51,335]\u001b[0m Trial 74 finished with value: -0.6008242923012659 and parameters: {'max_depth': 11, 'n_estimators': 386, 'learning_rate': 0.9104647080637774, 'subsample': 0.8610280848210214, 'colsample_bytree': 0.26733804205044936, 'min_child_weight': 1.4003178567422707}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:51,512]\u001b[0m Trial 75 finished with value: -0.6361017507711586 and parameters: {'max_depth': 8, 'n_estimators': 347, 'learning_rate': 0.9235692999358308, 'subsample': 0.9066178958760163, 'colsample_bytree': 0.40449696840334526, 'min_child_weight': 1.3596204034183665}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:52,165]\u001b[0m Trial 76 finished with value: -0.635991061387443 and parameters: {'max_depth': 8, 'n_estimators': 148, 'learning_rate': 0.9208297716678172, 'subsample': 0.9178879424578308, 'colsample_bytree': 0.40590102020747465, 'min_child_weight': 0.38153346564499024}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:52,363]\u001b[0m Trial 77 finished with value: -0.6374429663903348 and parameters: {'max_depth': 12, 'n_estimators': 151, 'learning_rate': 0.8134742197937629, 'subsample': 0.9915951083484129, 'colsample_bytree': 0.35664497198447903, 'min_child_weight': 0.4212315247100365}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:52,966]\u001b[0m Trial 78 finished with value: -0.5985378871859136 and parameters: {'max_depth': 9, 'n_estimators': 368, 'learning_rate': 0.8108629901494303, 'subsample': 0.9953254109674075, 'colsample_bytree': 0.34983680972375825, 'min_child_weight': 2.2070793255583174}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:53,375]\u001b[0m Trial 79 finished with value: -0.6371229134535056 and parameters: {'max_depth': 9, 'n_estimators': 277, 'learning_rate': 0.7673780301623985, 'subsample': 0.7673586398549335, 'colsample_bytree': 0.4510754729967044, 'min_child_weight': 2.205635411421904}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:53,834]\u001b[0m Trial 80 finished with value: -0.6407713213321765 and parameters: {'max_depth': 10, 'n_estimators': 470, 'learning_rate': 0.7504597801976053, 'subsample': 0.7679085683114244, 'colsample_bytree': 0.4413219876447999, 'min_child_weight': 1.9082398986543732}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:54,262]\u001b[0m Trial 81 finished with value: -0.5976753663003663 and parameters: {'max_depth': 10, 'n_estimators': 328, 'learning_rate': 0.8399192258669608, 'subsample': 0.8335915896441881, 'colsample_bytree': 0.285906733603875, 'min_child_weight': 1.8008366155891304}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:55,046]\u001b[0m Trial 82 finished with value: -0.5951180013334619 and parameters: {'max_depth': 11, 'n_estimators': 326, 'learning_rate': 0.8739479087664981, 'subsample': 0.8841348690224379, 'colsample_bytree': 0.2826640814232645, 'min_child_weight': 0.9667219965106969}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:55,615]\u001b[0m Trial 83 finished with value: -0.5952705260748025 and parameters: {'max_depth': 11, 'n_estimators': 352, 'learning_rate': 0.8665945890546376, 'subsample': 0.8796063288562285, 'colsample_bytree': 0.2505661046942758, 'min_child_weight': 1.0218943187550782}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:56,387]\u001b[0m Trial 84 finished with value: -0.6755889282501124 and parameters: {'max_depth': 3, 'n_estimators': 355, 'learning_rate': 0.8763687987540811, 'subsample': 0.8891953010444227, 'colsample_bytree': 0.3155932520992611, 'min_child_weight': 1.1072403660797696}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:57,511]\u001b[0m Trial 85 finished with value: -0.5978111424394318 and parameters: {'max_depth': 9, 'n_estimators': 440, 'learning_rate': 0.9776688276104285, 'subsample': 0.900785287653728, 'colsample_bytree': 0.31578865034773484, 'min_child_weight': 1.4915374128748475}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:58,212]\u001b[0m Trial 86 finished with value: -0.5925975355054302 and parameters: {'max_depth': 12, 'n_estimators': 408, 'learning_rate': 0.8374323008727859, 'subsample': 0.8263705330633679, 'colsample_bytree': 0.3299542423674333, 'min_child_weight': 1.4481095270144444}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:59,274]\u001b[0m Trial 87 finished with value: -0.635688401693336 and parameters: {'max_depth': 12, 'n_estimators': 313, 'learning_rate': 0.7981444543913212, 'subsample': 0.8258811965905111, 'colsample_bytree': 0.3793521907541895, 'min_child_weight': 2.652545124488329}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:00:59,714]\u001b[0m Trial 88 finished with value: -0.592603955802969 and parameters: {'max_depth': 13, 'n_estimators': 413, 'learning_rate': 0.8399557886978998, 'subsample': 0.8232020252655797, 'colsample_bytree': 0.3336728594466948, 'min_child_weight': 1.5809036882435745}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:00,266]\u001b[0m Trial 89 finished with value: -0.6053415369352869 and parameters: {'max_depth': 8, 'n_estimators': 411, 'learning_rate': 0.8336652578059733, 'subsample': 0.7935029782456331, 'colsample_bytree': 0.3376423242085437, 'min_child_weight': 1.6748140658084658}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:00,712]\u001b[0m Trial 90 finished with value: -0.6337708775142985 and parameters: {'max_depth': 13, 'n_estimators': 407, 'learning_rate': 0.8353140252352704, 'subsample': 0.847310397905498, 'colsample_bytree': 0.3848539262483528, 'min_child_weight': 2.07463800269419}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:01,215]\u001b[0m Trial 91 finished with value: -0.6382624168594564 and parameters: {'max_depth': 13, 'n_estimators': 396, 'learning_rate': 0.7720516581152428, 'subsample': 0.8466638537037905, 'colsample_bytree': 0.39066737347577535, 'min_child_weight': 2.021450150129536}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:01,626]\u001b[0m Trial 92 finished with value: -0.6097883611271768 and parameters: {'max_depth': 14, 'n_estimators': 395, 'learning_rate': 0.7620071204692345, 'subsample': 0.7996749231125656, 'colsample_bytree': 0.3262665174588973, 'min_child_weight': 1.4648401257051953}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:02,187]\u001b[0m Trial 93 finished with value: -0.6078621272733115 and parameters: {'max_depth': 14, 'n_estimators': 369, 'learning_rate': 0.7124612736074583, 'subsample': 0.8070841784807321, 'colsample_bytree': 0.29374221212350843, 'min_child_weight': 1.5055493162724687}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:02,539]\u001b[0m Trial 94 finished with value: -0.6004766302776171 and parameters: {'max_depth': 12, 'n_estimators': 372, 'learning_rate': 0.7136094159141328, 'subsample': 0.8269881866631028, 'colsample_bytree': 0.29357219044329064, 'min_child_weight': 2.358578239818475}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:02,990]\u001b[0m Trial 95 finished with value: -0.6435272214992609 and parameters: {'max_depth': 12, 'n_estimators': 439, 'learning_rate': 0.7966601035152729, 'subsample': 0.8215386845257894, 'colsample_bytree': 0.24756662388418416, 'min_child_weight': 2.3570662617321636}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:03,340]\u001b[0m Trial 96 finished with value: -0.6015419919992289 and parameters: {'max_depth': 13, 'n_estimators': 439, 'learning_rate': 0.8027433758177889, 'subsample': 0.7767971119962075, 'colsample_bytree': 0.2511410186421109, 'min_child_weight': 1.7879057886072924}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:03,838]\u001b[0m Trial 97 finished with value: -0.5970918482102692 and parameters: {'max_depth': 9, 'n_estimators': 461, 'learning_rate': 0.8534570288765002, 'subsample': 0.7792690703024268, 'colsample_bytree': 0.3491542088826663, 'min_child_weight': 1.209088263916705}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:04,193]\u001b[0m Trial 98 finished with value: -0.6331445701593726 and parameters: {'max_depth': 9, 'n_estimators': 416, 'learning_rate': 0.84464393070716, 'subsample': 0.861806822745953, 'colsample_bytree': 0.3728303755752729, 'min_child_weight': 0.6811442593947399}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:04,645]\u001b[0m Trial 99 finished with value: -0.6048152874172612 and parameters: {'max_depth': 10, 'n_estimators': 418, 'learning_rate': 0.90322953221931, 'subsample': 0.8617769756670858, 'colsample_bytree': 0.3235090840581843, 'min_child_weight': 0.6135195622404324}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:05,022]\u001b[0m Trial 100 finished with value: -0.5953894853319195 and parameters: {'max_depth': 10, 'n_estimators': 379, 'learning_rate': 0.7358800746735814, 'subsample': 0.748189592421957, 'colsample_bytree': 0.3235435275383138, 'min_child_weight': 1.8507277056586742}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:05,441]\u001b[0m Trial 101 finished with value: -0.5953565905950774 and parameters: {'max_depth': 10, 'n_estimators': 379, 'learning_rate': 0.7348862316016859, 'subsample': 0.9086687611118863, 'colsample_bytree': 0.3466852488288083, 'min_child_weight': 1.225456088517821}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:06,006]\u001b[0m Trial 102 finished with value: -0.5951399753389884 and parameters: {'max_depth': 11, 'n_estimators': 332, 'learning_rate': 0.8732081262926952, 'subsample': 0.9159480798200486, 'colsample_bytree': 0.27386724599047796, 'min_child_weight': 0.9886159490839074}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:06,518]\u001b[0m Trial 103 finished with value: -0.6036769307081807 and parameters: {'max_depth': 11, 'n_estimators': 335, 'learning_rate': 0.8808399189524907, 'subsample': 0.8814169541229128, 'colsample_bytree': 0.2794697072143719, 'min_child_weight': 0.9825388750752628}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:07,836]\u001b[0m Trial 104 finished with value: -0.5998475816946212 and parameters: {'max_depth': 11, 'n_estimators': 340, 'learning_rate': 0.8297823693236596, 'subsample': 0.8815035897295435, 'colsample_bytree': 0.28259665306597487, 'min_child_weight': 1.5722592301085445}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:08,332]\u001b[0m Trial 105 finished with value: -0.6416878132832079 and parameters: {'max_depth': 8, 'n_estimators': 299, 'learning_rate': 0.8226493400177254, 'subsample': 0.9462039729985021, 'colsample_bytree': 0.21992001143610834, 'min_child_weight': 1.5963388564593703}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:09,207]\u001b[0m Trial 106 finished with value: -0.6075578188259109 and parameters: {'max_depth': 12, 'n_estimators': 392, 'learning_rate': 0.7837347211602991, 'subsample': 0.9370500364160556, 'colsample_bytree': 0.3036551672695482, 'min_child_weight': 2.68121044094071}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:09,562]\u001b[0m Trial 107 finished with value: -0.6012003847760428 and parameters: {'max_depth': 12, 'n_estimators': 395, 'learning_rate': 0.7885415557323737, 'subsample': 0.8459444174748447, 'colsample_bytree': 0.3006446333922057, 'min_child_weight': 2.5725630743525065}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:11,310]\u001b[0m Trial 108 finished with value: -0.6364101379249405 and parameters: {'max_depth': 9, 'n_estimators': 429, 'learning_rate': 0.852620441412158, 'subsample': 0.8424274540741155, 'colsample_bytree': 0.36341597052287056, 'min_child_weight': 1.3705105743829042}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:11,754]\u001b[0m Trial 109 finished with value: -0.6359374236874237 and parameters: {'max_depth': 13, 'n_estimators': 323, 'learning_rate': 0.9047286220622223, 'subsample': 0.7951100066203035, 'colsample_bytree': 0.36710093285211826, 'min_child_weight': 1.8371158522109392}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:14,065]\u001b[0m Trial 110 finished with value: -0.6443601792943899 and parameters: {'max_depth': 13, 'n_estimators': 323, 'learning_rate': 0.8977987848354075, 'subsample': 0.8004419269618555, 'colsample_bytree': 0.23789133741758833, 'min_child_weight': 2.1488069029589294}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:14,795]\u001b[0m Trial 111 finished with value: -0.6462929057419189 and parameters: {'max_depth': 9, 'n_estimators': 362, 'learning_rate': 0.7559048421416201, 'subsample': 0.8144706969365283, 'colsample_bytree': 0.2351914604024145, 'min_child_weight': 2.0994110250082585}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:17,272]\u001b[0m Trial 112 finished with value: -0.6036614754032518 and parameters: {'max_depth': 11, 'n_estimators': 358, 'learning_rate': 0.8798771039414721, 'subsample': 0.8721837754750613, 'colsample_bytree': 0.2684359817640566, 'min_child_weight': 0.8745445556138093}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:17,551]\u001b[0m Trial 113 finished with value: -0.5996851182443288 and parameters: {'max_depth': 11, 'n_estimators': 347, 'learning_rate': 0.9370784178737189, 'subsample': 0.8720941348329706, 'colsample_bytree': 0.2626564372068176, 'min_child_weight': 0.8205420884127715}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:17,890]\u001b[0m Trial 114 finished with value: -0.6708489934772828 and parameters: {'max_depth': 4, 'n_estimators': 407, 'learning_rate': 0.9338016024194283, 'subsample': 0.8992896036295113, 'colsample_bytree': 0.2087857351132949, 'min_child_weight': 0.8661262886315859}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:18,621]\u001b[0m Trial 115 finished with value: -0.6712102772958036 and parameters: {'max_depth': 4, 'n_estimators': 407, 'learning_rate': 0.8071590219762549, 'subsample': 0.8970036799608566, 'colsample_bytree': 0.20994887362691006, 'min_child_weight': 0.13585640978310964}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:19,420]\u001b[0m Trial 116 finished with value: -0.5924600162264636 and parameters: {'max_depth': 10, 'n_estimators': 279, 'learning_rate': 0.8151831585845519, 'subsample': 0.9165783522956644, 'colsample_bytree': 0.33674564017448483, 'min_child_weight': 0.3655308922404618}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:20,105]\u001b[0m Trial 117 finished with value: -0.5989044024323629 and parameters: {'max_depth': 10, 'n_estimators': 385, 'learning_rate': 0.8526855045671778, 'subsample': 0.9588811288684665, 'colsample_bytree': 0.3407829864095679, 'min_child_weight': 1.25326475559201}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:20,754]\u001b[0m Trial 118 finished with value: -0.6367328597937152 and parameters: {'max_depth': 10, 'n_estimators': 203, 'learning_rate': 0.8248848638001095, 'subsample': 0.957597643472944, 'colsample_bytree': 0.4103800901039388, 'min_child_weight': 0.4778111205858762}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:21,724]\u001b[0m Trial 119 finished with value: -0.635949025207249 and parameters: {'max_depth': 10, 'n_estimators': 225, 'learning_rate': 0.8229201043565589, 'subsample': 0.8265566279556629, 'colsample_bytree': 0.4148715648417638, 'min_child_weight': 0.3963369940134377}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:22,416]\u001b[0m Trial 120 finished with value: -0.5918217920602789 and parameters: {'max_depth': 9, 'n_estimators': 426, 'learning_rate': 0.7688415362711924, 'subsample': 0.8282826554420858, 'colsample_bytree': 0.31825888015015463, 'min_child_weight': 0.5678200386896943}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:22,986]\u001b[0m Trial 121 finished with value: -0.5964234404119273 and parameters: {'max_depth': 9, 'n_estimators': 281, 'learning_rate': 0.7722831458284265, 'subsample': 0.9149934517217246, 'colsample_bytree': 0.3157691875256757, 'min_child_weight': 0.6538424133191363}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:24,209]\u001b[0m Trial 122 finished with value: -0.5985205100089969 and parameters: {'max_depth': 9, 'n_estimators': 274, 'learning_rate': 0.7783049256668862, 'subsample': 0.8528253367876257, 'colsample_bytree': 0.3126150354148678, 'min_child_weight': 1.1027957096386216}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:24,489]\u001b[0m Trial 123 finished with value: -0.5905607930081616 and parameters: {'max_depth': 9, 'n_estimators': 255, 'learning_rate': 0.8048984736993277, 'subsample': 0.85621225774371, 'colsample_bytree': 0.3293490501593721, 'min_child_weight': 0.27410970635526155}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:25,956]\u001b[0m Trial 124 finished with value: -0.5900299828095881 and parameters: {'max_depth': 9, 'n_estimators': 425, 'learning_rate': 0.8092214642397747, 'subsample': 0.8333704951016493, 'colsample_bytree': 0.3337159491062625, 'min_child_weight': 0.24959166284936862}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:26,097]\u001b[0m Trial 125 finished with value: -0.5904995722479275 and parameters: {'max_depth': 9, 'n_estimators': 425, 'learning_rate': 0.8064640597850867, 'subsample': 0.7587166403795501, 'colsample_bytree': 0.3382406073488287, 'min_child_weight': 0.2487953735589633}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:28,000]\u001b[0m Trial 127 finished with value: -0.6366563704935415 and parameters: {'max_depth': 8, 'n_estimators': 257, 'learning_rate': 0.7529223777851366, 'subsample': 0.7588877810707729, 'colsample_bytree': 0.37779933653887304, 'min_child_weight': 0.11343719542051794}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:28,173]\u001b[0m Trial 126 finished with value: -0.5995726355150698 and parameters: {'max_depth': 8, 'n_estimators': 223, 'learning_rate': 0.7544970545910031, 'subsample': 0.7553170157337895, 'colsample_bytree': 0.3336635237506525, 'min_child_weight': 0.23172782085751376}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:30,974]\u001b[0m Trial 128 finished with value: -0.5956284842715764 and parameters: {'max_depth': 9, 'n_estimators': 450, 'learning_rate': 0.8028397866308596, 'subsample': 0.7267666231014809, 'colsample_bytree': 0.33211774485590395, 'min_child_weight': 0.34291770830099466}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:31,144]\u001b[0m Trial 129 finished with value: -0.637443898207056 and parameters: {'max_depth': 9, 'n_estimators': 448, 'learning_rate': 0.8059148077474633, 'subsample': 0.7923738601822957, 'colsample_bytree': 0.3589800083299334, 'min_child_weight': 0.5397248702092355}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:33,709]\u001b[0m Trial 130 finished with value: -0.5901450862733757 and parameters: {'max_depth': 9, 'n_estimators': 425, 'learning_rate': 0.8132048752672133, 'subsample': 0.7881852066865507, 'colsample_bytree': 0.29799223414685283, 'min_child_weight': 0.6264083117734457}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:33,958]\u001b[0m Trial 131 finished with value: -0.5900664742786453 and parameters: {'max_depth': 9, 'n_estimators': 430, 'learning_rate': 0.8415451752916586, 'subsample': 0.7781320868112884, 'colsample_bytree': 0.29906644178304365, 'min_child_weight': 0.559310470049986}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:35,628]\u001b[0m Trial 132 finished with value: -0.5900555455144271 and parameters: {'max_depth': 9, 'n_estimators': 428, 'learning_rate': 0.8299870998377993, 'subsample': 0.7785755295897223, 'colsample_bytree': 0.30220409045429, 'min_child_weight': 0.312955082578439}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:36,033]\u001b[0m Trial 133 finished with value: -0.590146006040743 and parameters: {'max_depth': 9, 'n_estimators': 432, 'learning_rate': 0.8151803890462523, 'subsample': 0.7791020889868733, 'colsample_bytree': 0.29520914303383355, 'min_child_weight': 0.25018855041612026}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:37,419]\u001b[0m Trial 134 finished with value: -0.5901761354508065 and parameters: {'max_depth': 9, 'n_estimators': 427, 'learning_rate': 0.8130282953058472, 'subsample': 0.7800894694941836, 'colsample_bytree': 0.3061790078082751, 'min_child_weight': 0.26542171517467206}. Best is trial 36 with value: -0.590018059973652.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:37,882]\u001b[0m Trial 135 finished with value: -0.5889032938917806 and parameters: {'max_depth': 9, 'n_estimators': 473, 'learning_rate': 0.7808677969101286, 'subsample': 0.7754287336549038, 'colsample_bytree': 0.29484517657916165, 'min_child_weight': 0.6882237146645294}. Best is trial 135 with value: -0.5889032938917806.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:39,101]\u001b[0m Trial 136 finished with value: -0.5888968836353705 and parameters: {'max_depth': 9, 'n_estimators': 429, 'learning_rate': 0.785461337067571, 'subsample': 0.7716244833348449, 'colsample_bytree': 0.2970813261191314, 'min_child_weight': 0.27106750258415757}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:39,824]\u001b[0m Trial 137 finished with value: -0.603893967691665 and parameters: {'max_depth': 8, 'n_estimators': 434, 'learning_rate': 0.7831919479507979, 'subsample': 0.7738615187089188, 'colsample_bytree': 0.3020915301193889, 'min_child_weight': 0.17461826065090968}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:40,820]\u001b[0m Trial 138 finished with value: -0.6028637920924105 and parameters: {'max_depth': 8, 'n_estimators': 472, 'learning_rate': 0.7881912137634557, 'subsample': 0.7286064145915532, 'colsample_bytree': 0.29258011403236905, 'min_child_weight': 0.21102244283493393}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:41,567]\u001b[0m Trial 139 finished with value: -0.6113753393901419 and parameters: {'max_depth': 9, 'n_estimators': 494, 'learning_rate': 0.7372549903412092, 'subsample': 0.7355422970754518, 'colsample_bytree': 0.28759085141478896, 'min_child_weight': 0.3010368128229679}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:42,544]\u001b[0m Trial 140 finished with value: -0.603513111705546 and parameters: {'max_depth': 9, 'n_estimators': 460, 'learning_rate': 0.705606038116787, 'subsample': 0.6993145601289087, 'colsample_bytree': 0.28508916040329235, 'min_child_weight': 0.29556969152913326}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:43,221]\u001b[0m Trial 141 finished with value: -0.5901002968157574 and parameters: {'max_depth': 9, 'n_estimators': 422, 'learning_rate': 0.8248748053541276, 'subsample': 0.7058762019851517, 'colsample_bytree': 0.3049347633314936, 'min_child_weight': 0.6693609329873793}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:44,714]\u001b[0m Trial 142 finished with value: -0.5901990151661205 and parameters: {'max_depth': 9, 'n_estimators': 421, 'learning_rate': 0.8146943361825456, 'subsample': 0.7589067448940681, 'colsample_bytree': 0.3075571310567766, 'min_child_weight': 0.6693215272058441}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:45,880]\u001b[0m Trial 143 finished with value: -0.5901112617280381 and parameters: {'max_depth': 9, 'n_estimators': 424, 'learning_rate': 0.8179849812546142, 'subsample': 0.751637676357329, 'colsample_bytree': 0.3075892343143389, 'min_child_weight': 0.6904671686895536}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:46,811]\u001b[0m Trial 144 finished with value: -0.5901021102435577 and parameters: {'max_depth': 9, 'n_estimators': 433, 'learning_rate': 0.8243754523825516, 'subsample': 0.7556764808739964, 'colsample_bytree': 0.310174599986706, 'min_child_weight': 0.7098647658178642}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:47,780]\u001b[0m Trial 145 finished with value: -0.5901168044309492 and parameters: {'max_depth': 9, 'n_estimators': 430, 'learning_rate': 0.8233230003618672, 'subsample': 0.7506890357943495, 'colsample_bytree': 0.25005371251280023, 'min_child_weight': 0.6094380491811087}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:48,298]\u001b[0m Trial 146 finished with value: -0.5956856403830089 and parameters: {'max_depth': 8, 'n_estimators': 431, 'learning_rate': 0.830074566494916, 'subsample': 0.7538562618195093, 'colsample_bytree': 0.25175202860047224, 'min_child_weight': 0.6799376416618086}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:48,716]\u001b[0m Trial 147 finished with value: -0.6400858797635114 and parameters: {'max_depth': 8, 'n_estimators': 440, 'learning_rate': 0.856581274825439, 'subsample': 0.7151255055399303, 'colsample_bytree': 0.2491540641591412, 'min_child_weight': 0.6652839392941146}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:49,130]\u001b[0m Trial 148 finished with value: -0.5975594213900135 and parameters: {'max_depth': 9, 'n_estimators': 441, 'learning_rate': 0.856305731285535, 'subsample': 0.705687724711462, 'colsample_bytree': 0.2670796459990668, 'min_child_weight': 0.7168770980878816}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:49,617]\u001b[0m Trial 149 finished with value: -0.5900966800012852 and parameters: {'max_depth': 9, 'n_estimators': 462, 'learning_rate': 0.8447917599849292, 'subsample': 0.7778457162195614, 'colsample_bytree': 0.2627244676610221, 'min_child_weight': 0.4869231732579448}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:50,085]\u001b[0m Trial 150 finished with value: -0.5900573328352934 and parameters: {'max_depth': 9, 'n_estimators': 458, 'learning_rate': 0.8283606179865067, 'subsample': 0.6818388801777834, 'colsample_bytree': 0.3040415461237862, 'min_child_weight': 0.5510415722377608}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:50,729]\u001b[0m Trial 151 finished with value: -0.642450055427029 and parameters: {'max_depth': 9, 'n_estimators': 465, 'learning_rate': 0.8392213352166963, 'subsample': 0.7807292076508732, 'colsample_bytree': 0.23237037417242926, 'min_child_weight': 0.4757849292860471}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:51,004]\u001b[0m Trial 152 finished with value: -0.5968365444862155 and parameters: {'max_depth': 9, 'n_estimators': 464, 'learning_rate': 0.834835948875444, 'subsample': 0.7336635276400224, 'colsample_bytree': 0.28130233519209447, 'min_child_weight': 0.47115135168190964}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:51,676]\u001b[0m Trial 153 finished with value: -0.5901478114356404 and parameters: {'max_depth': 9, 'n_estimators': 457, 'learning_rate': 0.8244933611080132, 'subsample': 0.6810971829360449, 'colsample_bytree': 0.2833906835499581, 'min_child_weight': 0.5366704661249894}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:51,850]\u001b[0m Trial 154 finished with value: -0.5942474435286936 and parameters: {'max_depth': 9, 'n_estimators': 481, 'learning_rate': 0.8809421830932327, 'subsample': 0.6769187978859261, 'colsample_bytree': 0.26615016260868984, 'min_child_weight': 0.5292443026350497}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:52,471]\u001b[0m Trial 155 finished with value: -0.594172468430692 and parameters: {'max_depth': 9, 'n_estimators': 474, 'learning_rate': 0.8871193936308873, 'subsample': 0.6967789972819945, 'colsample_bytree': 0.26924044044171963, 'min_child_weight': 0.81692776080976}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:52,588]\u001b[0m Trial 156 finished with value: -0.6400904665509929 and parameters: {'max_depth': 8, 'n_estimators': 455, 'learning_rate': 0.860669095223095, 'subsample': 0.6835305763561061, 'colsample_bytree': 0.22847354132813855, 'min_child_weight': 0.11105278994494205}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:53,332]\u001b[0m Trial 157 finished with value: -0.6399860789795001 and parameters: {'max_depth': 8, 'n_estimators': 455, 'learning_rate': 0.8631056675216274, 'subsample': 0.6531215605376204, 'colsample_bytree': 0.18240631923461992, 'min_child_weight': 0.9787785755050425}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:53,442]\u001b[0m Trial 158 finished with value: -0.6464211409935093 and parameters: {'max_depth': 9, 'n_estimators': 513, 'learning_rate': 0.7804816518908799, 'subsample': 0.6585547305206426, 'colsample_bytree': 0.1941883830542584, 'min_child_weight': 1.0021039053324337}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:54,300]\u001b[0m Trial 159 finished with value: -0.5918181290566159 and parameters: {'max_depth': 9, 'n_estimators': 489, 'learning_rate': 0.7717295203058837, 'subsample': 0.7409106782943404, 'colsample_bytree': 0.2975420364742327, 'min_child_weight': 0.49846182715572585}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:54,397]\u001b[0m Trial 160 finished with value: -0.5901277693432299 and parameters: {'max_depth': 9, 'n_estimators': 496, 'learning_rate': 0.8248993179923907, 'subsample': 0.7428177921566584, 'colsample_bytree': 0.2852782071579707, 'min_child_weight': 0.7817640920563562}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:55,235]\u001b[0m Trial 161 finished with value: -0.5924335337542576 and parameters: {'max_depth': 10, 'n_estimators': 437, 'learning_rate': 0.8336293363735643, 'subsample': 0.7186168607885576, 'colsample_bytree': 0.2879074552122268, 'min_child_weight': 0.7068648565674078}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:55,320]\u001b[0m Trial 162 finished with value: -0.5900436447689738 and parameters: {'max_depth': 9, 'n_estimators': 499, 'learning_rate': 0.830191855066292, 'subsample': 0.7222789439825819, 'colsample_bytree': 0.2887948408671748, 'min_child_weight': 0.7686789148915779}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:56,059]\u001b[0m Trial 163 finished with value: -0.5900829216470664 and parameters: {'max_depth': 9, 'n_estimators': 502, 'learning_rate': 0.8234615695505919, 'subsample': 0.7431215398882297, 'colsample_bytree': 0.25272568354713104, 'min_child_weight': 0.8225369592538878}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:56,268]\u001b[0m Trial 164 finished with value: -0.5994841069018702 and parameters: {'max_depth': 9, 'n_estimators': 522, 'learning_rate': 0.7923178010051818, 'subsample': 0.7464415185821476, 'colsample_bytree': 0.2506905111051725, 'min_child_weight': 0.7918183954820953}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:57,043]\u001b[0m Trial 165 finished with value: -0.6010556479339374 and parameters: {'max_depth': 9, 'n_estimators': 519, 'learning_rate': 0.8488232972235915, 'subsample': 0.72248651489133, 'colsample_bytree': 0.25551944049699965, 'min_child_weight': 0.7832477910061713}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:57,150]\u001b[0m Trial 166 finished with value: -0.5924362749823276 and parameters: {'max_depth': 10, 'n_estimators': 537, 'learning_rate': 0.8450130351547116, 'subsample': 0.7230977181009399, 'colsample_bytree': 0.3160458439000693, 'min_child_weight': 1.1642459057964825}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:58,110]\u001b[0m Trial 167 finished with value: -0.5924034023359681 and parameters: {'max_depth': 10, 'n_estimators': 539, 'learning_rate': 0.8436117839539363, 'subsample': 0.7454568708784434, 'colsample_bytree': 0.3188645041808429, 'min_child_weight': 1.1793240288818203}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:01:58,329]\u001b[0m Trial 168 finished with value: -0.6394989717884454 and parameters: {'max_depth': 8, 'n_estimators': 482, 'learning_rate': 0.7644944027915225, 'subsample': 0.7449275731272853, 'colsample_bytree': 0.23888174495598485, 'min_child_weight': 1.087280916517789}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:00,100]\u001b[0m Trial 169 finished with value: -0.6424642335004177 and parameters: {'max_depth': 8, 'n_estimators': 502, 'learning_rate': 0.75131546484637, 'subsample': 0.7639489285931373, 'colsample_bytree': 0.2420122209394744, 'min_child_weight': 0.9263080254940881}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:00,330]\u001b[0m Trial 170 finished with value: -0.6436162561853352 and parameters: {'max_depth': 9, 'n_estimators': 501, 'learning_rate': 0.9119247093774755, 'subsample': 0.7713969973772918, 'colsample_bytree': 0.21979502081712535, 'min_child_weight': 0.856673419619615}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:02,032]\u001b[0m Trial 171 finished with value: -0.6368331666506007 and parameters: {'max_depth': 9, 'n_estimators': 500, 'learning_rate': 0.899913018788942, 'subsample': 0.6984585695517473, 'colsample_bytree': 0.3515214251933773, 'min_child_weight': 0.45970893137398244}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:02,251]\u001b[0m Trial 172 finished with value: -0.5901377823565322 and parameters: {'max_depth': 9, 'n_estimators': 446, 'learning_rate': 0.8145998661202144, 'subsample': 0.7881348028660335, 'colsample_bytree': 0.29853745047398034, 'min_child_weight': 0.4359608375217172}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:03,161]\u001b[0m Trial 173 finished with value: -0.5901112316046526 and parameters: {'max_depth': 9, 'n_estimators': 446, 'learning_rate': 0.8217887441395513, 'subsample': 0.7865249142884384, 'colsample_bytree': 0.2730588317414868, 'min_child_weight': 0.585550834622069}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:03,346]\u001b[0m Trial 174 finished with value: -0.5900957421598869 and parameters: {'max_depth': 9, 'n_estimators': 474, 'learning_rate': 0.8232967266434011, 'subsample': 0.7927483783811271, 'colsample_bytree': 0.27115904352969233, 'min_child_weight': 0.6090915000177244}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:04,113]\u001b[0m Trial 175 finished with value: -0.6049726419413919 and parameters: {'max_depth': 9, 'n_estimators': 466, 'learning_rate': 0.7886796510646613, 'subsample': 0.8050468725010261, 'colsample_bytree': 0.27241274775303814, 'min_child_weight': 0.43634335555657167}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:04,373]\u001b[0m Trial 176 finished with value: -0.5986247067990489 and parameters: {'max_depth': 9, 'n_estimators': 469, 'learning_rate': 0.8675958940561241, 'subsample': 0.7990152965429416, 'colsample_bytree': 0.27366265381587696, 'min_child_weight': 0.6565178224764203}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:05,027]\u001b[0m Trial 177 finished with value: -0.5949937985990617 and parameters: {'max_depth': 9, 'n_estimators': 476, 'learning_rate': 0.8708267330288024, 'subsample': 0.7614263211577694, 'colsample_bytree': 0.26903859128814883, 'min_child_weight': 0.7113705627368055}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:05,345]\u001b[0m Trial 178 finished with value: -0.5923832779063042 and parameters: {'max_depth': 10, 'n_estimators': 492, 'learning_rate': 0.8286232720211798, 'subsample': 0.7624060551344535, 'colsample_bytree': 0.26074500679405926, 'min_child_weight': 0.998979234218158}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:06,068]\u001b[0m Trial 179 finished with value: -0.6012528697545144 and parameters: {'max_depth': 10, 'n_estimators': 489, 'learning_rate': 0.7999857823121728, 'subsample': 0.7095406947307975, 'colsample_bytree': 0.3152436322762206, 'min_child_weight': 1.032744966788405}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:06,377]\u001b[0m Trial 180 finished with value: -0.6032194528789925 and parameters: {'max_depth': 10, 'n_estimators': 565, 'learning_rate': 0.7915969037999762, 'subsample': 0.7140889633926311, 'colsample_bytree': 0.3157353063362937, 'min_child_weight': 1.2784914189973435}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:07,088]\u001b[0m Trial 181 finished with value: -0.6463576870863056 and parameters: {'max_depth': 9, 'n_estimators': 402, 'learning_rate': 0.726015939936702, 'subsample': 0.7371799108728475, 'colsample_bytree': 0.22122230284242167, 'min_child_weight': 0.3538494198523087}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:07,308]\u001b[0m Trial 182 finished with value: -0.590109446292012 and parameters: {'max_depth': 9, 'n_estimators': 448, 'learning_rate': 0.8243953313152574, 'subsample': 0.7860627164589283, 'colsample_bytree': 0.29458700982442165, 'min_child_weight': 0.12401822848602484}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:07,919]\u001b[0m Trial 183 finished with value: -0.5901003008322088 and parameters: {'max_depth': 9, 'n_estimators': 444, 'learning_rate': 0.8242337273338342, 'subsample': 0.7870881401577079, 'colsample_bytree': 0.2941368465087427, 'min_child_weight': 0.5159953945078122}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:08,188]\u001b[0m Trial 184 finished with value: -0.5900619115898722 and parameters: {'max_depth': 9, 'n_estimators': 445, 'learning_rate': 0.8282466182154753, 'subsample': 0.7884943452810205, 'colsample_bytree': 0.28557800976684544, 'min_child_weight': 0.11244240268566602}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:08,751]\u001b[0m Trial 186 finished with value: -0.6642686604331342 and parameters: {'max_depth': 9, 'n_estimators': 444, 'learning_rate': 0.8455873798653253, 'subsample': 0.777557965938222, 'colsample_bytree': 0.12512373384023534, 'min_child_weight': 0.10959129742328749}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:08,886]\u001b[0m Trial 185 finished with value: -0.596788999742947 and parameters: {'max_depth': 9, 'n_estimators': 443, 'learning_rate': 0.8356443522027501, 'subsample': 0.780622621348019, 'colsample_bytree': 0.2866072579131698, 'min_child_weight': 0.10023853893450169}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:09,842]\u001b[0m Trial 187 finished with value: -0.6126016061789088 and parameters: {'max_depth': 8, 'n_estimators': 447, 'learning_rate': 0.5655908088880088, 'subsample': 0.8059397191918017, 'colsample_bytree': 0.2509549436004109, 'min_child_weight': 0.10838401600404796}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:09,938]\u001b[0m Trial 188 finished with value: -0.6491584028179422 and parameters: {'max_depth': 8, 'n_estimators': 453, 'learning_rate': 0.604303723194968, 'subsample': 0.8072432613786477, 'colsample_bytree': 0.24821034805987882, 'min_child_weight': 0.36698972458729634}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:10,899]\u001b[0m Trial 189 finished with value: -0.6388490597487307 and parameters: {'max_depth': 9, 'n_estimators': 416, 'learning_rate': 0.8583414045687229, 'subsample': 0.7677131648700465, 'colsample_bytree': 0.4865860402813521, 'min_child_weight': 0.4925658932057489}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:11,098]\u001b[0m Trial 190 finished with value: -0.6374366846603688 and parameters: {'max_depth': 9, 'n_estimators': 434, 'learning_rate': 0.43292628070193573, 'subsample': 0.7684285441698393, 'colsample_bytree': 0.3071088406929347, 'min_child_weight': 0.5583255357565428}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:11,897]\u001b[0m Trial 191 finished with value: -0.5901176960831567 and parameters: {'max_depth': 9, 'n_estimators': 433, 'learning_rate': 0.8245991180881073, 'subsample': 0.7937218505951521, 'colsample_bytree': 0.30412866161764096, 'min_child_weight': 0.5766288693876309}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:12,046]\u001b[0m Trial 192 finished with value: -0.590118593760041 and parameters: {'max_depth': 9, 'n_estimators': 478, 'learning_rate': 0.8248597779294369, 'subsample': 0.747167081765421, 'colsample_bytree': 0.2813889936347308, 'min_child_weight': 0.8381217258474283}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:13,311]\u001b[0m Trial 193 finished with value: -0.5956074460992224 and parameters: {'max_depth': 9, 'n_estimators': 421, 'learning_rate': 0.8006634665612133, 'subsample': 0.7934090998200326, 'colsample_bytree': 0.2760250680477679, 'min_child_weight': 0.32637949860042503}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:14,298]\u001b[0m Trial 194 finished with value: -0.594749411589872 and parameters: {'max_depth': 9, 'n_estimators': 460, 'learning_rate': 0.7969695611066075, 'subsample': 0.7934133523358156, 'colsample_bytree': 0.3266066957804695, 'min_child_weight': 0.3105894822000195}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:16,192]\u001b[0m Trial 195 finished with value: -0.5987668470053339 and parameters: {'max_depth': 9, 'n_estimators': 461, 'learning_rate': 0.7783118765281457, 'subsample': 0.8132872993587562, 'colsample_bytree': 0.30097337969543353, 'min_child_weight': 0.5899989572851936}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:16,502]\u001b[0m Trial 196 finished with value: -0.6038223583799242 and parameters: {'max_depth': 9, 'n_estimators': 433, 'learning_rate': 0.680110549596088, 'subsample': 0.8147539713883165, 'colsample_bytree': 0.29266494994456527, 'min_child_weight': 0.6229146513555279}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:17,688]\u001b[0m Trial 197 finished with value: -0.5900811242850715 and parameters: {'max_depth': 9, 'n_estimators': 436, 'learning_rate': 0.8266581171248752, 'subsample': 0.7282089590845084, 'colsample_bytree': 0.34285526170508046, 'min_child_weight': 0.5981909110341391}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:17,932]\u001b[0m Trial 198 finished with value: -0.5988831112235717 and parameters: {'max_depth': 10, 'n_estimators': 437, 'learning_rate': 0.8898544623674386, 'subsample': 0.7514294096161573, 'colsample_bytree': 0.3462457777104411, 'min_child_weight': 0.3457953738776927}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:02:18,206]\u001b[0m Trial 199 finished with value: -0.593700467113296 and parameters: {'max_depth': 8, 'n_estimators': 447, 'learning_rate': 0.8854927593869414, 'subsample': 0.7386450450637887, 'colsample_bytree': 0.3428072802852308, 'min_child_weight': 0.34561452471869025}. Best is trial 136 with value: -0.5888968836353705.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'max_depth': 9, 'n_estimators': 429, 'learning_rate': 0.785461337067571, 'subsample': 0.7716244833348449, 'colsample_bytree': 0.2970813261191314, 'min_child_weight': 0.27106750258415757}\n",
            "Best ROC-AUC Score: -0.5888968836353705\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "from functools import partial\n",
        "\n",
        "X = train_df.drop(['TenYearCHD'], axis=1)\n",
        "y = train_df['TenYearCHD']\n",
        "X = X.reset_index(drop=True)\n",
        "\n",
        "def optimize(trial, X, y):\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 600),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),\n",
        "        'subsample': trial.suggest_float('subsample', 0.1, 1),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1),\n",
        "        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10)\n",
        "    }\n",
        "    model = lgb.LGBMClassifier(**params)\n",
        "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
        "    scores = []\n",
        "    for train_idx, val_idx in cv.split(X, y):\n",
        "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "        model.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_val, y_val)], verbose=False)\n",
        "        y_pred = model.predict_proba(X_val)[:, 1]\n",
        "        score = roc_auc_score(y_val, y_pred)\n",
        "        scores.append(score)\n",
        "    return -1.0 * np.mean(scores)\n",
        "\n",
        "# Create the objective function with partial arguments\n",
        "objective = partial(optimize, X=X, y=y)\n",
        "\n",
        "# Create the study object with the TPE sampler\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
        "\n",
        "# Optimize the objective function\n",
        "study.optimize(objective, n_trials=200, n_jobs=-1)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "best_params = study.best_params\n",
        "best_score = study.best_value\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best ROC-AUC Score:\", best_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6a-mBGvKrCz",
        "outputId": "b29f5878-3d58-4021-fae0-751088a91efb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.5996364501223158\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Assuming you have your training and evaluation data as X_train, y_train, X_eval, y_eval, and test data as X_test, y_test\n",
        "\n",
        "# Define the parameters\n",
        "params = {'max_depth': 9, 'n_estimators': 429, \n",
        "          'learning_rate': 0.785461337067571, \n",
        "          'subsample': 0.7716244833348449, \n",
        "          'colsample_bytree': 0.2970813261191314, \n",
        "          'min_child_weight': 0.27106750258415757}\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model = lgb.LGBMClassifier(**params)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S3-ckwnOmPq"
      },
      "source": [
        "Optuna CMA es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK5sWz4CLy3Y",
        "outputId": "adfe37a7-6193-43d3-9978-b1b34dcee9d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-25 14:05:27,840]\u001b[0m A new study created in memory with name: no-name-41dd3321-b802-4f5b-ad03-1dd1e2f5bee8\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:29,311]\u001b[0m Trial 0 finished with value: -0.7060940331598228 and parameters: {'max_depth': 3, 'n_estimators': 507, 'learning_rate': 0.49476589130576076, 'subsample': 0.9020776914051981, 'colsample_bytree': 0.6597056269275677, 'min_child_weight': 5.045128028844567}. Best is trial 0 with value: -0.7060940331598228.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:29,893]\u001b[0m Trial 1 finished with value: -0.6802005775657091 and parameters: {'max_depth': 8, 'n_estimators': 477, 'learning_rate': 0.457262131086608, 'subsample': 0.2195707793567391, 'colsample_bytree': 0.7529799156865982, 'min_child_weight': 8.414283199551413}. Best is trial 1 with value: -0.6802005775657091.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:31,020]\u001b[0m Trial 2 finished with value: -0.6834266354990038 and parameters: {'max_depth': 9, 'n_estimators': 403, 'learning_rate': 0.43185778642083034, 'subsample': 0.5926737137629609, 'colsample_bytree': 0.5847923353911374, 'min_child_weight': 7.691734835082303}. Best is trial 1 with value: -0.6802005775657091.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:31,564]\u001b[0m Trial 3 finished with value: -0.6642315885868518 and parameters: {'max_depth': 9, 'n_estimators': 387, 'learning_rate': 0.5478718182344426, 'subsample': 0.6163138794411857, 'colsample_bytree': 0.6960060527292582, 'min_child_weight': 3.8074809142419315}. Best is trial 3 with value: -0.6642315885868518.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:32,161]\u001b[0m Trial 4 finished with value: -0.6574709188837479 and parameters: {'max_depth': 9, 'n_estimators': 416, 'learning_rate': 0.6471524400161321, 'subsample': 0.50259476076273, 'colsample_bytree': 0.38932686789021365, 'min_child_weight': 6.215985120263557}. Best is trial 4 with value: -0.6574709188837479.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:32,421]\u001b[0m Trial 5 finished with value: -0.6851121714542768 and parameters: {'max_depth': 5, 'n_estimators': 299, 'learning_rate': 0.6042229598006696, 'subsample': 0.4962432247626919, 'colsample_bytree': 0.4123523751609426, 'min_child_weight': 7.683265000023419}. Best is trial 4 with value: -0.6574709188837479.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:33,173]\u001b[0m Trial 7 finished with value: -0.6648199304350622 and parameters: {'max_depth': 10, 'n_estimators': 359, 'learning_rate': 0.7528344079082332, 'subsample': 0.38822416262715653, 'colsample_bytree': 0.6982263283041975, 'min_child_weight': 6.51690913945913}. Best is trial 4 with value: -0.6574709188837479.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:33,327]\u001b[0m Trial 6 finished with value: -0.6796239758048969 and parameters: {'max_depth': 10, 'n_estimators': 332, 'learning_rate': 0.3015090149968594, 'subsample': 0.5946051550503123, 'colsample_bytree': 0.3684514917653846, 'min_child_weight': 4.91925502948945}. Best is trial 4 with value: -0.6574709188837479.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:34,066]\u001b[0m Trial 9 finished with value: -0.655655747943577 and parameters: {'max_depth': 10, 'n_estimators': 555, 'learning_rate': 0.4492714166024656, 'subsample': 0.39577668344147, 'colsample_bytree': 0.6449781404617401, 'min_child_weight': 3.269021145948262}. Best is trial 9 with value: -0.655655747943577.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:34,186]\u001b[0m Trial 8 finished with value: -0.670460652834008 and parameters: {'max_depth': 12, 'n_estimators': 350, 'learning_rate': 0.3473706393701421, 'subsample': 0.37910166220923747, 'colsample_bytree': 0.43672726086948344, 'min_child_weight': 2.070594810878292}. Best is trial 9 with value: -0.655655747943577.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:34,911]\u001b[0m Trial 10 finished with value: -0.6527677225917358 and parameters: {'max_depth': 7, 'n_estimators': 331, 'learning_rate': 0.5985579760242119, 'subsample': 0.517895771154284, 'colsample_bytree': 0.5288813086565903, 'min_child_weight': 2.296313124580726}. Best is trial 10 with value: -0.6527677225917358.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:35,318]\u001b[0m Trial 11 finished with value: -0.6633945279866331 and parameters: {'max_depth': 12, 'n_estimators': 362, 'learning_rate': 0.3854762555508109, 'subsample': 0.5907315729133689, 'colsample_bytree': 0.44732507878641947, 'min_child_weight': 3.2141022444091343}. Best is trial 10 with value: -0.6527677225917358.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:35,818]\u001b[0m Trial 12 finished with value: -0.6408013704132126 and parameters: {'max_depth': 8, 'n_estimators': 410, 'learning_rate': 0.7325337598396301, 'subsample': 0.40671824510938914, 'colsample_bytree': 0.7758986234764987, 'min_child_weight': 4.0275689916443005}. Best is trial 12 with value: -0.6408013704132126.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:36,055]\u001b[0m Trial 13 finished with value: -0.6844081497815052 and parameters: {'max_depth': 4, 'n_estimators': 393, 'learning_rate': 0.6714958420920228, 'subsample': 0.5533265324221571, 'colsample_bytree': 0.595367390892788, 'min_child_weight': 2.575703942246793}. Best is trial 12 with value: -0.6408013704132126.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:36,473]\u001b[0m Trial 14 finished with value: -0.7068293811451708 and parameters: {'max_depth': 3, 'n_estimators': 368, 'learning_rate': 0.2597499468193589, 'subsample': 0.18120890935841766, 'colsample_bytree': 0.5281931537160813, 'min_child_weight': 0.35178047713349414}. Best is trial 12 with value: -0.6408013704132126.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:37,176]\u001b[0m Trial 15 finished with value: -0.6727259454726562 and parameters: {'max_depth': 9, 'n_estimators': 308, 'learning_rate': 0.3280614131105095, 'subsample': 0.6159275600504912, 'colsample_bytree': 0.4857272769398375, 'min_child_weight': 1.8542192381585376}. Best is trial 12 with value: -0.6408013704132126.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:37,434]\u001b[0m Trial 16 finished with value: -0.6797838667180773 and parameters: {'max_depth': 7, 'n_estimators': 465, 'learning_rate': 0.3926836555080755, 'subsample': 0.4111071461725506, 'colsample_bytree': 0.5876202812976574, 'min_child_weight': 4.2225354597123586}. Best is trial 12 with value: -0.6408013704132126.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:37,998]\u001b[0m Trial 17 finished with value: -0.6573294574577468 and parameters: {'max_depth': 7, 'n_estimators': 498, 'learning_rate': 0.47651120492274973, 'subsample': 0.5409835646685625, 'colsample_bytree': 0.46718430915201636, 'min_child_weight': 0.7057259016001858}. Best is trial 12 with value: -0.6408013704132126.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:38,322]\u001b[0m Trial 18 finished with value: -0.6632696846282372 and parameters: {'max_depth': 7, 'n_estimators': 497, 'learning_rate': 0.4138992387709535, 'subsample': 0.5075603536989028, 'colsample_bytree': 0.45229367138516774, 'min_child_weight': 2.474997316407151}. Best is trial 12 with value: -0.6408013704132126.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:38,825]\u001b[0m Trial 19 finished with value: -0.6247707550125312 and parameters: {'max_depth': 9, 'n_estimators': 494, 'learning_rate': 0.794688558549423, 'subsample': 0.3371949253920797, 'colsample_bytree': 0.31947737090190576, 'min_child_weight': 4.488323686126518}. Best is trial 19 with value: -0.6247707550125312.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:39,171]\u001b[0m Trial 20 finished with value: -0.6545252936025963 and parameters: {'max_depth': 9, 'n_estimators': 425, 'learning_rate': 0.5501422485795147, 'subsample': 0.5609286468332167, 'colsample_bytree': 0.5631936518536854, 'min_child_weight': 1.772442488030902}. Best is trial 19 with value: -0.6247707550125312.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:39,640]\u001b[0m Trial 21 finished with value: -0.6526244537626117 and parameters: {'max_depth': 8, 'n_estimators': 359, 'learning_rate': 0.47435014772475764, 'subsample': 0.6083481011192057, 'colsample_bytree': 0.5975232339436722, 'min_child_weight': 2.907259206674864}. Best is trial 19 with value: -0.6247707550125312.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:40,187]\u001b[0m Trial 22 finished with value: -0.6490269142407302 and parameters: {'max_depth': 9, 'n_estimators': 459, 'learning_rate': 0.8507949726751252, 'subsample': 0.1929222817508148, 'colsample_bytree': 0.6488098259377288, 'min_child_weight': 3.5395478299259855}. Best is trial 19 with value: -0.6247707550125312.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:40,786]\u001b[0m Trial 23 finished with value: -0.634108572713836 and parameters: {'max_depth': 11, 'n_estimators': 445, 'learning_rate': 0.6709571117889894, 'subsample': 0.44375283554897627, 'colsample_bytree': 0.4710562842971421, 'min_child_weight': 0.6111453776000475}. Best is trial 19 with value: -0.6247707550125312.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:41,642]\u001b[0m Trial 24 finished with value: -0.601418146729002 and parameters: {'max_depth': 9, 'n_estimators': 564, 'learning_rate': 0.8289550520446232, 'subsample': 0.6080772469585258, 'colsample_bytree': 0.32770863646707804, 'min_child_weight': 2.794537695202727}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:42,551]\u001b[0m Trial 25 finished with value: -0.6540294405886512 and parameters: {'max_depth': 9, 'n_estimators': 349, 'learning_rate': 0.6333069802774464, 'subsample': 0.501881830106339, 'colsample_bytree': 0.47592245629252794, 'min_child_weight': 3.8578348858483964}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:43,441]\u001b[0m Trial 26 finished with value: -0.6450786903155323 and parameters: {'max_depth': 7, 'n_estimators': 413, 'learning_rate': 0.6852249061412607, 'subsample': 0.5539635828410289, 'colsample_bytree': 0.59958885307561, 'min_child_weight': 2.37825522092812}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:44,431]\u001b[0m Trial 27 finished with value: -0.6566275564713064 and parameters: {'max_depth': 10, 'n_estimators': 451, 'learning_rate': 0.545235829249416, 'subsample': 0.31644569723155314, 'colsample_bytree': 0.6190146126310971, 'min_child_weight': 4.296228958923722}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:45,235]\u001b[0m Trial 28 finished with value: -0.6402116830537884 and parameters: {'max_depth': 8, 'n_estimators': 499, 'learning_rate': 0.7314812874932367, 'subsample': 0.43248427029094316, 'colsample_bytree': 0.5701484116129303, 'min_child_weight': 1.5707256268180276}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:45,761]\u001b[0m Trial 29 finished with value: -0.6663988275978407 and parameters: {'max_depth': 9, 'n_estimators': 533, 'learning_rate': 0.4331041606804904, 'subsample': 0.6394187057163307, 'colsample_bytree': 0.9740775609457903, 'min_child_weight': 1.4423613044326211}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:46,156]\u001b[0m Trial 30 finished with value: -0.6407796614934773 and parameters: {'max_depth': 8, 'n_estimators': 579, 'learning_rate': 0.8388383093901187, 'subsample': 0.4043209890653927, 'colsample_bytree': 0.508639952172737, 'min_child_weight': 3.929728669949935}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:46,655]\u001b[0m Trial 31 finished with value: -0.6475030605359553 and parameters: {'max_depth': 10, 'n_estimators': 385, 'learning_rate': 0.8068190749344768, 'subsample': 0.2487878727687717, 'colsample_bytree': 0.5768999798247951, 'min_child_weight': 4.499369589341072}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:46,986]\u001b[0m Trial 32 finished with value: -0.6659078003502346 and parameters: {'max_depth': 7, 'n_estimators': 352, 'learning_rate': 0.6529015989969371, 'subsample': 0.4194204011203915, 'colsample_bytree': 0.5230779483858858, 'min_child_weight': 5.960195470575313}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:47,579]\u001b[0m Trial 33 finished with value: -0.6318876859616991 and parameters: {'max_depth': 8, 'n_estimators': 470, 'learning_rate': 0.8723868949241793, 'subsample': 0.6523448394289318, 'colsample_bytree': 0.6461027645022611, 'min_child_weight': 1.8614761842022973}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:47,948]\u001b[0m Trial 34 finished with value: -0.6338881317878028 and parameters: {'max_depth': 9, 'n_estimators': 392, 'learning_rate': 0.8745895073884989, 'subsample': 0.4961669258837319, 'colsample_bytree': 0.3757820298377146, 'min_child_weight': 2.104953093987275}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:48,449]\u001b[0m Trial 35 finished with value: -0.6348076180033416 and parameters: {'max_depth': 9, 'n_estimators': 493, 'learning_rate': 0.931568821045983, 'subsample': 0.48465730169476207, 'colsample_bytree': 0.37092322212473083, 'min_child_weight': 1.0229204521846382}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:48,768]\u001b[0m Trial 36 finished with value: -0.6352558158216053 and parameters: {'max_depth': 11, 'n_estimators': 506, 'learning_rate': 0.7461878849471311, 'subsample': 0.5797706901567172, 'colsample_bytree': 0.5613546065327142, 'min_child_weight': 2.740053317307963}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:49,169]\u001b[0m Trial 37 finished with value: -0.6390339390142022 and parameters: {'max_depth': 7, 'n_estimators': 471, 'learning_rate': 0.98530028017783, 'subsample': 0.6811935430505727, 'colsample_bytree': 0.3628692544547507, 'min_child_weight': 2.644269593315526}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:49,608]\u001b[0m Trial 38 finished with value: -0.6414705774853802 and parameters: {'max_depth': 8, 'n_estimators': 597, 'learning_rate': 0.8186630709023143, 'subsample': 0.3799406023041606, 'colsample_bytree': 0.7034675449442046, 'min_child_weight': 1.3606450353348019}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:50,115]\u001b[0m Trial 39 finished with value: -0.6488331124285073 and parameters: {'max_depth': 9, 'n_estimators': 522, 'learning_rate': 0.7833320721559416, 'subsample': 0.761611997322197, 'colsample_bytree': 0.8547736403406283, 'min_child_weight': 3.157797383807807}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:50,436]\u001b[0m Trial 40 finished with value: -0.6563148897885739 and parameters: {'max_depth': 10, 'n_estimators': 452, 'learning_rate': 0.5386144296443744, 'subsample': 0.6429824378260398, 'colsample_bytree': 0.4006369230210398, 'min_child_weight': 5.292279950961122}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:50,883]\u001b[0m Trial 41 finished with value: -0.6464488163517769 and parameters: {'max_depth': 8, 'n_estimators': 516, 'learning_rate': 0.5469521964657877, 'subsample': 0.3818807259528103, 'colsample_bytree': 0.4065908568181724, 'min_child_weight': 1.8732895041057596}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:51,234]\u001b[0m Trial 42 finished with value: -0.6409960096555491 and parameters: {'max_depth': 9, 'n_estimators': 434, 'learning_rate': 0.8839181653629897, 'subsample': 0.29600763742911773, 'colsample_bytree': 0.7329576379555728, 'min_child_weight': 2.0857236602512015}. Best is trial 24 with value: -0.601418146729002.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:51,637]\u001b[0m Trial 43 finished with value: -0.5949288445472656 and parameters: {'max_depth': 11, 'n_estimators': 508, 'learning_rate': 0.8562955236766502, 'subsample': 0.9273332927620199, 'colsample_bytree': 0.30719316432526583, 'min_child_weight': 1.23353689051707}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:52,060]\u001b[0m Trial 44 finished with value: -0.6521886045241309 and parameters: {'max_depth': 6, 'n_estimators': 419, 'learning_rate': 0.6870888627555389, 'subsample': 0.25711979584871614, 'colsample_bytree': 0.6221829113497932, 'min_child_weight': 3.109843385817068}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:52,320]\u001b[0m Trial 45 finished with value: -0.6352122875297217 and parameters: {'max_depth': 9, 'n_estimators': 549, 'learning_rate': 0.7216275392927735, 'subsample': 0.5984842803897992, 'colsample_bytree': 0.3959450355429044, 'min_child_weight': 1.7663211798755958}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:52,866]\u001b[0m Trial 46 finished with value: -0.6189193135081293 and parameters: {'max_depth': 9, 'n_estimators': 553, 'learning_rate': 0.5717391179872376, 'subsample': 0.2485230050566345, 'colsample_bytree': 0.28317618590801175, 'min_child_weight': 2.2994382723255735}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:53,194]\u001b[0m Trial 47 finished with value: -0.642848744457297 and parameters: {'max_depth': 11, 'n_estimators': 372, 'learning_rate': 0.8543255486791039, 'subsample': 0.5209748283602182, 'colsample_bytree': 0.7302209762767514, 'min_child_weight': 0.12804776763565848}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:53,514]\u001b[0m Trial 48 finished with value: -0.6564151082835292 and parameters: {'max_depth': 5, 'n_estimators': 420, 'learning_rate': 0.9505822701921962, 'subsample': 0.9061443707773942, 'colsample_bytree': 0.47351562960891336, 'min_child_weight': 1.576807137796871}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:54,153]\u001b[0m Trial 49 finished with value: -0.6496514342747896 and parameters: {'max_depth': 8, 'n_estimators': 337, 'learning_rate': 0.6904036780403867, 'subsample': 0.7981975795777853, 'colsample_bytree': 0.7305697805789232, 'min_child_weight': 3.256152275152576}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:54,421]\u001b[0m Trial 50 finished with value: -0.647481678957008 and parameters: {'max_depth': 8, 'n_estimators': 447, 'learning_rate': 0.5959798724456357, 'subsample': 0.4876876740456175, 'colsample_bytree': 0.654853768218787, 'min_child_weight': 2.1834898985920037}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:54,882]\u001b[0m Trial 51 finished with value: -0.6536604491999227 and parameters: {'max_depth': 6, 'n_estimators': 575, 'learning_rate': 0.7528069865994751, 'subsample': 0.7871235400266126, 'colsample_bytree': 0.6877696601829657, 'min_child_weight': 2.132257457053216}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:55,266]\u001b[0m Trial 52 finished with value: -0.6294965016708438 and parameters: {'max_depth': 10, 'n_estimators': 483, 'learning_rate': 0.5110054138405953, 'subsample': 0.8782684278611652, 'colsample_bytree': 0.32404319862803843, 'min_child_weight': 0.5180183084853233}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:56,200]\u001b[0m Trial 53 finished with value: -0.6431215378189062 and parameters: {'max_depth': 11, 'n_estimators': 451, 'learning_rate': 0.6023488229731171, 'subsample': 0.9278314225378463, 'colsample_bytree': 0.4265266923143527, 'min_child_weight': 0.7830993337643979}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:57,280]\u001b[0m Trial 54 finished with value: -0.6462115324368615 and parameters: {'max_depth': 12, 'n_estimators': 571, 'learning_rate': 0.5151103648202238, 'subsample': 0.7456161553442419, 'colsample_bytree': 0.5442873079608155, 'min_child_weight': 1.2503653399572714}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:57,914]\u001b[0m Trial 55 finished with value: -0.5997545345736135 and parameters: {'max_depth': 10, 'n_estimators': 506, 'learning_rate': 0.8731030788576463, 'subsample': 0.7482105096316782, 'colsample_bytree': 0.2914885021549747, 'min_child_weight': 2.4466265022470357}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:58,892]\u001b[0m Trial 56 finished with value: -0.6384728869449264 and parameters: {'max_depth': 9, 'n_estimators': 405, 'learning_rate': 0.8164849129450358, 'subsample': 0.5061069645342346, 'colsample_bytree': 0.44733455610890915, 'min_child_weight': 0.7555869275089221}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:05:59,593]\u001b[0m Trial 57 finished with value: -0.6428755743525482 and parameters: {'max_depth': 9, 'n_estimators': 483, 'learning_rate': 0.6950839548731301, 'subsample': 0.6371396498033407, 'colsample_bytree': 0.4186890917541708, 'min_child_weight': 3.4555198127097033}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:00,185]\u001b[0m Trial 58 finished with value: -0.6425177446822182 and parameters: {'max_depth': 7, 'n_estimators': 523, 'learning_rate': 0.7920383685722332, 'subsample': 0.5704171224758533, 'colsample_bytree': 0.15717087059188728, 'min_child_weight': 1.284748212616894}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:00,697]\u001b[0m Trial 59 finished with value: -0.639837207200694 and parameters: {'max_depth': 8, 'n_estimators': 330, 'learning_rate': 0.6876133180623911, 'subsample': 0.45378876289258796, 'colsample_bytree': 0.1552849190645212, 'min_child_weight': 0.6479432456920112}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:00,926]\u001b[0m Trial 60 finished with value: -0.6662446480785298 and parameters: {'max_depth': 9, 'n_estimators': 486, 'learning_rate': 0.8103426044690809, 'subsample': 0.655198047769649, 'colsample_bytree': 0.11195561436402814, 'min_child_weight': 4.454637098797948}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:01,638]\u001b[0m Trial 61 finished with value: -0.5971664939592571 and parameters: {'max_depth': 13, 'n_estimators': 427, 'learning_rate': 0.9727271675907262, 'subsample': 0.6109051161353322, 'colsample_bytree': 0.2697110164845633, 'min_child_weight': 2.287120241177068}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:01,853]\u001b[0m Trial 62 finished with value: -0.6361112054977186 and parameters: {'max_depth': 8, 'n_estimators': 358, 'learning_rate': 0.978922046584942, 'subsample': 0.6715554331269626, 'colsample_bytree': 0.4818714468254861, 'min_child_weight': 3.9707264077471764}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:02,477]\u001b[0m Trial 63 finished with value: -0.61775184154296 and parameters: {'max_depth': 10, 'n_estimators': 460, 'learning_rate': 0.5615970788146468, 'subsample': 0.7134847892033296, 'colsample_bytree': 0.3269419656270174, 'min_child_weight': 1.645833503046101}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:02,661]\u001b[0m Trial 64 finished with value: -0.6439343229869546 and parameters: {'max_depth': 12, 'n_estimators': 475, 'learning_rate': 0.8747110287934713, 'subsample': 0.7478470093675663, 'colsample_bytree': 0.15877402742245744, 'min_child_weight': 1.9688814117985558}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:03,393]\u001b[0m Trial 65 finished with value: -0.6326555451127819 and parameters: {'max_depth': 8, 'n_estimators': 453, 'learning_rate': 0.8468988914191923, 'subsample': 0.6467995153854771, 'colsample_bytree': 0.45747199289315565, 'min_child_weight': 1.3510987570325603}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:04,347]\u001b[0m Trial 67 finished with value: -0.6431820597165991 and parameters: {'max_depth': 11, 'n_estimators': 523, 'learning_rate': 0.6382903462520843, 'subsample': 0.6765179616692104, 'colsample_bytree': 0.3821917966862869, 'min_child_weight': 1.9864797773083636}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:05,326]\u001b[0m Trial 68 finished with value: -0.6360603270998008 and parameters: {'max_depth': 10, 'n_estimators': 383, 'learning_rate': 0.7185940986574665, 'subsample': 0.879801820808345, 'colsample_bytree': 0.4689379015180164, 'min_child_weight': 1.9964193019033365}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:06,153]\u001b[0m Trial 69 finished with value: -0.605909631852066 and parameters: {'max_depth': 10, 'n_estimators': 403, 'learning_rate': 0.7854987886634978, 'subsample': 0.7043423743578494, 'colsample_bytree': 0.2881863932652861, 'min_child_weight': 0.7407955490770733}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:06,692]\u001b[0m Trial 66 finished with value: -0.6815636567379989 and parameters: {'max_depth': 9, 'n_estimators': 459, 'learning_rate': 0.043841452626417324, 'subsample': 0.8096597332460865, 'colsample_bytree': 0.3100856748169334, 'min_child_weight': 3.529885187623291}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:07,037]\u001b[0m Trial 70 finished with value: -0.6494180583670716 and parameters: {'max_depth': 11, 'n_estimators': 478, 'learning_rate': 0.5551319484746662, 'subsample': 0.7931216899650422, 'colsample_bytree': 0.1773402233307803, 'min_child_weight': 2.1530685461921175}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:07,593]\u001b[0m Trial 71 finished with value: -0.6029901155131419 and parameters: {'max_depth': 11, 'n_estimators': 516, 'learning_rate': 0.8115521852420466, 'subsample': 0.6170376326724003, 'colsample_bytree': 0.3395039883905192, 'min_child_weight': 1.5717528410672217}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:07,985]\u001b[0m Trial 72 finished with value: -0.6383445814054368 and parameters: {'max_depth': 10, 'n_estimators': 330, 'learning_rate': 0.734855534773736, 'subsample': 0.7051163238617483, 'colsample_bytree': 0.394275376929872, 'min_child_weight': 1.7386846082506677}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:08,521]\u001b[0m Trial 73 finished with value: -0.636122969683825 and parameters: {'max_depth': 10, 'n_estimators': 353, 'learning_rate': 0.7153885035684939, 'subsample': 0.6052060437854329, 'colsample_bytree': 0.3703070725752434, 'min_child_weight': 1.0426109859456196}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:08,952]\u001b[0m Trial 74 finished with value: -0.6453725439399781 and parameters: {'max_depth': 11, 'n_estimators': 558, 'learning_rate': 0.6819786430808551, 'subsample': 0.7455211752917016, 'colsample_bytree': 0.1741825969321938, 'min_child_weight': 0.42731681051463233}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:09,390]\u001b[0m Trial 75 finished with value: -0.6007595209980079 and parameters: {'max_depth': 9, 'n_estimators': 538, 'learning_rate': 0.7218295035170842, 'subsample': 0.6277298868078407, 'colsample_bytree': 0.26063818292626184, 'min_child_weight': 1.2380882675030156}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:09,963]\u001b[0m Trial 76 finished with value: -0.6310914224664225 and parameters: {'max_depth': 12, 'n_estimators': 437, 'learning_rate': 0.5120517015199507, 'subsample': 0.6968541648629057, 'colsample_bytree': 0.26269859108307014, 'min_child_weight': 0.38785278352213537}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:10,386]\u001b[0m Trial 77 finished with value: -0.6382314138712164 and parameters: {'max_depth': 8, 'n_estimators': 493, 'learning_rate': 0.6366227650770699, 'subsample': 0.4472772500783573, 'colsample_bytree': 0.4131323590956548, 'min_child_weight': 1.3851373508340754}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:11,481]\u001b[0m Trial 78 finished with value: -0.6415961237227684 and parameters: {'max_depth': 10, 'n_estimators': 555, 'learning_rate': 0.9185322929528728, 'subsample': 0.8108154486157281, 'colsample_bytree': 0.3591450533388708, 'min_child_weight': 2.7757939169520816}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:12,274]\u001b[0m Trial 79 finished with value: -0.623561905565195 and parameters: {'max_depth': 11, 'n_estimators': 464, 'learning_rate': 0.5752819144274313, 'subsample': 0.7703923385281356, 'colsample_bytree': 0.34391007286291725, 'min_child_weight': 0.3577906317650825}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:13,618]\u001b[0m Trial 80 finished with value: -0.6506133302005014 and parameters: {'max_depth': 9, 'n_estimators': 297, 'learning_rate': 0.5379814605588695, 'subsample': 0.7312938570351177, 'colsample_bytree': 0.5018346446042271, 'min_child_weight': 1.2743961582506682}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:14,092]\u001b[0m Trial 81 finished with value: -0.636787728536084 and parameters: {'max_depth': 9, 'n_estimators': 433, 'learning_rate': 0.7874970070708236, 'subsample': 0.7205049151843218, 'colsample_bytree': 0.4981762000378712, 'min_child_weight': 2.6931590066361584}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:14,987]\u001b[0m Trial 82 finished with value: -0.6061994529593214 and parameters: {'max_depth': 8, 'n_estimators': 504, 'learning_rate': 0.6456909604458707, 'subsample': 0.43381276123108337, 'colsample_bytree': 0.25690387410144366, 'min_child_weight': 1.3004339417059716}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:15,236]\u001b[0m Trial 83 finished with value: -0.6409786666184693 and parameters: {'max_depth': 8, 'n_estimators': 504, 'learning_rate': 0.6084483397827071, 'subsample': 0.6073335734835277, 'colsample_bytree': 0.3908271121575595, 'min_child_weight': 1.321785169400544}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:15,854]\u001b[0m Trial 84 finished with value: -0.6491007085020243 and parameters: {'max_depth': 9, 'n_estimators': 316, 'learning_rate': 0.47202480439148514, 'subsample': 0.8102768969943843, 'colsample_bytree': 0.3529971083646284, 'min_child_weight': 0.36201109893693695}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:16,242]\u001b[0m Trial 85 finished with value: -0.6400632350106036 and parameters: {'max_depth': 11, 'n_estimators': 529, 'learning_rate': 0.587569012145873, 'subsample': 0.5982502276036373, 'colsample_bytree': 0.4640333999174491, 'min_child_weight': 1.5529649012473599}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:16,817]\u001b[0m Trial 86 finished with value: -0.636605006105006 and parameters: {'max_depth': 11, 'n_estimators': 516, 'learning_rate': 0.6489601758417783, 'subsample': 0.5818971562247756, 'colsample_bytree': 0.3596187594183524, 'min_child_weight': 1.5572682277898935}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:17,224]\u001b[0m Trial 87 finished with value: -0.629419988271962 and parameters: {'max_depth': 9, 'n_estimators': 488, 'learning_rate': 0.8440316403224762, 'subsample': 0.532919298927975, 'colsample_bytree': 0.6269140298843386, 'min_child_weight': 0.7682353286956137}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:17,565]\u001b[0m Trial 88 finished with value: -0.6097317331791016 and parameters: {'max_depth': 11, 'n_estimators': 478, 'learning_rate': 0.6930517422439074, 'subsample': 0.675649759970808, 'colsample_bytree': 0.33805276520023275, 'min_child_weight': 1.7528880095253856}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:18,022]\u001b[0m Trial 89 finished with value: -0.6448919052438788 and parameters: {'max_depth': 9, 'n_estimators': 545, 'learning_rate': 0.6807838048955912, 'subsample': 0.6012563826386268, 'colsample_bytree': 0.2292476423354322, 'min_child_weight': 0.41737082369917566}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:18,344]\u001b[0m Trial 90 finished with value: -0.665051452750466 and parameters: {'max_depth': 11, 'n_estimators': 390, 'learning_rate': 0.4699318711747276, 'subsample': 0.8010386426475191, 'colsample_bytree': 0.42840046920491104, 'min_child_weight': 5.363450020828345}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:19,019]\u001b[0m Trial 91 finished with value: -0.6538103752972173 and parameters: {'max_depth': 9, 'n_estimators': 497, 'learning_rate': 0.5132756585947288, 'subsample': 0.7600397013102334, 'colsample_bytree': 0.22987954901447688, 'min_child_weight': 0.42543647909881366}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:19,203]\u001b[0m Trial 92 finished with value: -0.6359123951706189 and parameters: {'max_depth': 10, 'n_estimators': 398, 'learning_rate': 0.718322147195606, 'subsample': 0.465745797473357, 'colsample_bytree': 0.3828821295060191, 'min_child_weight': 0.980099474851752}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:20,023]\u001b[0m Trial 94 finished with value: -0.5991280565195038 and parameters: {'max_depth': 9, 'n_estimators': 519, 'learning_rate': 0.8662734229540938, 'subsample': 0.5954738380969652, 'colsample_bytree': 0.2653786768044706, 'min_child_weight': 2.272232264656007}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:20,027]\u001b[0m Trial 93 finished with value: -0.6168052101407365 and parameters: {'max_depth': 12, 'n_estimators': 487, 'learning_rate': 0.6422759463980616, 'subsample': 0.5102482630619252, 'colsample_bytree': 0.30166018573337755, 'min_child_weight': 0.2924753150112398}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:20,900]\u001b[0m Trial 95 finished with value: -0.6187956469699891 and parameters: {'max_depth': 8, 'n_estimators': 461, 'learning_rate': 0.6184397782564137, 'subsample': 0.7426812747940504, 'colsample_bytree': 0.31502053509394246, 'min_child_weight': 1.32782157484999}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:20,978]\u001b[0m Trial 96 finished with value: -0.6081166959867618 and parameters: {'max_depth': 9, 'n_estimators': 501, 'learning_rate': 0.625646768563063, 'subsample': 0.6313880335971118, 'colsample_bytree': 0.3066795385754689, 'min_child_weight': 2.021899904238685}. Best is trial 43 with value: -0.5949288445472656.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:21,782]\u001b[0m Trial 98 finished with value: -0.5905607930081616 and parameters: {'max_depth': 9, 'n_estimators': 505, 'learning_rate': 0.8056173890081537, 'subsample': 0.5038652060058203, 'colsample_bytree': 0.2944652686714238, 'min_child_weight': 1.8641070019820825}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:21,803]\u001b[0m Trial 97 finished with value: -0.637588604925776 and parameters: {'max_depth': 8, 'n_estimators': 471, 'learning_rate': 0.7257695074599904, 'subsample': 0.7676445206883962, 'colsample_bytree': 0.3558590732600301, 'min_child_weight': 1.3340552722621957}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:22,686]\u001b[0m Trial 100 finished with value: -0.6393348736424395 and parameters: {'max_depth': 9, 'n_estimators': 533, 'learning_rate': 0.8960127648562574, 'subsample': 0.5491918190077908, 'colsample_bytree': 0.41036921720992026, 'min_child_weight': 3.006157470511489}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:22,766]\u001b[0m Trial 99 finished with value: -0.5998849186266949 and parameters: {'max_depth': 10, 'n_estimators': 556, 'learning_rate': 0.8845234833344033, 'subsample': 0.642476903834036, 'colsample_bytree': 0.3423733665491251, 'min_child_weight': 1.4485522817026346}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:23,660]\u001b[0m Trial 101 finished with value: -0.5989499971884841 and parameters: {'max_depth': 11, 'n_estimators': 463, 'learning_rate': 0.7646003962178284, 'subsample': 0.45640689091829256, 'colsample_bytree': 0.34348701525028413, 'min_child_weight': 2.351732722185259}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:23,812]\u001b[0m Trial 102 finished with value: -0.6504582791112395 and parameters: {'max_depth': 10, 'n_estimators': 500, 'learning_rate': 0.7629692492349467, 'subsample': 0.5179518652151098, 'colsample_bytree': 0.19412584879275133, 'min_child_weight': 2.703056035424618}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:24,671]\u001b[0m Trial 104 finished with value: -0.6381787140125957 and parameters: {'max_depth': 8, 'n_estimators': 516, 'learning_rate': 0.9293918541602844, 'subsample': 0.5446937192013285, 'colsample_bytree': 0.36948065064587565, 'min_child_weight': 1.8080700502516531}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:24,790]\u001b[0m Trial 103 finished with value: -0.612170458196774 and parameters: {'max_depth': 10, 'n_estimators': 469, 'learning_rate': 0.9187217087739034, 'subsample': 0.5468506614612084, 'colsample_bytree': 0.32272269163099254, 'min_child_weight': 3.157173026486527}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:26,195]\u001b[0m Trial 106 finished with value: -0.6654372690540454 and parameters: {'max_depth': 10, 'n_estimators': 475, 'learning_rate': 0.6351124250737442, 'subsample': 0.6180656745129249, 'colsample_bytree': 0.10199807452347372, 'min_child_weight': 1.3726773682261937}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:26,199]\u001b[0m Trial 105 finished with value: -0.6376550832208726 and parameters: {'max_depth': 7, 'n_estimators': 405, 'learning_rate': 0.7499610812682336, 'subsample': 0.5049528687288115, 'colsample_bytree': 0.3653029846632865, 'min_child_weight': 1.7558887530553429}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:27,937]\u001b[0m Trial 107 finished with value: -0.5961137137716085 and parameters: {'max_depth': 10, 'n_estimators': 553, 'learning_rate': 0.9403121886017939, 'subsample': 0.4340390255148703, 'colsample_bytree': 0.27923374326012507, 'min_child_weight': 1.2721211637561836}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:27,957]\u001b[0m Trial 108 finished with value: -0.6431684057901165 and parameters: {'max_depth': 9, 'n_estimators': 431, 'learning_rate': 0.8903276478033008, 'subsample': 0.6521817130661526, 'colsample_bytree': 0.24687845480625198, 'min_child_weight': 0.5135063140992872}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:29,491]\u001b[0m Trial 109 finished with value: -0.6388442379988433 and parameters: {'max_depth': 8, 'n_estimators': 585, 'learning_rate': 0.8358269262278057, 'subsample': 0.6650182145835105, 'colsample_bytree': 0.4233070157958594, 'min_child_weight': 3.2072901147166792}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:29,571]\u001b[0m Trial 110 finished with value: -0.6464462317653107 and parameters: {'max_depth': 10, 'n_estimators': 547, 'learning_rate': 0.8252373916291393, 'subsample': 0.5405802789434621, 'colsample_bytree': 0.24485139021140195, 'min_child_weight': 0.9916363135362678}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:30,156]\u001b[0m Trial 112 finished with value: -0.6079340157284235 and parameters: {'max_depth': 8, 'n_estimators': 586, 'learning_rate': 0.9808174637800903, 'subsample': 0.3650706772938468, 'colsample_bytree': 0.3361771985375203, 'min_child_weight': 3.5405516984181356}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:30,368]\u001b[0m Trial 111 finished with value: -0.604959994135981 and parameters: {'max_depth': 11, 'n_estimators': 538, 'learning_rate': 0.7996401995818461, 'subsample': 0.6536403852726761, 'colsample_bytree': 0.33403522389929674, 'min_child_weight': 2.2788840684233156}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:30,830]\u001b[0m Trial 113 finished with value: -0.5941887069436411 and parameters: {'max_depth': 9, 'n_estimators': 526, 'learning_rate': 0.9443177757205736, 'subsample': 0.24562355038097947, 'colsample_bytree': 0.3443696035951933, 'min_child_weight': 0.2437938903976124}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:31,230]\u001b[0m Trial 114 finished with value: -0.6413215811965812 and parameters: {'max_depth': 8, 'n_estimators': 494, 'learning_rate': 0.8813005101363328, 'subsample': 0.5604430766016967, 'colsample_bytree': 0.20712993810183258, 'min_child_weight': 0.8609972488881988}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:31,464]\u001b[0m Trial 115 finished with value: -0.66363958775143 and parameters: {'max_depth': 10, 'n_estimators': 536, 'learning_rate': 0.9171527006022286, 'subsample': 0.5468328938118281, 'colsample_bytree': 0.14755063730284354, 'min_child_weight': 0.20372707072441754}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:32,026]\u001b[0m Trial 116 finished with value: -0.6382546871987661 and parameters: {'max_depth': 11, 'n_estimators': 405, 'learning_rate': 0.7918519057096983, 'subsample': 0.3429826523485915, 'colsample_bytree': 0.36427355230200675, 'min_child_weight': 2.274546080389233}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:32,145]\u001b[0m Trial 117 finished with value: -0.6437159645909645 and parameters: {'max_depth': 9, 'n_estimators': 518, 'learning_rate': 0.833477544416524, 'subsample': 0.4829515186626244, 'colsample_bytree': 0.24524088915808717, 'min_child_weight': 2.33568463402414}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:32,780]\u001b[0m Trial 118 finished with value: -0.6033083871537818 and parameters: {'max_depth': 12, 'n_estimators': 431, 'learning_rate': 0.977734804319695, 'subsample': 0.4961641229942022, 'colsample_bytree': 0.297286800819374, 'min_child_weight': 2.7431096494666254}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:33,144]\u001b[0m Trial 119 finished with value: -0.6115368087687167 and parameters: {'max_depth': 12, 'n_estimators': 460, 'learning_rate': 0.9977582525847805, 'subsample': 0.29400569577871344, 'colsample_bytree': 0.3058529379574867, 'min_child_weight': 4.54757211452891}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:33,548]\u001b[0m Trial 120 finished with value: -0.6344612814086498 and parameters: {'max_depth': 11, 'n_estimators': 398, 'learning_rate': 0.776866480306214, 'subsample': 0.46646266151124327, 'colsample_bytree': 0.4111380627590042, 'min_child_weight': 1.626662908357591}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:34,192]\u001b[0m Trial 121 finished with value: -0.6342341751815437 and parameters: {'max_depth': 11, 'n_estimators': 536, 'learning_rate': 0.8744098015910992, 'subsample': 0.5863735966466286, 'colsample_bytree': 0.37484635936035193, 'min_child_weight': 1.6663262805670376}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:34,528]\u001b[0m Trial 122 finished with value: -0.6334316580714607 and parameters: {'max_depth': 9, 'n_estimators': 465, 'learning_rate': 0.9416996043476898, 'subsample': 0.22215364317124026, 'colsample_bytree': 0.47776278589718646, 'min_child_weight': 1.5186387601546423}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:35,148]\u001b[0m Trial 123 finished with value: -0.6351636141636142 and parameters: {'max_depth': 11, 'n_estimators': 490, 'learning_rate': 0.9587564710606681, 'subsample': 0.3235929023248904, 'colsample_bytree': 0.3566096997361373, 'min_child_weight': 2.255065136019848}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:35,420]\u001b[0m Trial 124 finished with value: -0.6347909517383201 and parameters: {'max_depth': 12, 'n_estimators': 562, 'learning_rate': 0.9518593596478541, 'subsample': 0.34591111699751176, 'colsample_bytree': 0.42705655156608124, 'min_child_weight': 1.5380706620729467}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:35,984]\u001b[0m Trial 125 finished with value: -0.6313139860870124 and parameters: {'max_depth': 11, 'n_estimators': 495, 'learning_rate': 0.9351434756801523, 'subsample': 0.16152595193271727, 'colsample_bytree': 0.49860471995229294, 'min_child_weight': 1.6486387354279473}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:36,373]\u001b[0m Trial 126 finished with value: -0.6347900359874045 and parameters: {'max_depth': 11, 'n_estimators': 443, 'learning_rate': 0.8820716817924789, 'subsample': 0.2644527732307499, 'colsample_bytree': 0.4423517978845867, 'min_child_weight': 2.3855631804918813}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:37,024]\u001b[0m Trial 127 finished with value: -0.6067679535376904 and parameters: {'max_depth': 12, 'n_estimators': 491, 'learning_rate': 0.9883167577327245, 'subsample': 0.1659838179282045, 'colsample_bytree': 0.27509221925397964, 'min_child_weight': 2.7709029960980294}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:37,343]\u001b[0m Trial 128 finished with value: -0.6389128791530106 and parameters: {'max_depth': 10, 'n_estimators': 495, 'learning_rate': 0.9700477678784797, 'subsample': 0.2599141435446689, 'colsample_bytree': 0.509416388780853, 'min_child_weight': 3.6630060610844803}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:38,052]\u001b[0m Trial 129 finished with value: -0.634986980672836 and parameters: {'max_depth': 10, 'n_estimators': 512, 'learning_rate': 0.9723268323849776, 'subsample': 0.255790877265938, 'colsample_bytree': 0.44140839641218077, 'min_child_weight': 2.578314887631604}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:38,293]\u001b[0m Trial 130 finished with value: -0.6056694500674763 and parameters: {'max_depth': 9, 'n_estimators': 487, 'learning_rate': 0.8387407276326361, 'subsample': 0.4757679669684275, 'colsample_bytree': 0.2840671482302452, 'min_child_weight': 3.1541757228288025}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:38,999]\u001b[0m Trial 131 finished with value: -0.6189749453762612 and parameters: {'max_depth': 7, 'n_estimators': 524, 'learning_rate': 0.6510344112193679, 'subsample': 0.47040519888703247, 'colsample_bytree': 0.3277759127501203, 'min_child_weight': 1.9839380383205694}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:39,251]\u001b[0m Trial 132 finished with value: -0.6201678053306342 and parameters: {'max_depth': 9, 'n_estimators': 435, 'learning_rate': 0.9388559334790909, 'subsample': 0.16815844641023825, 'colsample_bytree': 0.32121807229026583, 'min_child_weight': 3.736939987866536}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:40,260]\u001b[0m Trial 133 finished with value: -0.6037984765599896 and parameters: {'max_depth': 9, 'n_estimators': 472, 'learning_rate': 0.9615096433191436, 'subsample': 0.3137002076087728, 'colsample_bytree': 0.3022577331774031, 'min_child_weight': 2.490210211415752}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:40,409]\u001b[0m Trial 134 finished with value: -0.6354956803065356 and parameters: {'max_depth': 10, 'n_estimators': 416, 'learning_rate': 0.7630509229795073, 'subsample': 0.3379575784297022, 'colsample_bytree': 0.39284411391513907, 'min_child_weight': 1.4980151808113984}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:41,961]\u001b[0m Trial 136 finished with value: -0.6363293992192018 and parameters: {'max_depth': 8, 'n_estimators': 551, 'learning_rate': 0.847924074081319, 'subsample': 0.3637671593928177, 'colsample_bytree': 0.4245354331743175, 'min_child_weight': 1.6687902458357415}. Best is trial 98 with value: -0.5905607930081616.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:42,207]\u001b[0m Trial 135 finished with value: -0.5889490091414433 and parameters: {'max_depth': 9, 'n_estimators': 393, 'learning_rate': 0.7836959863238611, 'subsample': 0.223447751878518, 'colsample_bytree': 0.32143814302439705, 'min_child_weight': 1.0809143939825714}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:43,693]\u001b[0m Trial 137 finished with value: -0.608809379217274 and parameters: {'max_depth': 11, 'n_estimators': 460, 'learning_rate': 0.9011976945305693, 'subsample': 0.15598315725354614, 'colsample_bytree': 0.3285232789482946, 'min_child_weight': 3.073655082322731}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:44,133]\u001b[0m Trial 138 finished with value: -0.6563277323918771 and parameters: {'max_depth': 12, 'n_estimators': 451, 'learning_rate': 0.8089069416137693, 'subsample': 0.527309075516261, 'colsample_bytree': 0.15964406390738198, 'min_child_weight': 4.458444214128379}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:44,937]\u001b[0m Trial 139 finished with value: -0.6323701581678556 and parameters: {'max_depth': 11, 'n_estimators': 415, 'learning_rate': 0.8528474428916286, 'subsample': 0.32561641458303064, 'colsample_bytree': 0.4797062982459287, 'min_child_weight': 2.74663694254105}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:45,119]\u001b[0m Trial 140 finished with value: -0.634945713643082 and parameters: {'max_depth': 13, 'n_estimators': 487, 'learning_rate': 0.9470126811704693, 'subsample': 0.3929124151551371, 'colsample_bytree': 0.38726061034450554, 'min_child_weight': 1.3132454761964976}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:45,859]\u001b[0m Trial 141 finished with value: -0.6497097611818006 and parameters: {'max_depth': 10, 'n_estimators': 555, 'learning_rate': 0.7434981895303603, 'subsample': 0.32037422061939375, 'colsample_bytree': 0.22422438203155134, 'min_child_weight': 2.7015220694960913}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:46,145]\u001b[0m Trial 142 finished with value: -0.6163416996015679 and parameters: {'max_depth': 8, 'n_estimators': 422, 'learning_rate': 0.5880141964645363, 'subsample': 0.19646745532917304, 'colsample_bytree': 0.2992064011770232, 'min_child_weight': 1.2705236800827489}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:46,804]\u001b[0m Trial 143 finished with value: -0.5976497092089198 and parameters: {'max_depth': 11, 'n_estimators': 330, 'learning_rate': 0.9497973035033225, 'subsample': 0.2833945315766842, 'colsample_bytree': 0.335162082508707, 'min_child_weight': 1.458225311308483}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:47,046]\u001b[0m Trial 144 finished with value: -0.635012653830088 and parameters: {'max_depth': 9, 'n_estimators': 521, 'learning_rate': 0.9401897380006626, 'subsample': 0.28988174206446876, 'colsample_bytree': 0.3599995455130186, 'min_child_weight': 1.1472341340043102}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:47,642]\u001b[0m Trial 145 finished with value: -0.6431235299787931 and parameters: {'max_depth': 9, 'n_estimators': 456, 'learning_rate': 0.8707286327167244, 'subsample': 0.6214495663512439, 'colsample_bytree': 0.1653933791447702, 'min_child_weight': 1.268495312600615}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:47,882]\u001b[0m Trial 146 finished with value: -0.6333460353608379 and parameters: {'max_depth': 9, 'n_estimators': 438, 'learning_rate': 0.8716638783856194, 'subsample': 0.30490076013287915, 'colsample_bytree': 0.3819024537999044, 'min_child_weight': 1.7791597128866803}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:48,432]\u001b[0m Trial 147 finished with value: -0.5924134615384615 and parameters: {'max_depth': 10, 'n_estimators': 458, 'learning_rate': 0.8415220551656278, 'subsample': 0.3440895275749362, 'colsample_bytree': 0.2781654038467891, 'min_child_weight': 0.251480526258676}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:48,795]\u001b[0m Trial 148 finished with value: -0.6033243625891651 and parameters: {'max_depth': 13, 'n_estimators': 424, 'learning_rate': 0.7679814637889585, 'subsample': 0.21660291077808497, 'colsample_bytree': 0.2965821310837365, 'min_child_weight': 2.3326652726468025}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:49,338]\u001b[0m Trial 149 finished with value: -0.6359352126309363 and parameters: {'max_depth': 8, 'n_estimators': 415, 'learning_rate': 0.9131013522843486, 'subsample': 0.21426140039683944, 'colsample_bytree': 0.42381579016327875, 'min_child_weight': 1.1194185385864222}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:49,674]\u001b[0m Trial 150 finished with value: -0.6344834080393292 and parameters: {'max_depth': 8, 'n_estimators': 403, 'learning_rate': 0.8820929817569174, 'subsample': 0.14177163330261908, 'colsample_bytree': 0.4724929881793509, 'min_child_weight': 2.452058187243055}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:50,102]\u001b[0m Trial 151 finished with value: -0.5998262804447015 and parameters: {'max_depth': 10, 'n_estimators': 505, 'learning_rate': 0.9047691373509267, 'subsample': 0.17107708035335295, 'colsample_bytree': 0.2525815697241919, 'min_child_weight': 1.8636696227287368}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:50,322]\u001b[0m Trial 152 finished with value: -0.6650234982488272 and parameters: {'max_depth': 11, 'n_estimators': 362, 'learning_rate': 0.8038753700652248, 'subsample': 0.12191286575167946, 'colsample_bytree': 0.14658217775045454, 'min_child_weight': 2.924350843376646}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:50,839]\u001b[0m Trial 153 finished with value: -0.66659572207763 and parameters: {'max_depth': 9, 'n_estimators': 453, 'learning_rate': 0.8368523851521857, 'subsample': 0.42179039739733437, 'colsample_bytree': 0.13350313343265063, 'min_child_weight': 3.193629741122952}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:51,327]\u001b[0m Trial 154 finished with value: -0.6439206389370863 and parameters: {'max_depth': 13, 'n_estimators': 494, 'learning_rate': 0.8396826558170124, 'subsample': 0.10590361170931734, 'colsample_bytree': 0.2041857097210018, 'min_child_weight': 0.5802034361766952}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:51,748]\u001b[0m Trial 155 finished with value: -0.6392625634599319 and parameters: {'max_depth': 11, 'n_estimators': 373, 'learning_rate': 0.7472702249199253, 'subsample': 0.5165768127910856, 'colsample_bytree': 0.3718931050791442, 'min_child_weight': 0.1322685018322026}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:52,439]\u001b[0m Trial 156 finished with value: -0.647663752731187 and parameters: {'max_depth': 10, 'n_estimators': 420, 'learning_rate': 0.6658569639394304, 'subsample': 0.5653934003607579, 'colsample_bytree': 0.20942209219645933, 'min_child_weight': 0.4713003346386164}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:52,601]\u001b[0m Trial 157 finished with value: -0.603134199681897 and parameters: {'max_depth': 11, 'n_estimators': 456, 'learning_rate': 0.8456406956023114, 'subsample': 0.5109194070432386, 'colsample_bytree': 0.2832658442679228, 'min_child_weight': 0.3618612311622935}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:53,386]\u001b[0m Trial 158 finished with value: -0.6040387366653814 and parameters: {'max_depth': 12, 'n_estimators': 450, 'learning_rate': 0.7241223941640301, 'subsample': 0.38721122494653104, 'colsample_bytree': 0.2779707388368158, 'min_child_weight': 2.5208859550977882}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:53,491]\u001b[0m Trial 159 finished with value: -0.64171769166506 and parameters: {'max_depth': 12, 'n_estimators': 406, 'learning_rate': 0.6074170202352586, 'subsample': 0.3593811871294986, 'colsample_bytree': 0.38131748656504283, 'min_child_weight': 1.5766969935588349}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:54,297]\u001b[0m Trial 160 finished with value: -0.645999987950646 and parameters: {'max_depth': 10, 'n_estimators': 526, 'learning_rate': 0.7940259389086348, 'subsample': 0.22040862828590846, 'colsample_bytree': 0.17373994274309662, 'min_child_weight': 3.192752004296491}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:54,522]\u001b[0m Trial 161 finished with value: -0.643098595848596 and parameters: {'max_depth': 11, 'n_estimators': 403, 'learning_rate': 0.6196540153711164, 'subsample': 0.6448919211358837, 'colsample_bytree': 0.41774049082691334, 'min_child_weight': 1.0793588273613064}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:56,117]\u001b[0m Trial 162 finished with value: -0.6358833943833945 and parameters: {'max_depth': 12, 'n_estimators': 481, 'learning_rate': 0.8967079010633601, 'subsample': 0.4982905368226802, 'colsample_bytree': 0.40728953789036004, 'min_child_weight': 1.3915486914263762}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:56,489]\u001b[0m Trial 163 finished with value: -0.6334742706124284 and parameters: {'max_depth': 11, 'n_estimators': 438, 'learning_rate': 0.8643828651183014, 'subsample': 0.5532305129338307, 'colsample_bytree': 0.4587507466245143, 'min_child_weight': 2.02641259351525}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:58,102]\u001b[0m Trial 164 finished with value: -0.6098395226849174 and parameters: {'max_depth': 14, 'n_estimators': 466, 'learning_rate': 0.7612588269302885, 'subsample': 0.6835710655992023, 'colsample_bytree': 0.2828397096139255, 'min_child_weight': 1.483292318296443}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:58,475]\u001b[0m Trial 165 finished with value: -0.6370091936572199 and parameters: {'max_depth': 12, 'n_estimators': 407, 'learning_rate': 0.9821005303303507, 'subsample': 0.5572375083092281, 'colsample_bytree': 0.3926061112397615, 'min_child_weight': 2.4608519608769464}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:59,408]\u001b[0m Trial 166 finished with value: -0.6527425273921985 and parameters: {'max_depth': 9, 'n_estimators': 458, 'learning_rate': 0.7914720140969284, 'subsample': 0.5613435750642158, 'colsample_bytree': 0.2492638324212965, 'min_child_weight': 2.7151294087937066}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:06:59,529]\u001b[0m Trial 167 finished with value: -0.6057441600796865 and parameters: {'max_depth': 10, 'n_estimators': 464, 'learning_rate': 0.8940584054793432, 'subsample': 0.6854660739057059, 'colsample_bytree': 0.33654051777901245, 'min_child_weight': 2.745891350603512}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:00,364]\u001b[0m Trial 168 finished with value: -0.6203094535216246 and parameters: {'max_depth': 12, 'n_estimators': 415, 'learning_rate': 0.5852927120750677, 'subsample': 0.386425989717718, 'colsample_bytree': 0.3055400158207912, 'min_child_weight': 2.8340305716936642}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:00,479]\u001b[0m Trial 169 finished with value: -0.6294830405340275 and parameters: {'max_depth': 10, 'n_estimators': 567, 'learning_rate': 0.527130040638791, 'subsample': 0.5774587106331643, 'colsample_bytree': 0.2571678295093954, 'min_child_weight': 0.7182363810745958}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:01,232]\u001b[0m Trial 170 finished with value: -0.6055755534670008 and parameters: {'max_depth': 8, 'n_estimators': 529, 'learning_rate': 0.8405478507352071, 'subsample': 0.388404814047957, 'colsample_bytree': 0.3343284215843599, 'min_child_weight': 2.9081826908140123}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:01,351]\u001b[0m Trial 171 finished with value: -0.6439810784975258 and parameters: {'max_depth': 12, 'n_estimators': 502, 'learning_rate': 0.7424333469035801, 'subsample': 0.46831026150933397, 'colsample_bytree': 0.19670043471333404, 'min_child_weight': 1.304596664296087}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:02,540]\u001b[0m Trial 172 finished with value: -0.6422147214992611 and parameters: {'max_depth': 7, 'n_estimators': 424, 'learning_rate': 0.7456226374480983, 'subsample': 0.42697475222559134, 'colsample_bytree': 0.36970128707287897, 'min_child_weight': 2.8926759531165116}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:02,692]\u001b[0m Trial 173 finished with value: -0.6246783906882591 and parameters: {'max_depth': 10, 'n_estimators': 583, 'learning_rate': 0.6719463526325528, 'subsample': 0.6270544626124249, 'colsample_bytree': 0.33826206689010485, 'min_child_weight': 2.8568096287157885}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:03,344]\u001b[0m Trial 174 finished with value: -0.6414525074705996 and parameters: {'max_depth': 6, 'n_estimators': 448, 'learning_rate': 0.9909705017408029, 'subsample': 0.5601294049963274, 'colsample_bytree': 0.38849895115108724, 'min_child_weight': 3.7391924206024116}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:03,674]\u001b[0m Trial 175 finished with value: -0.6085653095077438 and parameters: {'max_depth': 9, 'n_estimators': 523, 'learning_rate': 0.7292360922562378, 'subsample': 0.5973134380407592, 'colsample_bytree': 0.34020153671327846, 'min_child_weight': 1.8127477422353568}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:04,177]\u001b[0m Trial 176 finished with value: -0.6095837630936316 and parameters: {'max_depth': 10, 'n_estimators': 506, 'learning_rate': 0.8806983157055273, 'subsample': 0.5491409047846957, 'colsample_bytree': 0.3167056599017317, 'min_child_weight': 3.364254850444683}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:04,478]\u001b[0m Trial 177 finished with value: -0.6231598607897949 and parameters: {'max_depth': 9, 'n_estimators': 400, 'learning_rate': 0.9079961380000281, 'subsample': 0.38062624289378366, 'colsample_bytree': 0.30759184125072536, 'min_child_weight': 5.0755602539872715}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:05,081]\u001b[0m Trial 178 finished with value: -0.6385015001445923 and parameters: {'max_depth': 10, 'n_estimators': 489, 'learning_rate': 0.694125391539411, 'subsample': 0.5708736319860273, 'colsample_bytree': 0.40937444283305535, 'min_child_weight': 3.667329170170725}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:05,369]\u001b[0m Trial 179 finished with value: -0.6454371003630871 and parameters: {'max_depth': 7, 'n_estimators': 539, 'learning_rate': 0.9392027772737731, 'subsample': 0.3382469015659071, 'colsample_bytree': 0.23583878750296453, 'min_child_weight': 3.5317230082405127}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:05,866]\u001b[0m Trial 180 finished with value: -0.6143243364822312 and parameters: {'max_depth': 10, 'n_estimators': 453, 'learning_rate': 0.765713160054936, 'subsample': 0.6272926177237541, 'colsample_bytree': 0.3486361002274305, 'min_child_weight': 4.203222293878787}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:06,131]\u001b[0m Trial 181 finished with value: -0.5948895676691729 and parameters: {'max_depth': 11, 'n_estimators': 571, 'learning_rate': 0.8639415025416343, 'subsample': 0.492413160446569, 'colsample_bytree': 0.26670966084352055, 'min_child_weight': 1.6163407047970402}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:06,891]\u001b[0m Trial 182 finished with value: -0.6255021869577791 and parameters: {'max_depth': 9, 'n_estimators': 571, 'learning_rate': 0.7660580388163147, 'subsample': 0.6699934373333583, 'colsample_bytree': 0.32261311139239957, 'min_child_weight': 4.031167853497769}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:06,956]\u001b[0m Trial 183 finished with value: -0.6324589719491035 and parameters: {'max_depth': 7, 'n_estimators': 396, 'learning_rate': 0.7933478633560727, 'subsample': 0.6049462087672677, 'colsample_bytree': 0.459662486532707, 'min_child_weight': 3.1431762313633738}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:07,898]\u001b[0m Trial 185 finished with value: -0.5970518242722189 and parameters: {'max_depth': 10, 'n_estimators': 487, 'learning_rate': 0.9432436527477819, 'subsample': 0.631857974084433, 'colsample_bytree': 0.2702360842545858, 'min_child_weight': 1.9006464821362385}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:07,919]\u001b[0m Trial 184 finished with value: -0.604857586675021 and parameters: {'max_depth': 10, 'n_estimators': 542, 'learning_rate': 0.7886266233064091, 'subsample': 0.9155352374740214, 'colsample_bytree': 0.33593419723302576, 'min_child_weight': 3.1841946253055884}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:08,806]\u001b[0m Trial 186 finished with value: -0.5993665252875778 and parameters: {'max_depth': 11, 'n_estimators': 382, 'learning_rate': 0.8090492006881987, 'subsample': 0.4725269350315813, 'colsample_bytree': 0.3260750438336209, 'min_child_weight': 2.0040299658705525}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:08,850]\u001b[0m Trial 187 finished with value: -0.5980982765407107 and parameters: {'max_depth': 10, 'n_estimators': 505, 'learning_rate': 0.8561441423493612, 'subsample': 0.39557607959088026, 'colsample_bytree': 0.2870736774430473, 'min_child_weight': 1.6373012056448855}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:10,403]\u001b[0m Trial 189 finished with value: -0.6007364665670586 and parameters: {'max_depth': 10, 'n_estimators': 473, 'learning_rate': 0.9111587628768376, 'subsample': 0.5913198085291523, 'colsample_bytree': 0.30307726791286277, 'min_child_weight': 0.5511003267758672}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:10,550]\u001b[0m Trial 188 finished with value: -0.6131752498232762 and parameters: {'max_depth': 10, 'n_estimators': 491, 'learning_rate': 0.9386377070435956, 'subsample': 0.4578067391055065, 'colsample_bytree': 0.307183230911386, 'min_child_weight': 4.139886166546381}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:12,198]\u001b[0m Trial 190 finished with value: -0.5970058579943448 and parameters: {'max_depth': 9, 'n_estimators': 496, 'learning_rate': 0.699198217197847, 'subsample': 0.5699784484369766, 'colsample_bytree': 0.31054918057431036, 'min_child_weight': 1.655167726078288}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:12,322]\u001b[0m Trial 191 finished with value: -0.6130441468253969 and parameters: {'max_depth': 9, 'n_estimators': 536, 'learning_rate': 0.7840904304195282, 'subsample': 0.6698118340595522, 'colsample_bytree': 0.2706711973893773, 'min_child_weight': 3.215854956850006}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:13,835]\u001b[0m Trial 192 finished with value: -0.595108839807853 and parameters: {'max_depth': 11, 'n_estimators': 401, 'learning_rate': 0.8707007364414688, 'subsample': 0.5304260566752723, 'colsample_bytree': 0.3091261708968134, 'min_child_weight': 0.8211510690865378}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:13,884]\u001b[0m Trial 193 finished with value: -0.6475259864404601 and parameters: {'max_depth': 10, 'n_estimators': 520, 'learning_rate': 0.7111713278819566, 'subsample': 0.6826804053529568, 'colsample_bytree': 0.21990025593296114, 'min_child_weight': 0.7614452029407569}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:14,836]\u001b[0m Trial 194 finished with value: -0.609332688692886 and parameters: {'max_depth': 11, 'n_estimators': 539, 'learning_rate': 0.7769993736622318, 'subsample': 0.5794513706745416, 'colsample_bytree': 0.33154684753060437, 'min_child_weight': 1.0162347676936359}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:14,855]\u001b[0m Trial 195 finished with value: -0.6307749200726176 and parameters: {'max_depth': 11, 'n_estimators': 443, 'learning_rate': 0.9025862429097098, 'subsample': 0.7111429787588932, 'colsample_bytree': 0.45034955596182147, 'min_child_weight': 2.777062444077929}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:15,627]\u001b[0m Trial 197 finished with value: -0.6048047482488272 and parameters: {'max_depth': 7, 'n_estimators': 585, 'learning_rate': 0.7856265018070095, 'subsample': 0.5655856795639451, 'colsample_bytree': 0.281917041853813, 'min_child_weight': 0.9208653120134769}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:15,818]\u001b[0m Trial 196 finished with value: -0.6060416365432812 and parameters: {'max_depth': 9, 'n_estimators': 383, 'learning_rate': 0.684447515681364, 'subsample': 0.4114699989253211, 'colsample_bytree': 0.3149925487383313, 'min_child_weight': 1.4799573663280579}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:16,544]\u001b[0m Trial 198 finished with value: -0.6416561415558127 and parameters: {'max_depth': 11, 'n_estimators': 410, 'learning_rate': 0.9611236391308084, 'subsample': 0.5314175358674734, 'colsample_bytree': 0.22564537601951437, 'min_child_weight': 1.014852665325627}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n",
            "\u001b[32m[I 2023-05-25 14:07:16,630]\u001b[0m Trial 199 finished with value: -0.6059813114517063 and parameters: {'max_depth': 11, 'n_estimators': 402, 'learning_rate': 0.7566778626620323, 'subsample': 0.7546112568594735, 'colsample_bytree': 0.34674216964769566, 'min_child_weight': 1.506828641746817}. Best is trial 135 with value: -0.5889490091414433.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'max_depth': 9, 'n_estimators': 393, 'learning_rate': 0.7836959863238611, 'subsample': 0.223447751878518, 'colsample_bytree': 0.32143814302439705, 'min_child_weight': 1.0809143939825714}\n",
            "Best ROC-AUC Score: -0.5889490091414433\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "from functools import partial\n",
        "\n",
        "X = train_df.drop(['TenYearCHD'], axis=1)\n",
        "y = train_df['TenYearCHD']\n",
        "X = X.reset_index(drop=True)\n",
        "\n",
        "def optimize(trial, X, y):\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 600),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),\n",
        "        'subsample': trial.suggest_float('subsample', 0.1, 1),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1),\n",
        "        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10)\n",
        "    }\n",
        "    model = lgb.LGBMClassifier(**params)\n",
        "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
        "    scores = []\n",
        "    for train_idx, val_idx in cv.split(X, y):\n",
        "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "        model.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_val, y_val)], verbose=False)\n",
        "        y_pred = model.predict_proba(X_val)[:, 1]\n",
        "        score = roc_auc_score(y_val, y_pred)\n",
        "        scores.append(score)\n",
        "    return -1.0 * np.mean(scores)\n",
        "\n",
        "# Create the objective function with partial arguments\n",
        "objective = partial(optimize, X=X, y=y)\n",
        "\n",
        "# Create the study object with the CMA-ES sampler\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.CmaEsSampler())\n",
        "\n",
        "# Optimize the objective function\n",
        "study.optimize(objective, n_trials=200, n_jobs=-1)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "best_params = study.best_params\n",
        "best_score = study.best_value\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best ROC-AUC Score:\", best_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI6pon_-OpAa",
        "outputId": "fbc090cf-cc62-46d2-b72c-999d1cc5d2b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.6265459363957597\n"
          ]
        }
      ],
      "source": [
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Assuming you have your training and evaluation data as X_train, y_train, X_eval, y_eval, and test data as X_test, y_test\n",
        "\n",
        "# Define the parameters\n",
        "params = {'max_depth': 9, 'n_estimators': 393, \n",
        "          'learning_rate': 0.7836959863238611, 'subsample': 0.223447751878518, \n",
        "          'colsample_bytree': 0.32143814302439705, 'min_child_weight': 1.0809143939825714}\n",
        "\n",
        "# Train the model\n",
        "model = lgb.LGBMClassifier(**params)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND8FDLhaPYhp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xn8KSxCPkYQ"
      },
      "source": [
        "### XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5SPrpAiPjbf",
        "outputId": "605e69b6-567e-47b5-845d-a8c2177071a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration No: 1 started. Evaluating function at random point.\n",
            "Iteration No: 1 ended. Evaluation done at random point.\n",
            "Time taken: 6.1596\n",
            "Function value obtained: -0.6906\n",
            "Current minimum: -0.6906\n",
            "Iteration No: 2 started. Evaluating function at random point.\n",
            "Iteration No: 2 ended. Evaluation done at random point.\n",
            "Time taken: 1.1131\n",
            "Function value obtained: -0.6755\n",
            "Current minimum: -0.6906\n",
            "Iteration No: 3 started. Evaluating function at random point.\n",
            "Iteration No: 3 ended. Evaluation done at random point.\n",
            "Time taken: 1.5695\n",
            "Function value obtained: -0.6952\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 4 started. Evaluating function at random point.\n",
            "Iteration No: 4 ended. Evaluation done at random point.\n",
            "Time taken: 0.7505\n",
            "Function value obtained: -0.6847\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 5 started. Evaluating function at random point.\n",
            "Iteration No: 5 ended. Evaluation done at random point.\n",
            "Time taken: 0.5306\n",
            "Function value obtained: -0.6526\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 6 started. Evaluating function at random point.\n",
            "Iteration No: 6 ended. Evaluation done at random point.\n",
            "Time taken: 0.6729\n",
            "Function value obtained: -0.6696\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 7 started. Evaluating function at random point.\n",
            "Iteration No: 7 ended. Evaluation done at random point.\n",
            "Time taken: 1.3705\n",
            "Function value obtained: -0.6786\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 8 started. Evaluating function at random point.\n",
            "Iteration No: 8 ended. Evaluation done at random point.\n",
            "Time taken: 0.4295\n",
            "Function value obtained: -0.6549\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 9 started. Evaluating function at random point.\n",
            "Iteration No: 9 ended. Evaluation done at random point.\n",
            "Time taken: 0.4858\n",
            "Function value obtained: -0.6483\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 10 started. Evaluating function at random point.\n",
            "Iteration No: 10 ended. Evaluation done at random point.\n",
            "Time taken: 0.6673\n",
            "Function value obtained: -0.6881\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 11 started. Evaluating function at random point.\n",
            "Iteration No: 11 ended. Evaluation done at random point.\n",
            "Time taken: 0.7287\n",
            "Function value obtained: -0.6917\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 12 started. Evaluating function at random point.\n",
            "Iteration No: 12 ended. Evaluation done at random point.\n",
            "Time taken: 1.8779\n",
            "Function value obtained: -0.6545\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 13 started. Evaluating function at random point.\n",
            "Iteration No: 13 ended. Evaluation done at random point.\n",
            "Time taken: 4.4487\n",
            "Function value obtained: -0.6921\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 14 started. Evaluating function at random point.\n",
            "Iteration No: 14 ended. Evaluation done at random point.\n",
            "Time taken: 0.6324\n",
            "Function value obtained: -0.6640\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 15 started. Evaluating function at random point.\n",
            "Iteration No: 15 ended. Evaluation done at random point.\n",
            "Time taken: 5.4483\n",
            "Function value obtained: -0.6926\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 16 started. Evaluating function at random point.\n",
            "Iteration No: 16 ended. Evaluation done at random point.\n",
            "Time taken: 3.2048\n",
            "Function value obtained: -0.6875\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 17 started. Evaluating function at random point.\n",
            "Iteration No: 17 ended. Evaluation done at random point.\n",
            "Time taken: 0.7307\n",
            "Function value obtained: -0.6878\n",
            "Current minimum: -0.6952\n",
            "Iteration No: 18 started. Evaluating function at random point.\n",
            "Iteration No: 18 ended. Evaluation done at random point.\n",
            "Time taken: 4.8728\n",
            "Function value obtained: -0.7073\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 19 started. Evaluating function at random point.\n",
            "Iteration No: 19 ended. Evaluation done at random point.\n",
            "Time taken: 0.9997\n",
            "Function value obtained: -0.6330\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 20 started. Evaluating function at random point.\n",
            "Iteration No: 20 ended. Evaluation done at random point.\n",
            "Time taken: 1.9065\n",
            "Function value obtained: -0.6987\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 21 started. Evaluating function at random point.\n",
            "Iteration No: 21 ended. Evaluation done at random point.\n",
            "Time taken: 1.2373\n",
            "Function value obtained: -0.6917\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 22 started. Evaluating function at random point.\n",
            "Iteration No: 22 ended. Evaluation done at random point.\n",
            "Time taken: 0.5632\n",
            "Function value obtained: -0.6094\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 23 started. Evaluating function at random point.\n",
            "Iteration No: 23 ended. Evaluation done at random point.\n",
            "Time taken: 0.6002\n",
            "Function value obtained: -0.6874\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 24 started. Evaluating function at random point.\n",
            "Iteration No: 24 ended. Evaluation done at random point.\n",
            "Time taken: 0.9892\n",
            "Function value obtained: -0.6945\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 25 started. Evaluating function at random point.\n",
            "Iteration No: 25 ended. Evaluation done at random point.\n",
            "Time taken: 0.7224\n",
            "Function value obtained: -0.6733\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 26 started. Evaluating function at random point.\n",
            "Iteration No: 26 ended. Evaluation done at random point.\n",
            "Time taken: 0.7097\n",
            "Function value obtained: -0.6919\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 27 started. Evaluating function at random point.\n",
            "Iteration No: 27 ended. Evaluation done at random point.\n",
            "Time taken: 6.3250\n",
            "Function value obtained: -0.7004\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 28 started. Evaluating function at random point.\n",
            "Iteration No: 28 ended. Evaluation done at random point.\n",
            "Time taken: 0.5313\n",
            "Function value obtained: -0.6949\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 29 started. Evaluating function at random point.\n",
            "Iteration No: 29 ended. Evaluation done at random point.\n",
            "Time taken: 0.7496\n",
            "Function value obtained: -0.6280\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 30 started. Evaluating function at random point.\n",
            "Iteration No: 30 ended. Evaluation done at random point.\n",
            "Time taken: 0.7169\n",
            "Function value obtained: -0.6762\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 31 started. Evaluating function at random point.\n",
            "Iteration No: 31 ended. Evaluation done at random point.\n",
            "Time taken: 2.2566\n",
            "Function value obtained: -0.6946\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 32 started. Evaluating function at random point.\n",
            "Iteration No: 32 ended. Evaluation done at random point.\n",
            "Time taken: 0.5037\n",
            "Function value obtained: -0.6692\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 33 started. Evaluating function at random point.\n",
            "Iteration No: 33 ended. Evaluation done at random point.\n",
            "Time taken: 1.4096\n",
            "Function value obtained: -0.6913\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 34 started. Evaluating function at random point.\n",
            "Iteration No: 34 ended. Evaluation done at random point.\n",
            "Time taken: 0.7899\n",
            "Function value obtained: -0.6620\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 35 started. Evaluating function at random point.\n",
            "Iteration No: 35 ended. Evaluation done at random point.\n",
            "Time taken: 7.0889\n",
            "Function value obtained: -0.7015\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 36 started. Evaluating function at random point.\n",
            "Iteration No: 36 ended. Evaluation done at random point.\n",
            "Time taken: 1.3855\n",
            "Function value obtained: -0.6887\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 37 started. Evaluating function at random point.\n",
            "Iteration No: 37 ended. Evaluation done at random point.\n",
            "Time taken: 1.8623\n",
            "Function value obtained: -0.6973\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 38 started. Evaluating function at random point.\n",
            "Iteration No: 38 ended. Evaluation done at random point.\n",
            "Time taken: 0.5422\n",
            "Function value obtained: -0.6647\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 39 started. Evaluating function at random point.\n",
            "Iteration No: 39 ended. Evaluation done at random point.\n",
            "Time taken: 0.7262\n",
            "Function value obtained: -0.6747\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 40 started. Evaluating function at random point.\n",
            "Iteration No: 40 ended. Evaluation done at random point.\n",
            "Time taken: 0.7592\n",
            "Function value obtained: -0.7066\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 41 started. Evaluating function at random point.\n",
            "Iteration No: 41 ended. Evaluation done at random point.\n",
            "Time taken: 0.8123\n",
            "Function value obtained: -0.6587\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 42 started. Evaluating function at random point.\n",
            "Iteration No: 42 ended. Evaluation done at random point.\n",
            "Time taken: 1.9671\n",
            "Function value obtained: -0.7067\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 43 started. Evaluating function at random point.\n",
            "Iteration No: 43 ended. Evaluation done at random point.\n",
            "Time taken: 0.7601\n",
            "Function value obtained: -0.6632\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 44 started. Evaluating function at random point.\n",
            "Iteration No: 44 ended. Evaluation done at random point.\n",
            "Time taken: 3.8716\n",
            "Function value obtained: -0.6481\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 45 started. Evaluating function at random point.\n",
            "Iteration No: 45 ended. Evaluation done at random point.\n",
            "Time taken: 0.5790\n",
            "Function value obtained: -0.6951\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 46 started. Evaluating function at random point.\n",
            "Iteration No: 46 ended. Evaluation done at random point.\n",
            "Time taken: 0.8911\n",
            "Function value obtained: -0.6684\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 47 started. Evaluating function at random point.\n",
            "Iteration No: 47 ended. Evaluation done at random point.\n",
            "Time taken: 0.5229\n",
            "Function value obtained: -0.6816\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 48 started. Evaluating function at random point.\n",
            "Iteration No: 48 ended. Evaluation done at random point.\n",
            "Time taken: 1.0795\n",
            "Function value obtained: -0.6880\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 49 started. Evaluating function at random point.\n",
            "Iteration No: 49 ended. Evaluation done at random point.\n",
            "Time taken: 1.4642\n",
            "Function value obtained: -0.6951\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 50 started. Evaluating function at random point.\n",
            "Iteration No: 50 ended. Evaluation done at random point.\n",
            "Time taken: 0.5884\n",
            "Function value obtained: -0.6881\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 51 started. Evaluating function at random point.\n",
            "Iteration No: 51 ended. Evaluation done at random point.\n",
            "Time taken: 1.1396\n",
            "Function value obtained: -0.7058\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 52 started. Evaluating function at random point.\n",
            "Iteration No: 52 ended. Evaluation done at random point.\n",
            "Time taken: 0.4194\n",
            "Function value obtained: -0.6764\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 53 started. Evaluating function at random point.\n",
            "Iteration No: 53 ended. Evaluation done at random point.\n",
            "Time taken: 0.4313\n",
            "Function value obtained: -0.6165\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 54 started. Evaluating function at random point.\n",
            "Iteration No: 54 ended. Evaluation done at random point.\n",
            "Time taken: 0.9684\n",
            "Function value obtained: -0.6932\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 55 started. Evaluating function at random point.\n",
            "Iteration No: 55 ended. Evaluation done at random point.\n",
            "Time taken: 0.9268\n",
            "Function value obtained: -0.6711\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 56 started. Evaluating function at random point.\n",
            "Iteration No: 56 ended. Evaluation done at random point.\n",
            "Time taken: 0.7540\n",
            "Function value obtained: -0.6784\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 57 started. Evaluating function at random point.\n",
            "Iteration No: 57 ended. Evaluation done at random point.\n",
            "Time taken: 4.2327\n",
            "Function value obtained: -0.6944\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 58 started. Evaluating function at random point.\n",
            "Iteration No: 58 ended. Evaluation done at random point.\n",
            "Time taken: 0.7867\n",
            "Function value obtained: -0.6824\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 59 started. Evaluating function at random point.\n",
            "Iteration No: 59 ended. Evaluation done at random point.\n",
            "Time taken: 1.0404\n",
            "Function value obtained: -0.6951\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 60 started. Evaluating function at random point.\n",
            "Iteration No: 60 ended. Evaluation done at random point.\n",
            "Time taken: 0.6615\n",
            "Function value obtained: -0.6614\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 61 started. Evaluating function at random point.\n",
            "Iteration No: 61 ended. Evaluation done at random point.\n",
            "Time taken: 1.5274\n",
            "Function value obtained: -0.7031\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 62 started. Evaluating function at random point.\n",
            "Iteration No: 62 ended. Evaluation done at random point.\n",
            "Time taken: 0.8843\n",
            "Function value obtained: -0.6405\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 63 started. Evaluating function at random point.\n",
            "Iteration No: 63 ended. Evaluation done at random point.\n",
            "Time taken: 1.0255\n",
            "Function value obtained: -0.6913\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 64 started. Evaluating function at random point.\n",
            "Iteration No: 64 ended. Evaluation done at random point.\n",
            "Time taken: 0.4855\n",
            "Function value obtained: -0.6497\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 65 started. Evaluating function at random point.\n",
            "Iteration No: 65 ended. Evaluation done at random point.\n",
            "Time taken: 0.9631\n",
            "Function value obtained: -0.6933\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 66 started. Evaluating function at random point.\n",
            "Iteration No: 66 ended. Evaluation done at random point.\n",
            "Time taken: 1.0543\n",
            "Function value obtained: -0.6967\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 67 started. Evaluating function at random point.\n",
            "Iteration No: 67 ended. Evaluation done at random point.\n",
            "Time taken: 0.9880\n",
            "Function value obtained: -0.6942\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 68 started. Evaluating function at random point.\n",
            "Iteration No: 68 ended. Evaluation done at random point.\n",
            "Time taken: 8.0488\n",
            "Function value obtained: -0.7000\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 69 started. Evaluating function at random point.\n",
            "Iteration No: 69 ended. Evaluation done at random point.\n",
            "Time taken: 0.5293\n",
            "Function value obtained: -0.6595\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 70 started. Evaluating function at random point.\n",
            "Iteration No: 70 ended. Evaluation done at random point.\n",
            "Time taken: 0.6365\n",
            "Function value obtained: -0.6898\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 71 started. Evaluating function at random point.\n",
            "Iteration No: 71 ended. Evaluation done at random point.\n",
            "Time taken: 1.1715\n",
            "Function value obtained: -0.6591\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 72 started. Evaluating function at random point.\n",
            "Iteration No: 72 ended. Evaluation done at random point.\n",
            "Time taken: 1.1186\n",
            "Function value obtained: -0.6966\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 73 started. Evaluating function at random point.\n",
            "Iteration No: 73 ended. Evaluation done at random point.\n",
            "Time taken: 0.6193\n",
            "Function value obtained: -0.6530\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 74 started. Evaluating function at random point.\n",
            "Iteration No: 74 ended. Evaluation done at random point.\n",
            "Time taken: 0.5214\n",
            "Function value obtained: -0.6927\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 75 started. Evaluating function at random point.\n",
            "Iteration No: 75 ended. Evaluation done at random point.\n",
            "Time taken: 2.3947\n",
            "Function value obtained: -0.7026\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 76 started. Evaluating function at random point.\n",
            "Iteration No: 76 ended. Evaluation done at random point.\n",
            "Time taken: 3.8586\n",
            "Function value obtained: -0.6879\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 77 started. Evaluating function at random point.\n",
            "Iteration No: 77 ended. Evaluation done at random point.\n",
            "Time taken: 0.5239\n",
            "Function value obtained: -0.6855\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 78 started. Evaluating function at random point.\n",
            "Iteration No: 78 ended. Evaluation done at random point.\n",
            "Time taken: 0.9504\n",
            "Function value obtained: -0.6758\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 79 started. Evaluating function at random point.\n",
            "Iteration No: 79 ended. Evaluation done at random point.\n",
            "Time taken: 1.0855\n",
            "Function value obtained: -0.6761\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 80 started. Evaluating function at random point.\n",
            "Iteration No: 80 ended. Evaluation done at random point.\n",
            "Time taken: 0.6581\n",
            "Function value obtained: -0.6911\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 81 started. Evaluating function at random point.\n",
            "Iteration No: 81 ended. Evaluation done at random point.\n",
            "Time taken: 0.5713\n",
            "Function value obtained: -0.6750\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 82 started. Evaluating function at random point.\n",
            "Iteration No: 82 ended. Evaluation done at random point.\n",
            "Time taken: 1.2788\n",
            "Function value obtained: -0.6867\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 83 started. Evaluating function at random point.\n",
            "Iteration No: 83 ended. Evaluation done at random point.\n",
            "Time taken: 0.6597\n",
            "Function value obtained: -0.6885\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 84 started. Evaluating function at random point.\n",
            "Iteration No: 84 ended. Evaluation done at random point.\n",
            "Time taken: 0.5538\n",
            "Function value obtained: -0.6920\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 85 started. Evaluating function at random point.\n",
            "Iteration No: 85 ended. Evaluation done at random point.\n",
            "Time taken: 0.5677\n",
            "Function value obtained: -0.6427\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 86 started. Evaluating function at random point.\n",
            "Iteration No: 86 ended. Evaluation done at random point.\n",
            "Time taken: 6.6085\n",
            "Function value obtained: -0.7028\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 87 started. Evaluating function at random point.\n",
            "Iteration No: 87 ended. Evaluation done at random point.\n",
            "Time taken: 0.6887\n",
            "Function value obtained: -0.6066\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 88 started. Evaluating function at random point.\n",
            "Iteration No: 88 ended. Evaluation done at random point.\n",
            "Time taken: 0.6494\n",
            "Function value obtained: -0.6635\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 89 started. Evaluating function at random point.\n",
            "Iteration No: 89 ended. Evaluation done at random point.\n",
            "Time taken: 0.6534\n",
            "Function value obtained: -0.6798\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 90 started. Evaluating function at random point.\n",
            "Iteration No: 90 ended. Evaluation done at random point.\n",
            "Time taken: 0.5837\n",
            "Function value obtained: -0.6923\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 91 started. Evaluating function at random point.\n",
            "Iteration No: 91 ended. Evaluation done at random point.\n",
            "Time taken: 0.8398\n",
            "Function value obtained: -0.7052\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 92 started. Evaluating function at random point.\n",
            "Iteration No: 92 ended. Evaluation done at random point.\n",
            "Time taken: 0.5047\n",
            "Function value obtained: -0.6716\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 93 started. Evaluating function at random point.\n",
            "Iteration No: 93 ended. Evaluation done at random point.\n",
            "Time taken: 0.9909\n",
            "Function value obtained: -0.6858\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 94 started. Evaluating function at random point.\n",
            "Iteration No: 94 ended. Evaluation done at random point.\n",
            "Time taken: 1.4513\n",
            "Function value obtained: -0.6989\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 95 started. Evaluating function at random point.\n",
            "Iteration No: 95 ended. Evaluation done at random point.\n",
            "Time taken: 0.6902\n",
            "Function value obtained: -0.6913\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 96 started. Evaluating function at random point.\n",
            "Iteration No: 96 ended. Evaluation done at random point.\n",
            "Time taken: 1.6156\n",
            "Function value obtained: -0.6904\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 97 started. Evaluating function at random point.\n",
            "Iteration No: 97 ended. Evaluation done at random point.\n",
            "Time taken: 0.8921\n",
            "Function value obtained: -0.6729\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 98 started. Evaluating function at random point.\n",
            "Iteration No: 98 ended. Evaluation done at random point.\n",
            "Time taken: 0.6293\n",
            "Function value obtained: -0.6390\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 99 started. Evaluating function at random point.\n",
            "Iteration No: 99 ended. Evaluation done at random point.\n",
            "Time taken: 7.0674\n",
            "Function value obtained: -0.7047\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 100 started. Evaluating function at random point.\n",
            "Iteration No: 100 ended. Evaluation done at random point.\n",
            "Time taken: 0.8152\n",
            "Function value obtained: -0.6537\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 101 started. Evaluating function at random point.\n",
            "Iteration No: 101 ended. Evaluation done at random point.\n",
            "Time taken: 0.5563\n",
            "Function value obtained: -0.6949\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 102 started. Evaluating function at random point.\n",
            "Iteration No: 102 ended. Evaluation done at random point.\n",
            "Time taken: 2.2116\n",
            "Function value obtained: -0.6991\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 103 started. Evaluating function at random point.\n",
            "Iteration No: 103 ended. Evaluation done at random point.\n",
            "Time taken: 0.6543\n",
            "Function value obtained: -0.6715\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 104 started. Evaluating function at random point.\n",
            "Iteration No: 104 ended. Evaluation done at random point.\n",
            "Time taken: 0.7309\n",
            "Function value obtained: -0.6969\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 105 started. Evaluating function at random point.\n",
            "Iteration No: 105 ended. Evaluation done at random point.\n",
            "Time taken: 0.4395\n",
            "Function value obtained: -0.6631\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 106 started. Evaluating function at random point.\n",
            "Iteration No: 106 ended. Evaluation done at random point.\n",
            "Time taken: 0.5746\n",
            "Function value obtained: -0.6807\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 107 started. Evaluating function at random point.\n",
            "Iteration No: 107 ended. Evaluation done at random point.\n",
            "Time taken: 0.4443\n",
            "Function value obtained: -0.6178\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 108 started. Evaluating function at random point.\n",
            "Iteration No: 108 ended. Evaluation done at random point.\n",
            "Time taken: 4.8335\n",
            "Function value obtained: -0.6979\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 109 started. Evaluating function at random point.\n",
            "Iteration No: 109 ended. Evaluation done at random point.\n",
            "Time taken: 0.4555\n",
            "Function value obtained: -0.6836\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 110 started. Evaluating function at random point.\n",
            "Iteration No: 110 ended. Evaluation done at random point.\n",
            "Time taken: 0.6218\n",
            "Function value obtained: -0.6925\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 111 started. Evaluating function at random point.\n",
            "Iteration No: 111 ended. Evaluation done at random point.\n",
            "Time taken: 0.8350\n",
            "Function value obtained: -0.6823\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 112 started. Evaluating function at random point.\n",
            "Iteration No: 112 ended. Evaluation done at random point.\n",
            "Time taken: 0.4032\n",
            "Function value obtained: -0.6114\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 113 started. Evaluating function at random point.\n",
            "Iteration No: 113 ended. Evaluation done at random point.\n",
            "Time taken: 0.7620\n",
            "Function value obtained: -0.6587\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 114 started. Evaluating function at random point.\n",
            "Iteration No: 114 ended. Evaluation done at random point.\n",
            "Time taken: 0.6573\n",
            "Function value obtained: -0.6776\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 115 started. Evaluating function at random point.\n",
            "Iteration No: 115 ended. Evaluation done at random point.\n",
            "Time taken: 0.5707\n",
            "Function value obtained: -0.6656\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 116 started. Evaluating function at random point.\n",
            "Iteration No: 116 ended. Evaluation done at random point.\n",
            "Time taken: 0.7692\n",
            "Function value obtained: -0.6895\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 117 started. Evaluating function at random point.\n",
            "Iteration No: 117 ended. Evaluation done at random point.\n",
            "Time taken: 0.9401\n",
            "Function value obtained: -0.6640\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 118 started. Evaluating function at random point.\n",
            "Iteration No: 118 ended. Evaluation done at random point.\n",
            "Time taken: 0.5865\n",
            "Function value obtained: -0.6860\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 119 started. Evaluating function at random point.\n",
            "Iteration No: 119 ended. Evaluation done at random point.\n",
            "Time taken: 1.0828\n",
            "Function value obtained: -0.6995\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 120 started. Evaluating function at random point.\n",
            "Iteration No: 120 ended. Evaluation done at random point.\n",
            "Time taken: 1.0638\n",
            "Function value obtained: -0.6717\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 121 started. Evaluating function at random point.\n",
            "Iteration No: 121 ended. Evaluation done at random point.\n",
            "Time taken: 4.1127\n",
            "Function value obtained: -0.6753\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 122 started. Evaluating function at random point.\n",
            "Iteration No: 122 ended. Evaluation done at random point.\n",
            "Time taken: 0.5124\n",
            "Function value obtained: -0.6837\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 123 started. Evaluating function at random point.\n",
            "Iteration No: 123 ended. Evaluation done at random point.\n",
            "Time taken: 0.6189\n",
            "Function value obtained: -0.6694\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 124 started. Evaluating function at random point.\n",
            "Iteration No: 124 ended. Evaluation done at random point.\n",
            "Time taken: 0.9192\n",
            "Function value obtained: -0.6945\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 125 started. Evaluating function at random point.\n",
            "Iteration No: 125 ended. Evaluation done at random point.\n",
            "Time taken: 2.1600\n",
            "Function value obtained: -0.7066\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 126 started. Evaluating function at random point.\n",
            "Iteration No: 126 ended. Evaluation done at random point.\n",
            "Time taken: 0.4249\n",
            "Function value obtained: -0.6529\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 127 started. Evaluating function at random point.\n",
            "Iteration No: 127 ended. Evaluation done at random point.\n",
            "Time taken: 1.0014\n",
            "Function value obtained: -0.6921\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 128 started. Evaluating function at random point.\n",
            "Iteration No: 128 ended. Evaluation done at random point.\n",
            "Time taken: 0.5492\n",
            "Function value obtained: -0.6596\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 129 started. Evaluating function at random point.\n",
            "Iteration No: 129 ended. Evaluation done at random point.\n",
            "Time taken: 0.5615\n",
            "Function value obtained: -0.6881\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 130 started. Evaluating function at random point.\n",
            "Iteration No: 130 ended. Evaluation done at random point.\n",
            "Time taken: 1.1596\n",
            "Function value obtained: -0.6911\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 131 started. Evaluating function at random point.\n",
            "Iteration No: 131 ended. Evaluation done at random point.\n",
            "Time taken: 0.6313\n",
            "Function value obtained: -0.7018\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 132 started. Evaluating function at random point.\n",
            "Iteration No: 132 ended. Evaluation done at random point.\n",
            "Time taken: 0.4305\n",
            "Function value obtained: -0.6776\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 133 started. Evaluating function at random point.\n",
            "Iteration No: 133 ended. Evaluation done at random point.\n",
            "Time taken: 0.7703\n",
            "Function value obtained: -0.6758\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 134 started. Evaluating function at random point.\n",
            "Iteration No: 134 ended. Evaluation done at random point.\n",
            "Time taken: 0.6364\n",
            "Function value obtained: -0.6511\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 135 started. Evaluating function at random point.\n",
            "Iteration No: 135 ended. Evaluation done at random point.\n",
            "Time taken: 3.8404\n",
            "Function value obtained: -0.6403\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 136 started. Evaluating function at random point.\n",
            "Iteration No: 136 ended. Evaluation done at random point.\n",
            "Time taken: 3.0061\n",
            "Function value obtained: -0.7057\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 137 started. Evaluating function at random point.\n",
            "Iteration No: 137 ended. Evaluation done at random point.\n",
            "Time taken: 0.7525\n",
            "Function value obtained: -0.6492\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 138 started. Evaluating function at random point.\n",
            "Iteration No: 138 ended. Evaluation done at random point.\n",
            "Time taken: 0.5455\n",
            "Function value obtained: -0.6680\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 139 started. Evaluating function at random point.\n",
            "Iteration No: 139 ended. Evaluation done at random point.\n",
            "Time taken: 1.6763\n",
            "Function value obtained: -0.6987\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 140 started. Evaluating function at random point.\n",
            "Iteration No: 140 ended. Evaluation done at random point.\n",
            "Time taken: 0.9872\n",
            "Function value obtained: -0.7026\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 141 started. Evaluating function at random point.\n",
            "Iteration No: 141 ended. Evaluation done at random point.\n",
            "Time taken: 0.6665\n",
            "Function value obtained: -0.6732\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 142 started. Evaluating function at random point.\n",
            "Iteration No: 142 ended. Evaluation done at random point.\n",
            "Time taken: 0.7696\n",
            "Function value obtained: -0.6706\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 143 started. Evaluating function at random point.\n",
            "Iteration No: 143 ended. Evaluation done at random point.\n",
            "Time taken: 0.6303\n",
            "Function value obtained: -0.6286\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 144 started. Evaluating function at random point.\n",
            "Iteration No: 144 ended. Evaluation done at random point.\n",
            "Time taken: 0.6090\n",
            "Function value obtained: -0.6772\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 145 started. Evaluating function at random point.\n",
            "Iteration No: 145 ended. Evaluation done at random point.\n",
            "Time taken: 8.0229\n",
            "Function value obtained: -0.7015\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 146 started. Evaluating function at random point.\n",
            "Iteration No: 146 ended. Evaluation done at random point.\n",
            "Time taken: 0.5580\n",
            "Function value obtained: -0.6902\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 147 started. Evaluating function at random point.\n",
            "Iteration No: 147 ended. Evaluation done at random point.\n",
            "Time taken: 0.5747\n",
            "Function value obtained: -0.6700\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 148 started. Evaluating function at random point.\n",
            "Iteration No: 148 ended. Evaluation done at random point.\n",
            "Time taken: 0.6600\n",
            "Function value obtained: -0.6777\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 149 started. Evaluating function at random point.\n",
            "Iteration No: 149 ended. Evaluation done at random point.\n",
            "Time taken: 0.5299\n",
            "Function value obtained: -0.6851\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 150 started. Evaluating function at random point.\n",
            "Iteration No: 150 ended. Evaluation done at random point.\n",
            "Time taken: 0.7046\n",
            "Function value obtained: -0.6717\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 151 started. Evaluating function at random point.\n",
            "Iteration No: 151 ended. Evaluation done at random point.\n",
            "Time taken: 2.1855\n",
            "Function value obtained: -0.6901\n",
            "Current minimum: -0.7073\n",
            "Iteration No: 152 started. Evaluating function at random point.\n",
            "Iteration No: 152 ended. Evaluation done at random point.\n",
            "Time taken: 5.1869\n",
            "Function value obtained: -0.7080\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 153 started. Evaluating function at random point.\n",
            "Iteration No: 153 ended. Evaluation done at random point.\n",
            "Time taken: 0.5755\n",
            "Function value obtained: -0.6950\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 154 started. Evaluating function at random point.\n",
            "Iteration No: 154 ended. Evaluation done at random point.\n",
            "Time taken: 0.5865\n",
            "Function value obtained: -0.6763\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 155 started. Evaluating function at random point.\n",
            "Iteration No: 155 ended. Evaluation done at random point.\n",
            "Time taken: 0.7822\n",
            "Function value obtained: -0.7026\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 156 started. Evaluating function at random point.\n",
            "Iteration No: 156 ended. Evaluation done at random point.\n",
            "Time taken: 0.7170\n",
            "Function value obtained: -0.6780\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 157 started. Evaluating function at random point.\n",
            "Iteration No: 157 ended. Evaluation done at random point.\n",
            "Time taken: 1.0007\n",
            "Function value obtained: -0.6904\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 158 started. Evaluating function at random point.\n",
            "Iteration No: 158 ended. Evaluation done at random point.\n",
            "Time taken: 1.0563\n",
            "Function value obtained: -0.6963\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 159 started. Evaluating function at random point.\n",
            "Iteration No: 159 ended. Evaluation done at random point.\n",
            "Time taken: 1.8994\n",
            "Function value obtained: -0.7029\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 160 started. Evaluating function at random point.\n",
            "Iteration No: 160 ended. Evaluation done at random point.\n",
            "Time taken: 1.4952\n",
            "Function value obtained: -0.6826\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 161 started. Evaluating function at random point.\n",
            "Iteration No: 161 ended. Evaluation done at random point.\n",
            "Time taken: 0.5258\n",
            "Function value obtained: -0.6668\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 162 started. Evaluating function at random point.\n",
            "Iteration No: 162 ended. Evaluation done at random point.\n",
            "Time taken: 4.7660\n",
            "Function value obtained: -0.6838\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 163 started. Evaluating function at random point.\n",
            "Iteration No: 163 ended. Evaluation done at random point.\n",
            "Time taken: 0.5030\n",
            "Function value obtained: -0.6904\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 164 started. Evaluating function at random point.\n",
            "Iteration No: 164 ended. Evaluation done at random point.\n",
            "Time taken: 2.2066\n",
            "Function value obtained: -0.6834\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 165 started. Evaluating function at random point.\n",
            "Iteration No: 165 ended. Evaluation done at random point.\n",
            "Time taken: 0.8687\n",
            "Function value obtained: -0.6576\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 166 started. Evaluating function at random point.\n",
            "Iteration No: 166 ended. Evaluation done at random point.\n",
            "Time taken: 0.8504\n",
            "Function value obtained: -0.6996\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 167 started. Evaluating function at random point.\n",
            "Iteration No: 167 ended. Evaluation done at random point.\n",
            "Time taken: 0.7399\n",
            "Function value obtained: -0.6548\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 168 started. Evaluating function at random point.\n",
            "Iteration No: 168 ended. Evaluation done at random point.\n",
            "Time taken: 0.5731\n",
            "Function value obtained: -0.6825\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 169 started. Evaluating function at random point.\n",
            "Iteration No: 169 ended. Evaluation done at random point.\n",
            "Time taken: 0.7094\n",
            "Function value obtained: -0.6930\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 170 started. Evaluating function at random point.\n",
            "Iteration No: 170 ended. Evaluation done at random point.\n",
            "Time taken: 0.5582\n",
            "Function value obtained: -0.6462\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 171 started. Evaluating function at random point.\n",
            "Iteration No: 171 ended. Evaluation done at random point.\n",
            "Time taken: 0.7311\n",
            "Function value obtained: -0.6726\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 172 started. Evaluating function at random point.\n",
            "Iteration No: 172 ended. Evaluation done at random point.\n",
            "Time taken: 0.6607\n",
            "Function value obtained: -0.6712\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 173 started. Evaluating function at random point.\n",
            "Iteration No: 173 ended. Evaluation done at random point.\n",
            "Time taken: 0.6089\n",
            "Function value obtained: -0.6962\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 174 started. Evaluating function at random point.\n",
            "Iteration No: 174 ended. Evaluation done at random point.\n",
            "Time taken: 0.4810\n",
            "Function value obtained: -0.6802\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 175 started. Evaluating function at random point.\n",
            "Iteration No: 175 ended. Evaluation done at random point.\n",
            "Time taken: 3.5355\n",
            "Function value obtained: -0.6670\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 176 started. Evaluating function at random point.\n",
            "Iteration No: 176 ended. Evaluation done at random point.\n",
            "Time taken: 1.1075\n",
            "Function value obtained: -0.6792\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 177 started. Evaluating function at random point.\n",
            "Iteration No: 177 ended. Evaluation done at random point.\n",
            "Time taken: 0.4644\n",
            "Function value obtained: -0.6868\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 178 started. Evaluating function at random point.\n",
            "Iteration No: 178 ended. Evaluation done at random point.\n",
            "Time taken: 0.6893\n",
            "Function value obtained: -0.6906\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 179 started. Evaluating function at random point.\n",
            "Iteration No: 179 ended. Evaluation done at random point.\n",
            "Time taken: 0.5716\n",
            "Function value obtained: -0.6210\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 180 started. Evaluating function at random point.\n",
            "Iteration No: 180 ended. Evaluation done at random point.\n",
            "Time taken: 0.6652\n",
            "Function value obtained: -0.6841\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 181 started. Evaluating function at random point.\n",
            "Iteration No: 181 ended. Evaluation done at random point.\n",
            "Time taken: 0.7460\n",
            "Function value obtained: -0.6808\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 182 started. Evaluating function at random point.\n",
            "Iteration No: 182 ended. Evaluation done at random point.\n",
            "Time taken: 0.8834\n",
            "Function value obtained: -0.7055\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 183 started. Evaluating function at random point.\n",
            "Iteration No: 183 ended. Evaluation done at random point.\n",
            "Time taken: 0.5614\n",
            "Function value obtained: -0.6828\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 184 started. Evaluating function at random point.\n",
            "Iteration No: 184 ended. Evaluation done at random point.\n",
            "Time taken: 0.4918\n",
            "Function value obtained: -0.6851\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 185 started. Evaluating function at random point.\n",
            "Iteration No: 185 ended. Evaluation done at random point.\n",
            "Time taken: 0.6704\n",
            "Function value obtained: -0.6776\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 186 started. Evaluating function at random point.\n",
            "Iteration No: 186 ended. Evaluation done at random point.\n",
            "Time taken: 0.9306\n",
            "Function value obtained: -0.6880\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 187 started. Evaluating function at random point.\n",
            "Iteration No: 187 ended. Evaluation done at random point.\n",
            "Time taken: 0.4539\n",
            "Function value obtained: -0.6902\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 188 started. Evaluating function at random point.\n",
            "Iteration No: 188 ended. Evaluation done at random point.\n",
            "Time taken: 0.7602\n",
            "Function value obtained: -0.6692\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 189 started. Evaluating function at random point.\n",
            "Iteration No: 189 ended. Evaluation done at random point.\n",
            "Time taken: 0.8595\n",
            "Function value obtained: -0.6872\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 190 started. Evaluating function at random point.\n",
            "Iteration No: 190 ended. Evaluation done at random point.\n",
            "Time taken: 0.5779\n",
            "Function value obtained: -0.6725\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 191 started. Evaluating function at random point.\n",
            "Iteration No: 191 ended. Evaluation done at random point.\n",
            "Time taken: 6.6408\n",
            "Function value obtained: -0.7023\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 192 started. Evaluating function at random point.\n",
            "Iteration No: 192 ended. Evaluation done at random point.\n",
            "Time taken: 1.2043\n",
            "Function value obtained: -0.6840\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 193 started. Evaluating function at random point.\n",
            "Iteration No: 193 ended. Evaluation done at random point.\n",
            "Time taken: 1.1793\n",
            "Function value obtained: -0.6604\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 194 started. Evaluating function at random point.\n",
            "Iteration No: 194 ended. Evaluation done at random point.\n",
            "Time taken: 0.6704\n",
            "Function value obtained: -0.6726\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 195 started. Evaluating function at random point.\n",
            "Iteration No: 195 ended. Evaluation done at random point.\n",
            "Time taken: 1.0246\n",
            "Function value obtained: -0.6803\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 196 started. Evaluating function at random point.\n",
            "Iteration No: 196 ended. Evaluation done at random point.\n",
            "Time taken: 7.5004\n",
            "Function value obtained: -0.6926\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 197 started. Evaluating function at random point.\n",
            "Iteration No: 197 ended. Evaluation done at random point.\n",
            "Time taken: 0.4826\n",
            "Function value obtained: -0.6599\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 198 started. Evaluating function at random point.\n",
            "Iteration No: 198 ended. Evaluation done at random point.\n",
            "Time taken: 0.5187\n",
            "Function value obtained: -0.6896\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 199 started. Evaluating function at random point.\n",
            "Iteration No: 199 ended. Evaluation done at random point.\n",
            "Time taken: 0.5351\n",
            "Function value obtained: -0.6803\n",
            "Current minimum: -0.7080\n",
            "Iteration No: 200 started. Evaluating function at random point.\n",
            "Iteration No: 200 ended. Evaluation done at random point.\n",
            "Time taken: 4.7419\n",
            "Function value obtained: -0.7015\n",
            "Current minimum: -0.7080\n",
            "{'max_depth': 4, 'n_estimators': 507, 'learning_rate': 0.08488585554676588, 'subsample': 0.7428342774870389, 'colsample_bytree': 0.8704321827011343, 'min_child_weight': 7.467421949775829}\n",
            "Best ROC-AUC Score: 0.708016983564681\n"
          ]
        }
      ],
      "source": [
        "X = train_df.drop(['TenYearCHD'], axis = 1)\n",
        "y = train_df['TenYearCHD']\n",
        "X = X.reset_index(drop=True)\n",
        "\n",
        "def optimize(params, X, y):\n",
        "  params = dict(zip(param_names, params))\n",
        "  model = xgb.XGBClassifier(**params)\n",
        "  cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
        "  scores = []\n",
        "  for train_idx, val_idx in cv.split(X, y):\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "    model.fit(X_train, y_train, early_stopping_rounds=10,\n",
        "              eval_set=[(X_val, y_val)], verbose=False)\n",
        "    y_pred = model.predict_proba(X_val)[:, 1]\n",
        "    score = roc_auc_score(y_val, y_pred)\n",
        "    scores.append(score)\n",
        "  return -1.0 * np.mean(scores)\n",
        "# Define the parameter space and names\n",
        "\n",
        "param_space = [\n",
        "    space.Integer(3, 15, name=\"max_depth\"),\n",
        "    space.Integer(100, 1000, name=\"n_estimators\"),\n",
        "    space.Real(0.01, 1, prior=\"uniform\", name=\"learning_rate\"),\n",
        "    space.Real(0.1, 1, prior=\"uniform\", name=\"subsample\"),\n",
        "    space.Real(0.1, 1, prior=\"uniform\", name=\"colsample_bytree\"),\n",
        "    space.Real(0.1, 10, prior=\"uniform\", name=\"min_child_weight\")\n",
        "    ]\n",
        "param_names = [\"max_depth\", \"n_estimators\", \"learning_rate\", \"subsample\", \"colsample_bytree\", \"min_child_weight\"]\n",
        "\n",
        "# Define the optimization function with partial arguments\n",
        "optimization_function = partial(optimize, X=X, y=y)\n",
        "\n",
        "# Run the optimization using gp_minimize\n",
        "result = gp_minimize(\n",
        "    optimization_function,\n",
        "    dimensions=param_space,\n",
        "    n_calls=200,\n",
        "    n_random_starts=200,\n",
        "    verbose=10,\n",
        "    acq_func='EI'\n",
        ")\n",
        "# Print the best hyperparameters found\n",
        "print(dict(zip(param_names, result.x)))\n",
        "print(f'Best ROC-AUC Score: {-1.0 * result.fun}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaS3yd7LP2SY",
        "outputId": "ee316e96-f623-44d8-e2f2-7febf02022fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.6808405816798042\n"
          ]
        }
      ],
      "source": [
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Assuming you have your training and evaluation data as X_train, y_train, X_eval, y_eval, and test data as X_test, y_test\n",
        "\n",
        "# Define the parameters\n",
        "params = {'max_depth': 4, 'n_estimators': 507, 'learning_rate': 0.08488585554676588, \n",
        "          'subsample': 0.7428342774870389, 'colsample_bytree': 0.8704321827011343, 'min_child_weight': 7.467421949775829}\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model = xgb.XGBClassifier(**params)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXAzHeJBRt0y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuncD_OaR5K9"
      },
      "source": [
        "### GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCs29aCHSLDk"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fREa8LMPR5K-",
        "outputId": "630649a8-b1e0-4265-9355-75955da56486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration No: 1 started. Evaluating function at random point.\n",
            "Iteration No: 1 ended. Evaluation done at random point.\n",
            "Time taken: 2.8573\n",
            "Function value obtained: -0.7024\n",
            "Current minimum: -0.7024\n",
            "Iteration No: 2 started. Evaluating function at random point.\n",
            "Iteration No: 2 ended. Evaluation done at random point.\n",
            "Time taken: 2.6492\n",
            "Function value obtained: -0.7047\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 3 started. Evaluating function at random point.\n",
            "Iteration No: 3 ended. Evaluation done at random point.\n",
            "Time taken: 3.2132\n",
            "Function value obtained: -0.7029\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 4 started. Evaluating function at random point.\n",
            "Iteration No: 4 ended. Evaluation done at random point.\n",
            "Time taken: 1.0672\n",
            "Function value obtained: -0.6888\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 5 started. Evaluating function at random point.\n",
            "Iteration No: 5 ended. Evaluation done at random point.\n",
            "Time taken: 1.5988\n",
            "Function value obtained: -0.7025\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 6 started. Evaluating function at random point.\n",
            "Iteration No: 6 ended. Evaluation done at random point.\n",
            "Time taken: 1.2851\n",
            "Function value obtained: -0.5868\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 7 started. Evaluating function at random point.\n",
            "Iteration No: 7 ended. Evaluation done at random point.\n",
            "Time taken: 1.1582\n",
            "Function value obtained: -0.6961\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 8 started. Evaluating function at random point.\n",
            "Iteration No: 8 ended. Evaluation done at random point.\n",
            "Time taken: 0.6750\n",
            "Function value obtained: -0.6838\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 9 started. Evaluating function at random point.\n",
            "Iteration No: 9 ended. Evaluation done at random point.\n",
            "Time taken: 4.0471\n",
            "Function value obtained: -0.7009\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 10 started. Evaluating function at random point.\n",
            "Iteration No: 10 ended. Evaluation done at random point.\n",
            "Time taken: 0.8358\n",
            "Function value obtained: -0.6883\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 11 started. Evaluating function at random point.\n",
            "Iteration No: 11 ended. Evaluation done at random point.\n",
            "Time taken: 1.0161\n",
            "Function value obtained: -0.5896\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 12 started. Evaluating function at random point.\n",
            "Iteration No: 12 ended. Evaluation done at random point.\n",
            "Time taken: 7.5480\n",
            "Function value obtained: -0.6985\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 13 started. Evaluating function at random point.\n",
            "Iteration No: 13 ended. Evaluation done at random point.\n",
            "Time taken: 2.9105\n",
            "Function value obtained: -0.6995\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 14 started. Evaluating function at random point.\n",
            "Iteration No: 14 ended. Evaluation done at random point.\n",
            "Time taken: 0.4760\n",
            "Function value obtained: -0.6169\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 15 started. Evaluating function at random point.\n",
            "Iteration No: 15 ended. Evaluation done at random point.\n",
            "Time taken: 0.4190\n",
            "Function value obtained: -0.6721\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 16 started. Evaluating function at random point.\n",
            "Iteration No: 16 ended. Evaluation done at random point.\n",
            "Time taken: 1.1777\n",
            "Function value obtained: -0.7003\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 17 started. Evaluating function at random point.\n",
            "Iteration No: 17 ended. Evaluation done at random point.\n",
            "Time taken: 0.4253\n",
            "Function value obtained: -0.6831\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 18 started. Evaluating function at random point.\n",
            "Iteration No: 18 ended. Evaluation done at random point.\n",
            "Time taken: 1.7328\n",
            "Function value obtained: -0.6966\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 19 started. Evaluating function at random point.\n",
            "Iteration No: 19 ended. Evaluation done at random point.\n",
            "Time taken: 1.0129\n",
            "Function value obtained: -0.6786\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 20 started. Evaluating function at random point.\n",
            "Iteration No: 20 ended. Evaluation done at random point.\n",
            "Time taken: 0.6373\n",
            "Function value obtained: -0.6342\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 21 started. Evaluating function at random point.\n",
            "Iteration No: 21 ended. Evaluation done at random point.\n",
            "Time taken: 2.6552\n",
            "Function value obtained: -0.6977\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 22 started. Evaluating function at random point.\n",
            "Iteration No: 22 ended. Evaluation done at random point.\n",
            "Time taken: 1.1949\n",
            "Function value obtained: -0.6664\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 23 started. Evaluating function at random point.\n",
            "Iteration No: 23 ended. Evaluation done at random point.\n",
            "Time taken: 2.5185\n",
            "Function value obtained: -0.6961\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 24 started. Evaluating function at random point.\n",
            "Iteration No: 24 ended. Evaluation done at random point.\n",
            "Time taken: 0.5961\n",
            "Function value obtained: -0.6423\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 25 started. Evaluating function at random point.\n",
            "Iteration No: 25 ended. Evaluation done at random point.\n",
            "Time taken: 0.9494\n",
            "Function value obtained: -0.6806\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 26 started. Evaluating function at random point.\n",
            "Iteration No: 26 ended. Evaluation done at random point.\n",
            "Time taken: 1.2032\n",
            "Function value obtained: -0.6798\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 27 started. Evaluating function at random point.\n",
            "Iteration No: 27 ended. Evaluation done at random point.\n",
            "Time taken: 0.3975\n",
            "Function value obtained: -0.6647\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 28 started. Evaluating function at random point.\n",
            "Iteration No: 28 ended. Evaluation done at random point.\n",
            "Time taken: 0.9685\n",
            "Function value obtained: -0.6915\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 29 started. Evaluating function at random point.\n",
            "Iteration No: 29 ended. Evaluation done at random point.\n",
            "Time taken: 2.0848\n",
            "Function value obtained: -0.6941\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 30 started. Evaluating function at random point.\n",
            "Iteration No: 30 ended. Evaluation done at random point.\n",
            "Time taken: 1.3914\n",
            "Function value obtained: -0.7042\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 31 started. Evaluating function at random point.\n",
            "Iteration No: 31 ended. Evaluation done at random point.\n",
            "Time taken: 1.1585\n",
            "Function value obtained: -0.6495\n",
            "Current minimum: -0.7047\n",
            "Iteration No: 32 started. Evaluating function at random point.\n",
            "Iteration No: 32 ended. Evaluation done at random point.\n",
            "Time taken: 1.5953\n",
            "Function value obtained: -0.7085\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 33 started. Evaluating function at random point.\n",
            "Iteration No: 33 ended. Evaluation done at random point.\n",
            "Time taken: 1.0490\n",
            "Function value obtained: -0.6697\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 34 started. Evaluating function at random point.\n",
            "Iteration No: 34 ended. Evaluation done at random point.\n",
            "Time taken: 4.8009\n",
            "Function value obtained: -0.7014\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 35 started. Evaluating function at random point.\n",
            "Iteration No: 35 ended. Evaluation done at random point.\n",
            "Time taken: 0.9375\n",
            "Function value obtained: -0.6058\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 36 started. Evaluating function at random point.\n",
            "Iteration No: 36 ended. Evaluation done at random point.\n",
            "Time taken: 1.3010\n",
            "Function value obtained: -0.6495\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 37 started. Evaluating function at random point.\n",
            "Iteration No: 37 ended. Evaluation done at random point.\n",
            "Time taken: 1.9630\n",
            "Function value obtained: -0.6826\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 38 started. Evaluating function at random point.\n",
            "Iteration No: 38 ended. Evaluation done at random point.\n",
            "Time taken: 0.8858\n",
            "Function value obtained: -0.6018\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 39 started. Evaluating function at random point.\n",
            "Iteration No: 39 ended. Evaluation done at random point.\n",
            "Time taken: 1.0558\n",
            "Function value obtained: -0.6972\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 40 started. Evaluating function at random point.\n",
            "Iteration No: 40 ended. Evaluation done at random point.\n",
            "Time taken: 3.0428\n",
            "Function value obtained: -0.6996\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 41 started. Evaluating function at random point.\n",
            "Iteration No: 41 ended. Evaluation done at random point.\n",
            "Time taken: 2.1880\n",
            "Function value obtained: -0.6904\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 42 started. Evaluating function at random point.\n",
            "Iteration No: 42 ended. Evaluation done at random point.\n",
            "Time taken: 1.5866\n",
            "Function value obtained: -0.6955\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 43 started. Evaluating function at random point.\n",
            "Iteration No: 43 ended. Evaluation done at random point.\n",
            "Time taken: 2.0303\n",
            "Function value obtained: -0.6959\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 44 started. Evaluating function at random point.\n",
            "Iteration No: 44 ended. Evaluation done at random point.\n",
            "Time taken: 1.2226\n",
            "Function value obtained: -0.7078\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 45 started. Evaluating function at random point.\n",
            "Iteration No: 45 ended. Evaluation done at random point.\n",
            "Time taken: 0.8975\n",
            "Function value obtained: -0.6602\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 46 started. Evaluating function at random point.\n",
            "Iteration No: 46 ended. Evaluation done at random point.\n",
            "Time taken: 1.1197\n",
            "Function value obtained: -0.6602\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 47 started. Evaluating function at random point.\n",
            "Iteration No: 47 ended. Evaluation done at random point.\n",
            "Time taken: 0.6594\n",
            "Function value obtained: -0.6513\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 48 started. Evaluating function at random point.\n",
            "Iteration No: 48 ended. Evaluation done at random point.\n",
            "Time taken: 0.4671\n",
            "Function value obtained: -0.6344\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 49 started. Evaluating function at random point.\n",
            "Iteration No: 49 ended. Evaluation done at random point.\n",
            "Time taken: 3.3093\n",
            "Function value obtained: -0.6989\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 50 started. Evaluating function at random point.\n",
            "Iteration No: 50 ended. Evaluation done at random point.\n",
            "Time taken: 2.8989\n",
            "Function value obtained: -0.6965\n",
            "Current minimum: -0.7085\n",
            "{'learning_rate': 0.026788911880272295, 'max_depth': 3, 'subsample': 0.21130099384109907, 'max_features': 0.6159058226808146, 'n_estimators': 56, 'min_samples_split': 11, 'min_samples_leaf': 8}\n",
            "Best ROC-AUC Score: 0.7084802109440267\n"
          ]
        }
      ],
      "source": [
        "X = train_df.drop(['TenYearCHD'], axis = 1)\n",
        "y = train_df['TenYearCHD']\n",
        "X = X.reset_index(drop=True)\n",
        "def optimize(params, X, y):\n",
        "  params = dict(zip(param_names, params))\n",
        "  model = GradientBoostingClassifier(**params, n_iter_no_change=10)\n",
        "  cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
        "  scores = []\n",
        "  for train_idx, val_idx in cv.split(X, y):\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict_proba(X_val)[:, 1]\n",
        "    score = roc_auc_score(y_val, y_pred)\n",
        "    scores.append(score)\n",
        "  return -1.0 * np.mean(scores)\n",
        "# Define the parameter space and names\n",
        "\n",
        "param_space = [\n",
        "    Real(0.01, 1, prior='log-uniform', name='learning_rate'),\n",
        "    Integer(2, 10, name='max_depth'),\n",
        "    Real(0.1, 1, prior='uniform', name='subsample'),\n",
        "    Real(0.1, 1, prior='uniform', name='max_features'),\n",
        "    Integer(2, 100, name='n_estimators'),\n",
        "    Integer(2, 50, name='min_samples_split'),\n",
        "    Integer(1, 20, name='min_samples_leaf')\n",
        "]\n",
        "\n",
        "param_names = [\"learning_rate\", \"max_depth\", \"subsample\", \"max_features\", \"n_estimators\", \"min_samples_split\", \"min_samples_leaf\"]\n",
        "\n",
        "# Define the optimization function with partial arguments\n",
        "optimization_function = partial(optimize, X=X, y=y)\n",
        "\n",
        "# Run the optimization using gp_minimize\n",
        "result = gp_minimize(\n",
        "    optimization_function,\n",
        "    dimensions=param_space,\n",
        "    n_calls=50,\n",
        "    n_random_starts=50,\n",
        "    verbose=10,\n",
        "    acq_func='EI'\n",
        ")\n",
        "# Print the best hyperparameters found\n",
        "print(dict(zip(param_names, result.x)))\n",
        "print(f'Best ROC-AUC Score: {-1.0 * result.fun}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_sfE3RtR5LA",
        "outputId": "38127ed5-6d21-479a-cfd7-e53da9b08213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.6999439385702636\n"
          ]
        }
      ],
      "source": [
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Assuming you have your training and evaluation data as X_train, y_train, X_eval, y_eval, and test data as X_test, y_test\n",
        "\n",
        "# Define the parameters\n",
        "params = {'learning_rate': 0.026788911880272295, 'max_depth': 3, \n",
        "          'subsample': 0.21130099384109907, 'max_features': 0.6159058226808146, 'n_estimators': 56, 'min_samples_split': 11, 'min_samples_leaf': 8}\n",
        "\n",
        "# Train the model\n",
        "model = GradientBoostingClassifier(**params)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elx9rPv5TViD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk0dL6H5R5LB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmKckdLFTY1l"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KcK-Z_5TY1m"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0biOAP7KTY1n",
        "outputId": "943132e4-e30e-4244-cabc-42030ca0a61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration No: 1 started. Evaluating function at random point.\n",
            "Iteration No: 1 ended. Evaluation done at random point.\n",
            "Time taken: 0.2516\n",
            "Function value obtained: -0.7211\n",
            "Current minimum: -0.7211\n",
            "Iteration No: 2 started. Evaluating function at random point.\n",
            "Iteration No: 2 ended. Evaluation done at random point.\n",
            "Time taken: 0.2546\n",
            "Function value obtained: -0.7208\n",
            "Current minimum: -0.7211\n",
            "Iteration No: 3 started. Evaluating function at random point.\n",
            "Iteration No: 3 ended. Evaluation done at random point.\n",
            "Time taken: 0.2436\n",
            "Function value obtained: -0.7222\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 4 started. Evaluating function at random point.\n",
            "Iteration No: 4 ended. Evaluation done at random point.\n",
            "Time taken: 0.2586\n",
            "Function value obtained: -0.7208\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 5 started. Evaluating function at random point.\n",
            "Iteration No: 5 ended. Evaluation done at random point.\n",
            "Time taken: 0.1636\n",
            "Function value obtained: -0.7210\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 6 started. Evaluating function at random point.\n",
            "Iteration No: 6 ended. Evaluation done at random point.\n",
            "Time taken: 0.1632\n",
            "Function value obtained: -0.6986\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 7 started. Evaluating function at random point.\n",
            "Iteration No: 7 ended. Evaluation done at random point.\n",
            "Time taken: 0.1726\n",
            "Function value obtained: -0.7210\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 8 started. Evaluating function at random point.\n",
            "Iteration No: 8 ended. Evaluation done at random point.\n",
            "Time taken: 0.2625\n",
            "Function value obtained: -0.7211\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 9 started. Evaluating function at random point.\n",
            "Iteration No: 9 ended. Evaluation done at random point.\n",
            "Time taken: 0.1813\n",
            "Function value obtained: -0.7216\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 10 started. Evaluating function at random point.\n",
            "Iteration No: 10 ended. Evaluation done at random point.\n",
            "Time taken: 0.2502\n",
            "Function value obtained: -0.6890\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 11 started. Evaluating function at random point.\n",
            "Iteration No: 11 ended. Evaluation done at random point.\n",
            "Time taken: 0.2520\n",
            "Function value obtained: -0.7011\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 12 started. Evaluating function at random point.\n",
            "Iteration No: 12 ended. Evaluation done at random point.\n",
            "Time taken: 0.1630\n",
            "Function value obtained: -0.7212\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 13 started. Evaluating function at random point.\n",
            "Iteration No: 13 ended. Evaluation done at random point.\n",
            "Time taken: 0.1218\n",
            "Function value obtained: -0.5000\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 14 started. Evaluating function at random point.\n",
            "Iteration No: 14 ended. Evaluation done at random point.\n",
            "Time taken: 0.2814\n",
            "Function value obtained: -0.7210\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 15 started. Evaluating function at random point.\n",
            "Iteration No: 15 ended. Evaluation done at random point.\n",
            "Time taken: 0.2600\n",
            "Function value obtained: -0.7011\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 16 started. Evaluating function at random point.\n",
            "Iteration No: 16 ended. Evaluation done at random point.\n",
            "Time taken: 0.3034\n",
            "Function value obtained: -0.7210\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 17 started. Evaluating function at random point.\n",
            "Iteration No: 17 ended. Evaluation done at random point.\n",
            "Time taken: 0.3083\n",
            "Function value obtained: -0.7210\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 18 started. Evaluating function at random point.\n",
            "Iteration No: 18 ended. Evaluation done at random point.\n",
            "Time taken: 0.2901\n",
            "Function value obtained: -0.7211\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 19 started. Evaluating function at random point.\n",
            "Iteration No: 19 ended. Evaluation done at random point.\n",
            "Time taken: 0.2353\n",
            "Function value obtained: -0.7222\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 20 started. Evaluating function at random point.\n",
            "Iteration No: 20 ended. Evaluation done at random point.\n",
            "Time taken: 0.2281\n",
            "Function value obtained: -0.7222\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 21 started. Evaluating function at random point.\n",
            "Iteration No: 21 ended. Evaluation done at random point.\n",
            "Time taken: 0.1217\n",
            "Function value obtained: -0.5000\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 22 started. Evaluating function at random point.\n",
            "Iteration No: 22 ended. Evaluation done at random point.\n",
            "Time taken: 0.1401\n",
            "Function value obtained: -0.5000\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 23 started. Evaluating function at random point.\n",
            "Iteration No: 23 ended. Evaluation done at random point.\n",
            "Time taken: 0.2772\n",
            "Function value obtained: -0.7210\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 24 started. Evaluating function at random point.\n",
            "Iteration No: 24 ended. Evaluation done at random point.\n",
            "Time taken: 0.2388\n",
            "Function value obtained: -0.7221\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 25 started. Evaluating function at random point.\n",
            "Iteration No: 25 ended. Evaluation done at random point.\n",
            "Time taken: 0.1591\n",
            "Function value obtained: -0.7210\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 26 started. Evaluating function at random point.\n",
            "Iteration No: 26 ended. Evaluation done at random point.\n",
            "Time taken: 0.2458\n",
            "Function value obtained: -0.7222\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 27 started. Evaluating function at random point.\n",
            "Iteration No: 27 ended. Evaluation done at random point.\n",
            "Time taken: 0.2803\n",
            "Function value obtained: -0.7210\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 28 started. Evaluating function at random point.\n",
            "Iteration No: 28 ended. Evaluation done at random point.\n",
            "Time taken: 0.2437\n",
            "Function value obtained: -0.7180\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 29 started. Evaluating function at random point.\n",
            "Iteration No: 29 ended. Evaluation done at random point.\n",
            "Time taken: 0.2356\n",
            "Function value obtained: -0.6986\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 30 started. Evaluating function at random point.\n",
            "Iteration No: 30 ended. Evaluation done at random point.\n",
            "Time taken: 0.3761\n",
            "Function value obtained: -0.7180\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 31 started. Evaluating function at random point.\n",
            "Iteration No: 31 ended. Evaluation done at random point.\n",
            "Time taken: 0.4457\n",
            "Function value obtained: -0.7211\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 32 started. Evaluating function at random point.\n",
            "Iteration No: 32 ended. Evaluation done at random point.\n",
            "Time taken: 0.4518\n",
            "Function value obtained: -0.7210\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 33 started. Evaluating function at random point.\n",
            "Iteration No: 33 ended. Evaluation done at random point.\n",
            "Time taken: 0.7347\n",
            "Function value obtained: -0.7079\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 34 started. Evaluating function at random point.\n",
            "Iteration No: 34 ended. Evaluation done at random point.\n",
            "Time taken: 0.8731\n",
            "Function value obtained: -0.7210\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 35 started. Evaluating function at random point.\n",
            "Iteration No: 35 ended. Evaluation done at random point.\n",
            "Time taken: 0.8467\n",
            "Function value obtained: -0.7216\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 36 started. Evaluating function at random point.\n",
            "Iteration No: 36 ended. Evaluation done at random point.\n",
            "Time taken: 0.4985\n",
            "Function value obtained: -0.5000\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 37 started. Evaluating function at random point.\n",
            "Iteration No: 37 ended. Evaluation done at random point.\n",
            "Time taken: 0.7914\n",
            "Function value obtained: -0.7208\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 38 started. Evaluating function at random point.\n",
            "Iteration No: 38 ended. Evaluation done at random point.\n",
            "Time taken: 0.7578\n",
            "Function value obtained: -0.7211\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 39 started. Evaluating function at random point.\n",
            "Iteration No: 39 ended. Evaluation done at random point.\n",
            "Time taken: 0.5402\n",
            "Function value obtained: -0.7180\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 40 started. Evaluating function at random point.\n",
            "Iteration No: 40 ended. Evaluation done at random point.\n",
            "Time taken: 0.4842\n",
            "Function value obtained: -0.7180\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 41 started. Evaluating function at random point.\n",
            "Iteration No: 41 ended. Evaluation done at random point.\n",
            "Time taken: 0.9272\n",
            "Function value obtained: -0.7211\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 42 started. Evaluating function at random point.\n",
            "Iteration No: 42 ended. Evaluation done at random point.\n",
            "Time taken: 0.1518\n",
            "Function value obtained: -0.7216\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 43 started. Evaluating function at random point.\n",
            "Iteration No: 43 ended. Evaluation done at random point.\n",
            "Time taken: 0.1606\n",
            "Function value obtained: -0.6986\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 44 started. Evaluating function at random point.\n",
            "Iteration No: 44 ended. Evaluation done at random point.\n",
            "Time taken: 0.2776\n",
            "Function value obtained: -0.7210\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 45 started. Evaluating function at random point.\n",
            "Iteration No: 45 ended. Evaluation done at random point.\n",
            "Time taken: 0.1563\n",
            "Function value obtained: -0.7079\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 46 started. Evaluating function at random point.\n",
            "Iteration No: 46 ended. Evaluation done at random point.\n",
            "Time taken: 0.2518\n",
            "Function value obtained: -0.7209\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 47 started. Evaluating function at random point.\n",
            "Iteration No: 47 ended. Evaluation done at random point.\n",
            "Time taken: 0.1534\n",
            "Function value obtained: -0.5000\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 48 started. Evaluating function at random point.\n",
            "Iteration No: 48 ended. Evaluation done at random point.\n",
            "Time taken: 0.1420\n",
            "Function value obtained: -0.7079\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 49 started. Evaluating function at random point.\n",
            "Iteration No: 49 ended. Evaluation done at random point.\n",
            "Time taken: 0.1695\n",
            "Function value obtained: -0.7212\n",
            "Current minimum: -0.7222\n",
            "Iteration No: 50 started. Evaluating function at random point.\n",
            "Iteration No: 50 ended. Evaluation done at random point.\n",
            "Time taken: 0.7777\n",
            "Function value obtained: -0.7212\n",
            "Current minimum: -0.7222\n",
            "{'C': 1.0, 'penalty': 'l2', 'solver': 'saga'}\n",
            "Best ROC-AUC Score: 0.7221579228520018\n"
          ]
        }
      ],
      "source": [
        "X = train_df.drop(['TenYearCHD'], axis = 1)\n",
        "y = train_df['TenYearCHD']\n",
        "X = X.reset_index(drop=True)\n",
        "def optimize(params, X, y):\n",
        "  params = dict(zip(param_names, params))\n",
        "  model = LogisticRegression(**params)\n",
        "  cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
        "  scores = []\n",
        "  for train_idx, val_idx in cv.split(X, y):\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict_proba(X_val)[:, 1]\n",
        "    score = roc_auc_score(y_val, y_pred)\n",
        "    scores.append(score)\n",
        "  return -1.0 * np.mean(scores)\n",
        "# Define the parameter space and names\n",
        "\n",
        "param_space = [\n",
        "    Categorical(categories=[0.01, 0.1, 1, 10, 100], name='C'),\n",
        "    Categorical(categories=['l1', 'l2'], name='penalty'),\n",
        "    Categorical(categories=['liblinear', 'saga'], name='solver')\n",
        "    ]\n",
        "\n",
        "param_names = [\"C\", \"penalty\", \"solver\"]\n",
        "\n",
        "# Define the optimization function with partial arguments\n",
        "optimization_function = partial(optimize, X=X, y=y)\n",
        "\n",
        "# Run the optimization using gp_minimize\n",
        "result = gp_minimize(\n",
        "    optimization_function,\n",
        "    dimensions=param_space,\n",
        "    n_calls=50,\n",
        "    n_random_starts=50,\n",
        "    verbose=10,\n",
        "    acq_func='EI'\n",
        ")\n",
        "# Print the best hyperparameters found\n",
        "print(dict(zip(param_names, result.x)))\n",
        "print(f'Best ROC-AUC Score: {-1.0 * result.fun}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzjeqc-6TY1n",
        "outputId": "04597e98-7fc0-445d-8feb-ad30c60f0020"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.7011076379450938\n"
          ]
        }
      ],
      "source": [
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Assuming you have your training and evaluation data as X_train, y_train, X_eval, y_eval, and test data as X_test, y_test\n",
        "\n",
        "# Define the parameters\n",
        "params = {'C': 1.0, 'penalty': 'l2', 'solver': 'saga'}\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(**params)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxvWqMrBTY1n"
      },
      "source": [
        "### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypAfGrmRTY1n",
        "outputId": "2fcfcf3b-bfd9-411c-bc1c-1b67c32f599f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration No: 1 started. Evaluating function at random point.\n",
            "Iteration No: 1 ended. Evaluation done at random point.\n",
            "Time taken: 1.1404\n",
            "Function value obtained: -0.6630\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 2 started. Evaluating function at random point.\n",
            "Iteration No: 2 ended. Evaluation done at random point.\n",
            "Time taken: 1.1256\n",
            "Function value obtained: -0.6506\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 3 started. Evaluating function at random point.\n",
            "Iteration No: 3 ended. Evaluation done at random point.\n",
            "Time taken: 0.7072\n",
            "Function value obtained: -0.5991\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 4 started. Evaluating function at random point.\n",
            "Iteration No: 4 ended. Evaluation done at random point.\n",
            "Time taken: 0.4472\n",
            "Function value obtained: -0.5959\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 5 started. Evaluating function at random point.\n",
            "Iteration No: 5 ended. Evaluation done at random point.\n",
            "Time taken: 0.5055\n",
            "Function value obtained: -0.6145\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 6 started. Evaluating function at random point.\n",
            "Iteration No: 6 ended. Evaluation done at random point.\n",
            "Time taken: 0.4937\n",
            "Function value obtained: -0.6241\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 7 started. Evaluating function at random point.\n",
            "Iteration No: 7 ended. Evaluation done at random point.\n",
            "Time taken: 0.4544\n",
            "Function value obtained: -0.6185\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 8 started. Evaluating function at random point.\n",
            "Iteration No: 8 ended. Evaluation done at random point.\n",
            "Time taken: 0.3401\n",
            "Function value obtained: -0.6531\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 9 started. Evaluating function at random point.\n",
            "Iteration No: 9 ended. Evaluation done at random point.\n",
            "Time taken: 0.2810\n",
            "Function value obtained: -0.5959\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 10 started. Evaluating function at random point.\n",
            "Iteration No: 10 ended. Evaluation done at random point.\n",
            "Time taken: 0.3395\n",
            "Function value obtained: -0.6506\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 11 started. Evaluating function at random point.\n",
            "Iteration No: 11 ended. Evaluation done at random point.\n",
            "Time taken: 0.3343\n",
            "Function value obtained: -0.6467\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 12 started. Evaluating function at random point.\n",
            "Iteration No: 12 ended. Evaluation done at random point.\n",
            "Time taken: 0.2831\n",
            "Function value obtained: -0.6185\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 13 started. Evaluating function at random point.\n",
            "Iteration No: 13 ended. Evaluation done at random point.\n",
            "Time taken: 0.3031\n",
            "Function value obtained: -0.6241\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 14 started. Evaluating function at random point.\n",
            "Iteration No: 14 ended. Evaluation done at random point.\n",
            "Time taken: 0.3378\n",
            "Function value obtained: -0.6467\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 15 started. Evaluating function at random point.\n",
            "Iteration No: 15 ended. Evaluation done at random point.\n",
            "Time taken: 0.3199\n",
            "Function value obtained: -0.6449\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 16 started. Evaluating function at random point.\n",
            "Iteration No: 16 ended. Evaluation done at random point.\n",
            "Time taken: 0.2736\n",
            "Function value obtained: -0.5991\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 17 started. Evaluating function at random point.\n",
            "Iteration No: 17 ended. Evaluation done at random point.\n",
            "Time taken: 0.3410\n",
            "Function value obtained: -0.6506\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 18 started. Evaluating function at random point.\n",
            "Iteration No: 18 ended. Evaluation done at random point.\n",
            "Time taken: 0.3357\n",
            "Function value obtained: -0.6467\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 19 started. Evaluating function at random point.\n",
            "Iteration No: 19 ended. Evaluation done at random point.\n",
            "Time taken: 0.2810\n",
            "Function value obtained: -0.5991\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 20 started. Evaluating function at random point.\n",
            "Iteration No: 20 ended. Evaluation done at random point.\n",
            "Time taken: 0.3490\n",
            "Function value obtained: -0.6579\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 21 started. Evaluating function at random point.\n",
            "Iteration No: 21 ended. Evaluation done at random point.\n",
            "Time taken: 0.3014\n",
            "Function value obtained: -0.6145\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 22 started. Evaluating function at random point.\n",
            "Iteration No: 22 ended. Evaluation done at random point.\n",
            "Time taken: 0.3042\n",
            "Function value obtained: -0.6243\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 23 started. Evaluating function at random point.\n",
            "Iteration No: 23 ended. Evaluation done at random point.\n",
            "Time taken: 0.3026\n",
            "Function value obtained: -0.6326\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 24 started. Evaluating function at random point.\n",
            "Iteration No: 24 ended. Evaluation done at random point.\n",
            "Time taken: 0.3128\n",
            "Function value obtained: -0.6185\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 25 started. Evaluating function at random point.\n",
            "Iteration No: 25 ended. Evaluation done at random point.\n",
            "Time taken: 0.2773\n",
            "Function value obtained: -0.5991\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 26 started. Evaluating function at random point.\n",
            "Iteration No: 26 ended. Evaluation done at random point.\n",
            "Time taken: 0.3479\n",
            "Function value obtained: -0.6531\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 27 started. Evaluating function at random point.\n",
            "Iteration No: 27 ended. Evaluation done at random point.\n",
            "Time taken: 0.3128\n",
            "Function value obtained: -0.6326\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 28 started. Evaluating function at random point.\n",
            "Iteration No: 28 ended. Evaluation done at random point.\n",
            "Time taken: 0.3425\n",
            "Function value obtained: -0.6579\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 29 started. Evaluating function at random point.\n",
            "Iteration No: 29 ended. Evaluation done at random point.\n",
            "Time taken: 0.2741\n",
            "Function value obtained: -0.5991\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 30 started. Evaluating function at random point.\n",
            "Iteration No: 30 ended. Evaluation done at random point.\n",
            "Time taken: 0.3393\n",
            "Function value obtained: -0.6502\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 31 started. Evaluating function at random point.\n",
            "Iteration No: 31 ended. Evaluation done at random point.\n",
            "Time taken: 0.3363\n",
            "Function value obtained: -0.6531\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 32 started. Evaluating function at random point.\n",
            "Iteration No: 32 ended. Evaluation done at random point.\n",
            "Time taken: 0.2851\n",
            "Function value obtained: -0.6185\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 33 started. Evaluating function at random point.\n",
            "Iteration No: 33 ended. Evaluation done at random point.\n",
            "Time taken: 0.3334\n",
            "Function value obtained: -0.6548\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 34 started. Evaluating function at random point.\n",
            "Iteration No: 34 ended. Evaluation done at random point.\n",
            "Time taken: 0.2787\n",
            "Function value obtained: -0.5959\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 35 started. Evaluating function at random point.\n",
            "Iteration No: 35 ended. Evaluation done at random point.\n",
            "Time taken: 0.3160\n",
            "Function value obtained: -0.6243\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 36 started. Evaluating function at random point.\n",
            "Iteration No: 36 ended. Evaluation done at random point.\n",
            "Time taken: 0.3429\n",
            "Function value obtained: -0.6579\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 37 started. Evaluating function at random point.\n",
            "Iteration No: 37 ended. Evaluation done at random point.\n",
            "Time taken: 0.3425\n",
            "Function value obtained: -0.6591\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 38 started. Evaluating function at random point.\n",
            "Iteration No: 38 ended. Evaluation done at random point.\n",
            "Time taken: 0.3312\n",
            "Function value obtained: -0.6467\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 39 started. Evaluating function at random point.\n",
            "Iteration No: 39 ended. Evaluation done at random point.\n",
            "Time taken: 0.4529\n",
            "Function value obtained: -0.6506\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 40 started. Evaluating function at random point.\n",
            "Iteration No: 40 ended. Evaluation done at random point.\n",
            "Time taken: 0.5176\n",
            "Function value obtained: -0.6502\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 41 started. Evaluating function at random point.\n",
            "Iteration No: 41 ended. Evaluation done at random point.\n",
            "Time taken: 0.4869\n",
            "Function value obtained: -0.6326\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 42 started. Evaluating function at random point.\n",
            "Iteration No: 42 ended. Evaluation done at random point.\n",
            "Time taken: 0.5384\n",
            "Function value obtained: -0.6548\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 43 started. Evaluating function at random point.\n",
            "Iteration No: 43 ended. Evaluation done at random point.\n",
            "Time taken: 0.4062\n",
            "Function value obtained: -0.5473\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 44 started. Evaluating function at random point.\n",
            "Iteration No: 44 ended. Evaluation done at random point.\n",
            "Time taken: 0.4933\n",
            "Function value obtained: -0.6241\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 45 started. Evaluating function at random point.\n",
            "Iteration No: 45 ended. Evaluation done at random point.\n",
            "Time taken: 0.5486\n",
            "Function value obtained: -0.6579\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 46 started. Evaluating function at random point.\n",
            "Iteration No: 46 ended. Evaluation done at random point.\n",
            "Time taken: 0.4652\n",
            "Function value obtained: -0.5991\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 47 started. Evaluating function at random point.\n",
            "Iteration No: 47 ended. Evaluation done at random point.\n",
            "Time taken: 0.5326\n",
            "Function value obtained: -0.6548\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 48 started. Evaluating function at random point.\n",
            "Iteration No: 48 ended. Evaluation done at random point.\n",
            "Time taken: 0.4572\n",
            "Function value obtained: -0.5959\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 49 started. Evaluating function at random point.\n",
            "Iteration No: 49 ended. Evaluation done at random point.\n",
            "Time taken: 0.2932\n",
            "Function value obtained: -0.6145\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 50 started. Evaluating function at random point.\n",
            "Iteration No: 50 ended. Evaluation done at random point.\n",
            "Time taken: 0.3553\n",
            "Function value obtained: -0.6591\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 51 started. Evaluating function at random point.\n",
            "Iteration No: 51 ended. Evaluation done at random point.\n",
            "Time taken: 0.2798\n",
            "Function value obtained: -0.5959\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 52 started. Evaluating function at random point.\n",
            "Iteration No: 52 ended. Evaluation done at random point.\n",
            "Time taken: 0.3539\n",
            "Function value obtained: -0.6579\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 53 started. Evaluating function at random point.\n",
            "Iteration No: 53 ended. Evaluation done at random point.\n",
            "Time taken: 0.3230\n",
            "Function value obtained: -0.6502\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 54 started. Evaluating function at random point.\n",
            "Iteration No: 54 ended. Evaluation done at random point.\n",
            "Time taken: 0.3224\n",
            "Function value obtained: -0.6396\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 55 started. Evaluating function at random point.\n",
            "Iteration No: 55 ended. Evaluation done at random point.\n",
            "Time taken: 0.3434\n",
            "Function value obtained: -0.6591\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 56 started. Evaluating function at random point.\n",
            "Iteration No: 56 ended. Evaluation done at random point.\n",
            "Time taken: 0.3373\n",
            "Function value obtained: -0.6548\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 57 started. Evaluating function at random point.\n",
            "Iteration No: 57 ended. Evaluation done at random point.\n",
            "Time taken: 0.3457\n",
            "Function value obtained: -0.6506\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 58 started. Evaluating function at random point.\n",
            "Iteration No: 58 ended. Evaluation done at random point.\n",
            "Time taken: 0.2791\n",
            "Function value obtained: -0.6185\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 59 started. Evaluating function at random point.\n",
            "Iteration No: 59 ended. Evaluation done at random point.\n",
            "Time taken: 0.3411\n",
            "Function value obtained: -0.6506\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 60 started. Evaluating function at random point.\n",
            "Iteration No: 60 ended. Evaluation done at random point.\n",
            "Time taken: 0.2815\n",
            "Function value obtained: -0.5959\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 61 started. Evaluating function at random point.\n",
            "Iteration No: 61 ended. Evaluation done at random point.\n",
            "Time taken: 0.3063\n",
            "Function value obtained: -0.6326\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 62 started. Evaluating function at random point.\n",
            "Iteration No: 62 ended. Evaluation done at random point.\n",
            "Time taken: 0.2748\n",
            "Function value obtained: -0.5959\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 63 started. Evaluating function at random point.\n",
            "Iteration No: 63 ended. Evaluation done at random point.\n",
            "Time taken: 0.3223\n",
            "Function value obtained: -0.6449\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 64 started. Evaluating function at random point.\n",
            "Iteration No: 64 ended. Evaluation done at random point.\n",
            "Time taken: 0.3521\n",
            "Function value obtained: -0.6591\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 65 started. Evaluating function at random point.\n",
            "Iteration No: 65 ended. Evaluation done at random point.\n",
            "Time taken: 0.3007\n",
            "Function value obtained: -0.6326\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 66 started. Evaluating function at random point.\n",
            "Iteration No: 66 ended. Evaluation done at random point.\n",
            "Time taken: 0.3273\n",
            "Function value obtained: -0.6396\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 67 started. Evaluating function at random point.\n",
            "Iteration No: 67 ended. Evaluation done at random point.\n",
            "Time taken: 0.3431\n",
            "Function value obtained: -0.6579\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 68 started. Evaluating function at random point.\n",
            "Iteration No: 68 ended. Evaluation done at random point.\n",
            "Time taken: 0.3205\n",
            "Function value obtained: -0.6502\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 69 started. Evaluating function at random point.\n",
            "Iteration No: 69 ended. Evaluation done at random point.\n",
            "Time taken: 0.2581\n",
            "Function value obtained: -0.5473\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 70 started. Evaluating function at random point.\n",
            "Iteration No: 70 ended. Evaluation done at random point.\n",
            "Time taken: 0.3088\n",
            "Function value obtained: -0.6145\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 71 started. Evaluating function at random point.\n",
            "Iteration No: 71 ended. Evaluation done at random point.\n",
            "Time taken: 0.3064\n",
            "Function value obtained: -0.6326\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 72 started. Evaluating function at random point.\n",
            "Iteration No: 72 ended. Evaluation done at random point.\n",
            "Time taken: 0.2711\n",
            "Function value obtained: -0.5991\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 73 started. Evaluating function at random point.\n",
            "Iteration No: 73 ended. Evaluation done at random point.\n",
            "Time taken: 0.3244\n",
            "Function value obtained: -0.6396\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 74 started. Evaluating function at random point.\n",
            "Iteration No: 74 ended. Evaluation done at random point.\n",
            "Time taken: 0.3347\n",
            "Function value obtained: -0.6548\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 75 started. Evaluating function at random point.\n",
            "Iteration No: 75 ended. Evaluation done at random point.\n",
            "Time taken: 0.3300\n",
            "Function value obtained: -0.6506\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 76 started. Evaluating function at random point.\n",
            "Iteration No: 76 ended. Evaluation done at random point.\n",
            "Time taken: 0.2806\n",
            "Function value obtained: -0.5959\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 77 started. Evaluating function at random point.\n",
            "Iteration No: 77 ended. Evaluation done at random point.\n",
            "Time taken: 0.2832\n",
            "Function value obtained: -0.5991\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 78 started. Evaluating function at random point.\n",
            "Iteration No: 78 ended. Evaluation done at random point.\n",
            "Time taken: 0.3117\n",
            "Function value obtained: -0.6449\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 79 started. Evaluating function at random point.\n",
            "Iteration No: 79 ended. Evaluation done at random point.\n",
            "Time taken: 0.3646\n",
            "Function value obtained: -0.6630\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 80 started. Evaluating function at random point.\n",
            "Iteration No: 80 ended. Evaluation done at random point.\n",
            "Time taken: 0.3669\n",
            "Function value obtained: -0.6145\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 81 started. Evaluating function at random point.\n",
            "Iteration No: 81 ended. Evaluation done at random point.\n",
            "Time taken: 0.4553\n",
            "Function value obtained: -0.6185\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 82 started. Evaluating function at random point.\n",
            "Iteration No: 82 ended. Evaluation done at random point.\n",
            "Time taken: 0.4438\n",
            "Function value obtained: -0.5959\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 83 started. Evaluating function at random point.\n",
            "Iteration No: 83 ended. Evaluation done at random point.\n",
            "Time taken: 0.5103\n",
            "Function value obtained: -0.6548\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 84 started. Evaluating function at random point.\n",
            "Iteration No: 84 ended. Evaluation done at random point.\n",
            "Time taken: 0.4556\n",
            "Function value obtained: -0.6185\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 85 started. Evaluating function at random point.\n",
            "Iteration No: 85 ended. Evaluation done at random point.\n",
            "Time taken: 0.4887\n",
            "Function value obtained: -0.6449\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 86 started. Evaluating function at random point.\n",
            "Iteration No: 86 ended. Evaluation done at random point.\n",
            "Time taken: 0.4469\n",
            "Function value obtained: -0.5991\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 87 started. Evaluating function at random point.\n",
            "Iteration No: 87 ended. Evaluation done at random point.\n",
            "Time taken: 0.5350\n",
            "Function value obtained: -0.6579\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 88 started. Evaluating function at random point.\n",
            "Iteration No: 88 ended. Evaluation done at random point.\n",
            "Time taken: 0.4982\n",
            "Function value obtained: -0.6243\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 89 started. Evaluating function at random point.\n",
            "Iteration No: 89 ended. Evaluation done at random point.\n",
            "Time taken: 0.4618\n",
            "Function value obtained: -0.6185\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 90 started. Evaluating function at random point.\n",
            "Iteration No: 90 ended. Evaluation done at random point.\n",
            "Time taken: 0.4655\n",
            "Function value obtained: -0.6579\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 91 started. Evaluating function at random point.\n",
            "Iteration No: 91 ended. Evaluation done at random point.\n",
            "Time taken: 0.3318\n",
            "Function value obtained: -0.6502\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 92 started. Evaluating function at random point.\n",
            "Iteration No: 92 ended. Evaluation done at random point.\n",
            "Time taken: 0.2960\n",
            "Function value obtained: -0.6145\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 93 started. Evaluating function at random point.\n",
            "Iteration No: 93 ended. Evaluation done at random point.\n",
            "Time taken: 0.3442\n",
            "Function value obtained: -0.6579\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 94 started. Evaluating function at random point.\n",
            "Iteration No: 94 ended. Evaluation done at random point.\n",
            "Time taken: 0.3092\n",
            "Function value obtained: -0.6145\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 95 started. Evaluating function at random point.\n",
            "Iteration No: 95 ended. Evaluation done at random point.\n",
            "Time taken: 0.2927\n",
            "Function value obtained: -0.6185\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 96 started. Evaluating function at random point.\n",
            "Iteration No: 96 ended. Evaluation done at random point.\n",
            "Time taken: 0.3294\n",
            "Function value obtained: -0.6467\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 97 started. Evaluating function at random point.\n",
            "Iteration No: 97 ended. Evaluation done at random point.\n",
            "Time taken: 0.3313\n",
            "Function value obtained: -0.6449\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 98 started. Evaluating function at random point.\n",
            "Iteration No: 98 ended. Evaluation done at random point.\n",
            "Time taken: 0.3199\n",
            "Function value obtained: -0.6396\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 99 started. Evaluating function at random point.\n",
            "Iteration No: 99 ended. Evaluation done at random point.\n",
            "Time taken: 0.3423\n",
            "Function value obtained: -0.6579\n",
            "Current minimum: -0.6630\n",
            "Iteration No: 100 started. Evaluating function at random point.\n",
            "Iteration No: 100 ended. Evaluation done at random point.\n",
            "Time taken: 1.0555\n",
            "Function value obtained: -0.5959\n",
            "Current minimum: -0.6630\n",
            "{'n_neighbors': 19, 'weights': 'distance'}\n",
            "Best ROC-AUC Score: 0.6629647074416811\n"
          ]
        }
      ],
      "source": [
        "X = train_df.drop(['TenYearCHD'], axis = 1)\n",
        "y = train_df['TenYearCHD']\n",
        "X = X.reset_index(drop=True)\n",
        "\n",
        "def optimize(params, X, y):\n",
        "  n_neighbors = int(params[0])\n",
        "  weights = params[1]\n",
        "  # metric = str(params[2])\n",
        "  model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n",
        "  cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
        "  scores = []\n",
        "  for train_idx, val_idx in cv.split(X, y):\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict_proba(X_val)[:, 1]\n",
        "    score = roc_auc_score(y_val, y_pred)\n",
        "    scores.append(score)\n",
        "  return -1.0 * np.mean(scores)\n",
        "\n",
        "# Define the parameter space and names\n",
        "\n",
        "param_space = [\n",
        "    Categorical(categories=[1, 3, 5, 7, 9, 11, 13, 15, 17, 19], name='n_neighbors'),\n",
        "    Categorical(categories=['uniform', 'distance'], name='weights')\n",
        "    # Categorical(categories=['euclidean', 'manhattan', 'minkowski'], name='metric')\n",
        "]\n",
        "\n",
        "param_names = [\"n_neighbors\", \"weights\", \"metric\"]\n",
        "\n",
        "# Define the optimization function with partial arguments\n",
        "optimization_function = partial(optimize, X=X, y=y)\n",
        "\n",
        "# Run the optimization using gp_minimize\n",
        "result = gp_minimize(\n",
        "    optimization_function,\n",
        "    dimensions=param_space,\n",
        "    n_calls=100,\n",
        "    n_random_starts=100,\n",
        "    verbose=10,\n",
        "    acq_func='EI'\n",
        ")\n",
        "# Print the best hyperparameters found\n",
        "print(dict(zip(param_names, result.x)))\n",
        "print(f'Best ROC-AUC Score: {-1.0 * result.fun}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAE_Kz2pV7wc",
        "outputId": "8543894b-7698-484e-a3e1-9ac4b2858b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.6672839086708344\n"
          ]
        }
      ],
      "source": [
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Assuming you have your training and evaluation data as X_train, y_train, X_eval, y_eval, and test data as X_test, y_test\n",
        "\n",
        "# Define the parameters\n",
        "params = {'n_neighbors': 19, 'weights': 'distance'}\n",
        "\n",
        "# Train the model\n",
        "model = KNeighborsClassifier(**params)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHx69zVRW828"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5PwXHSuWrto",
        "outputId": "92375faa-8955-49fd-bd94-d471da2744f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration No: 1 started. Evaluating function at random point.\n",
            "Iteration No: 1 ended. Evaluation done at random point.\n",
            "Time taken: 6.9058\n",
            "Function value obtained: -0.7085\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 2 started. Evaluating function at random point.\n",
            "Iteration No: 2 ended. Evaluation done at random point.\n",
            "Time taken: 3.5121\n",
            "Function value obtained: -0.6866\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 3 started. Evaluating function at random point.\n",
            "Iteration No: 3 ended. Evaluation done at random point.\n",
            "Time taken: 3.4531\n",
            "Function value obtained: -0.7068\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 4 started. Evaluating function at random point.\n",
            "Iteration No: 4 ended. Evaluation done at random point.\n",
            "Time taken: 7.8704\n",
            "Function value obtained: -0.6922\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 5 started. Evaluating function at random point.\n",
            "Iteration No: 5 ended. Evaluation done at random point.\n",
            "Time taken: 4.4191\n",
            "Function value obtained: -0.6895\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 6 started. Evaluating function at random point.\n",
            "Iteration No: 6 ended. Evaluation done at random point.\n",
            "Time taken: 5.7076\n",
            "Function value obtained: -0.6816\n",
            "Current minimum: -0.7085\n",
            "Iteration No: 7 started. Evaluating function at random point.\n",
            "Iteration No: 7 ended. Evaluation done at random point.\n",
            "Time taken: 4.5471\n",
            "Function value obtained: -0.7087\n",
            "Current minimum: -0.7087\n",
            "Iteration No: 8 started. Evaluating function at random point.\n",
            "Iteration No: 8 ended. Evaluation done at random point.\n",
            "Time taken: 3.5338\n",
            "Function value obtained: -0.6856\n",
            "Current minimum: -0.7087\n",
            "Iteration No: 9 started. Evaluating function at random point.\n",
            "Iteration No: 9 ended. Evaluation done at random point.\n",
            "Time taken: 3.5560\n",
            "Function value obtained: -0.7082\n",
            "Current minimum: -0.7087\n",
            "Iteration No: 10 started. Evaluating function at random point.\n",
            "Iteration No: 10 ended. Evaluation done at random point.\n",
            "Time taken: 5.2468\n",
            "Function value obtained: -0.7106\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 11 started. Evaluating function at random point.\n",
            "Iteration No: 11 ended. Evaluation done at random point.\n",
            "Time taken: 2.5600\n",
            "Function value obtained: -0.7064\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 12 started. Evaluating function at random point.\n",
            "Iteration No: 12 ended. Evaluation done at random point.\n",
            "Time taken: 2.6690\n",
            "Function value obtained: -0.7087\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 13 started. Evaluating function at random point.\n",
            "Iteration No: 13 ended. Evaluation done at random point.\n",
            "Time taken: 1.3359\n",
            "Function value obtained: -0.6864\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 14 started. Evaluating function at random point.\n",
            "Iteration No: 14 ended. Evaluation done at random point.\n",
            "Time taken: 6.6672\n",
            "Function value obtained: -0.7105\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 15 started. Evaluating function at random point.\n",
            "Iteration No: 15 ended. Evaluation done at random point.\n",
            "Time taken: 4.4252\n",
            "Function value obtained: -0.6827\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 16 started. Evaluating function at random point.\n",
            "Iteration No: 16 ended. Evaluation done at random point.\n",
            "Time taken: 3.4486\n",
            "Function value obtained: -0.6953\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 17 started. Evaluating function at random point.\n",
            "Iteration No: 17 ended. Evaluation done at random point.\n",
            "Time taken: 3.0712\n",
            "Function value obtained: -0.6775\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 18 started. Evaluating function at random point.\n",
            "Iteration No: 18 ended. Evaluation done at random point.\n",
            "Time taken: 4.8494\n",
            "Function value obtained: -0.6866\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 19 started. Evaluating function at random point.\n",
            "Iteration No: 19 ended. Evaluation done at random point.\n",
            "Time taken: 5.9369\n",
            "Function value obtained: -0.6904\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 20 started. Evaluating function at random point.\n",
            "Iteration No: 20 ended. Evaluation done at random point.\n",
            "Time taken: 2.1760\n",
            "Function value obtained: -0.6728\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 21 started. Evaluating function at random point.\n",
            "Iteration No: 21 ended. Evaluation done at random point.\n",
            "Time taken: 7.1386\n",
            "Function value obtained: -0.6900\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 22 started. Evaluating function at random point.\n",
            "Iteration No: 22 ended. Evaluation done at random point.\n",
            "Time taken: 1.1012\n",
            "Function value obtained: -0.7006\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 23 started. Evaluating function at random point.\n",
            "Iteration No: 23 ended. Evaluation done at random point.\n",
            "Time taken: 4.4936\n",
            "Function value obtained: -0.7017\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 24 started. Evaluating function at random point.\n",
            "Iteration No: 24 ended. Evaluation done at random point.\n",
            "Time taken: 7.4389\n",
            "Function value obtained: -0.6901\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 25 started. Evaluating function at random point.\n",
            "Iteration No: 25 ended. Evaluation done at random point.\n",
            "Time taken: 6.0148\n",
            "Function value obtained: -0.6890\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 26 started. Evaluating function at random point.\n",
            "Iteration No: 26 ended. Evaluation done at random point.\n",
            "Time taken: 7.2478\n",
            "Function value obtained: -0.6931\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 27 started. Evaluating function at random point.\n",
            "Iteration No: 27 ended. Evaluation done at random point.\n",
            "Time taken: 1.5746\n",
            "Function value obtained: -0.6707\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 28 started. Evaluating function at random point.\n",
            "Iteration No: 28 ended. Evaluation done at random point.\n",
            "Time taken: 1.1830\n",
            "Function value obtained: -0.6982\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 29 started. Evaluating function at random point.\n",
            "Iteration No: 29 ended. Evaluation done at random point.\n",
            "Time taken: 2.6294\n",
            "Function value obtained: -0.6799\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 30 started. Evaluating function at random point.\n",
            "Iteration No: 30 ended. Evaluation done at random point.\n",
            "Time taken: 1.1124\n",
            "Function value obtained: -0.6706\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 31 started. Evaluating function at random point.\n",
            "Iteration No: 31 ended. Evaluation done at random point.\n",
            "Time taken: 5.4945\n",
            "Function value obtained: -0.6832\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 32 started. Evaluating function at random point.\n",
            "Iteration No: 32 ended. Evaluation done at random point.\n",
            "Time taken: 2.7467\n",
            "Function value obtained: -0.6905\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 33 started. Evaluating function at random point.\n",
            "Iteration No: 33 ended. Evaluation done at random point.\n",
            "Time taken: 1.9883\n",
            "Function value obtained: -0.7057\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 34 started. Evaluating function at random point.\n",
            "Iteration No: 34 ended. Evaluation done at random point.\n",
            "Time taken: 1.5891\n",
            "Function value obtained: -0.6811\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 35 started. Evaluating function at random point.\n",
            "Iteration No: 35 ended. Evaluation done at random point.\n",
            "Time taken: 2.9110\n",
            "Function value obtained: -0.6873\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 36 started. Evaluating function at random point.\n",
            "Iteration No: 36 ended. Evaluation done at random point.\n",
            "Time taken: 4.4186\n",
            "Function value obtained: -0.7007\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 37 started. Evaluating function at random point.\n",
            "Iteration No: 37 ended. Evaluation done at random point.\n",
            "Time taken: 5.4850\n",
            "Function value obtained: -0.7068\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 38 started. Evaluating function at random point.\n",
            "Iteration No: 38 ended. Evaluation done at random point.\n",
            "Time taken: 5.7566\n",
            "Function value obtained: -0.6998\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 39 started. Evaluating function at random point.\n",
            "Iteration No: 39 ended. Evaluation done at random point.\n",
            "Time taken: 4.2132\n",
            "Function value obtained: -0.6844\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 40 started. Evaluating function at random point.\n",
            "Iteration No: 40 ended. Evaluation done at random point.\n",
            "Time taken: 7.1761\n",
            "Function value obtained: -0.6981\n",
            "Current minimum: -0.7106\n",
            "Iteration No: 41 started. Evaluating function at random point.\n",
            "Iteration No: 41 ended. Evaluation done at random point.\n",
            "Time taken: 3.4112\n",
            "Function value obtained: -0.7111\n",
            "Current minimum: -0.7111\n",
            "Iteration No: 42 started. Evaluating function at random point.\n",
            "Iteration No: 42 ended. Evaluation done at random point.\n",
            "Time taken: 2.8354\n",
            "Function value obtained: -0.7115\n",
            "Current minimum: -0.7115\n",
            "Iteration No: 43 started. Evaluating function at random point.\n",
            "Iteration No: 43 ended. Evaluation done at random point.\n",
            "Time taken: 2.5425\n",
            "Function value obtained: -0.6793\n",
            "Current minimum: -0.7115\n",
            "Iteration No: 44 started. Evaluating function at random point.\n",
            "Iteration No: 44 ended. Evaluation done at random point.\n",
            "Time taken: 3.6312\n",
            "Function value obtained: -0.6775\n",
            "Current minimum: -0.7115\n",
            "Iteration No: 45 started. Evaluating function at random point.\n",
            "Iteration No: 45 ended. Evaluation done at random point.\n",
            "Time taken: 3.0531\n",
            "Function value obtained: -0.7104\n",
            "Current minimum: -0.7115\n",
            "Iteration No: 46 started. Evaluating function at random point.\n",
            "Iteration No: 46 ended. Evaluation done at random point.\n",
            "Time taken: 4.9256\n",
            "Function value obtained: -0.6921\n",
            "Current minimum: -0.7115\n",
            "Iteration No: 47 started. Evaluating function at random point.\n",
            "Iteration No: 47 ended. Evaluation done at random point.\n",
            "Time taken: 1.0989\n",
            "Function value obtained: -0.7121\n",
            "Current minimum: -0.7121\n",
            "Iteration No: 48 started. Evaluating function at random point.\n",
            "Iteration No: 48 ended. Evaluation done at random point.\n",
            "Time taken: 5.9305\n",
            "Function value obtained: -0.6933\n",
            "Current minimum: -0.7121\n",
            "Iteration No: 49 started. Evaluating function at random point.\n",
            "Iteration No: 49 ended. Evaluation done at random point.\n",
            "Time taken: 3.2845\n",
            "Function value obtained: -0.6855\n",
            "Current minimum: -0.7121\n",
            "Iteration No: 50 started. Evaluating function at random point.\n",
            "Iteration No: 50 ended. Evaluation done at random point.\n",
            "Time taken: 4.8689\n",
            "Function value obtained: -0.6812\n",
            "Current minimum: -0.7121\n",
            "{'n_estimators': 20, 'max_depth': 7, 'criterion': 'gini'}\n",
            "Best ROC-AUC Score: 0.7121269901516613\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X = train_df.drop(['TenYearCHD'], axis = 1)\n",
        "y = train_df['TenYearCHD']\n",
        "X = X.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def optimize(params, X, y):\n",
        "  params = dict(zip(param_names, params))\n",
        "  model = RandomForestClassifier(**params)\n",
        "  cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
        "  scores = []\n",
        "  for train_idx, val_idx in cv.split(X, y):\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict_proba(X_val)[:, 1]\n",
        "    score = roc_auc_score(y_val, y_pred)\n",
        "    scores.append(score)\n",
        "  return -1.0 * np.mean(scores)\n",
        "\n",
        "param_space = [\n",
        "    Integer(10, 100, name='n_estimators'),\n",
        "    Integer(2, 20, name='max_depth'),\n",
        "    Categorical(categories=['gini', 'entropy'], name='criterion')\n",
        "]\n",
        "\n",
        "param_names = [\"n_estimators\", \"max_depth\", \"criterion\"]\n",
        "\n",
        "optimization_function = partial(optimize, X=X, y=y)\n",
        "\n",
        "result = gp_minimize(\n",
        "    optimization_function,\n",
        "    dimensions=param_space,\n",
        "    n_calls=50,\n",
        "    n_random_starts=50,\n",
        "    verbose=10,\n",
        "    acq_func='EI'\n",
        ")\n",
        "\n",
        "print(dict(zip(param_names, result.x)))\n",
        "print(f'Best ROC-AUC Score: {-1.0 * result.fun}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU7Gu63hXADw",
        "outputId": "7ec2b473-4ce6-41cb-cf68-1aa436568379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.6872961402555042\n"
          ]
        }
      ],
      "source": [
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Assuming you have your training and evaluation data as X_train, y_train, X_eval, y_eval, and test data as X_test, y_test\n",
        "\n",
        "# Define the parameters\n",
        "params = {'n_estimators': 20, 'max_depth': 7, 'criterion': 'gini'}\n",
        "\n",
        "# Train the model\n",
        "model = RandomForestClassifier(**params)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y30bzeN3dajs"
      },
      "source": [
        "### Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL81HdyldWLe",
        "outputId": "303e30f4-df29-4c78-fe4e-cb8108613fef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration No: 1 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 8ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "Iteration No: 1 ended. Evaluation done at random point.\n",
            "Time taken: 32.3382\n",
            "Function value obtained: -0.7247\n",
            "Current minimum: -0.7247\n",
            "Iteration No: 2 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 2 ended. Evaluation done at random point.\n",
            "Time taken: 24.9058\n",
            "Function value obtained: -0.7472\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 3 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "Iteration No: 3 ended. Evaluation done at random point.\n",
            "Time taken: 26.8740\n",
            "Function value obtained: -0.6969\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 4 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 4 ended. Evaluation done at random point.\n",
            "Time taken: 25.1126\n",
            "Function value obtained: -0.7364\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 5 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 5 ended. Evaluation done at random point.\n",
            "Time taken: 26.2762\n",
            "Function value obtained: -0.7391\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 6 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "Iteration No: 6 ended. Evaluation done at random point.\n",
            "Time taken: 22.4602\n",
            "Function value obtained: -0.7415\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 7 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 7 ended. Evaluation done at random point.\n",
            "Time taken: 27.0875\n",
            "Function value obtained: -0.6768\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 8 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 8 ended. Evaluation done at random point.\n",
            "Time taken: 26.8777\n",
            "Function value obtained: -0.7468\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 9 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 9 ended. Evaluation done at random point.\n",
            "Time taken: 26.5794\n",
            "Function value obtained: -0.6684\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 10 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 10 ended. Evaluation done at random point.\n",
            "Time taken: 26.1411\n",
            "Function value obtained: -0.7253\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 11 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "Iteration No: 11 ended. Evaluation done at random point.\n",
            "Time taken: 25.4364\n",
            "Function value obtained: -0.6735\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 12 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 12 ended. Evaluation done at random point.\n",
            "Time taken: 25.0982\n",
            "Function value obtained: -0.7332\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 13 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "Iteration No: 13 ended. Evaluation done at random point.\n",
            "Time taken: 25.6735\n",
            "Function value obtained: -0.7389\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 14 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 14 ended. Evaluation done at random point.\n",
            "Time taken: 25.8523\n",
            "Function value obtained: -0.7194\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 15 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 15 ended. Evaluation done at random point.\n",
            "Time taken: 25.2573\n",
            "Function value obtained: -0.7293\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 16 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 16 ended. Evaluation done at random point.\n",
            "Time taken: 22.0206\n",
            "Function value obtained: -0.7470\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 17 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "Iteration No: 17 ended. Evaluation done at random point.\n",
            "Time taken: 25.1124\n",
            "Function value obtained: -0.7251\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 18 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 18 ended. Evaluation done at random point.\n",
            "Time taken: 25.0154\n",
            "Function value obtained: -0.7400\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 19 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 19 ended. Evaluation done at random point.\n",
            "Time taken: 24.4376\n",
            "Function value obtained: -0.6956\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 20 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "Iteration No: 20 ended. Evaluation done at random point.\n",
            "Time taken: 25.4918\n",
            "Function value obtained: -0.7437\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 21 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 21 ended. Evaluation done at random point.\n",
            "Time taken: 27.5356\n",
            "Function value obtained: -0.7298\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 22 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 22 ended. Evaluation done at random point.\n",
            "Time taken: 27.9979\n",
            "Function value obtained: -0.7364\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 23 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "Iteration No: 23 ended. Evaluation done at random point.\n",
            "Time taken: 26.6590\n",
            "Function value obtained: -0.7319\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 24 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 24 ended. Evaluation done at random point.\n",
            "Time taken: 24.5654\n",
            "Function value obtained: -0.7247\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 25 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 25 ended. Evaluation done at random point.\n",
            "Time taken: 25.4404\n",
            "Function value obtained: -0.6760\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 26 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 26 ended. Evaluation done at random point.\n",
            "Time taken: 26.4148\n",
            "Function value obtained: -0.7469\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 27 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 27 ended. Evaluation done at random point.\n",
            "Time taken: 26.2728\n",
            "Function value obtained: -0.7229\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 28 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "Iteration No: 28 ended. Evaluation done at random point.\n",
            "Time taken: 23.4424\n",
            "Function value obtained: -0.7396\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 29 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 29 ended. Evaluation done at random point.\n",
            "Time taken: 26.5986\n",
            "Function value obtained: -0.7464\n",
            "Current minimum: -0.7472\n",
            "Iteration No: 30 started. Evaluating function at random point.\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 3ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Iteration No: 30 ended. Evaluation done at random point.\n",
            "Time taken: 26.6978\n",
            "Function value obtained: -0.7284\n",
            "Current minimum: -0.7472\n",
            "{'hidden_units': 236, 'learning_rate': 0.006173770394704579}\n",
            "Best ROC-AUC Score: 0.7471855239059185\n"
          ]
        }
      ],
      "source": [
        "X = train_df.drop(['TenYearCHD'], axis = 1)\n",
        "y = train_df['TenYearCHD']\n",
        "X = X.reset_index(drop=True)\n",
        "y = y.reset_index(drop=True)\n",
        "\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "from skopt import gp_minimize\n",
        "from skopt import space\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the neural network model\n",
        "def create_model(hidden_units, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=hidden_units, activation='relu', input_dim=X.shape[1]))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "    return model\n",
        "\n",
        "# Define the optimization function\n",
        "def optimize_nn(params):\n",
        "    hidden_units = params[0]\n",
        "    learning_rate = params[1]\n",
        "    \n",
        "    model = create_model(hidden_units=hidden_units, learning_rate=learning_rate)\n",
        "    \n",
        "    scores = []\n",
        "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
        "    \n",
        "    for train_idx, val_idx in cv.split(X, y):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        \n",
        "        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "        \n",
        "        y_pred = model.predict(X_val)[:, 0]\n",
        "        score = roc_auc_score(y_val, y_pred)\n",
        "        scores.append(score)\n",
        "    \n",
        "    return -np.mean(scores)\n",
        "\n",
        "# Define the parameter space and names\n",
        "param_space = [\n",
        "    space.Integer(10, 300, name='hidden_units'),\n",
        "    space.Real(0.0001, 0.1, prior='log-uniform', name='learning_rate')\n",
        "]\n",
        "param_names = ['hidden_units', 'learning_rate']\n",
        "\n",
        "# Define the optimization function with partial arguments\n",
        "optimization_function = partial(optimize_nn)\n",
        "\n",
        "# Run the optimization using gp_minimize\n",
        "result = gp_minimize(\n",
        "    optimization_function,\n",
        "    dimensions=param_space,\n",
        "    n_calls=30,\n",
        "    n_random_starts=30,\n",
        "    verbose=1,\n",
        "    acq_func='EI',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "best_params = dict(zip(param_names, result.x))\n",
        "print(best_params)\n",
        "print(f'Best ROC-AUC Score: {-result.fun}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Oj2IcBBdh0e",
        "outputId": "1cb51fcc-3a96-49d0-c4b9-28236e242e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "84/84 [==============================] - 1s 3ms/step - loss: 0.4385\n",
            "Epoch 2/10\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.3969\n",
            "Epoch 3/10\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.3881\n",
            "Epoch 4/10\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.3852\n",
            "Epoch 5/10\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.3842\n",
            "Epoch 6/10\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.3841\n",
            "Epoch 7/10\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.3808\n",
            "Epoch 8/10\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.3855\n",
            "Epoch 9/10\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.3813\n",
            "Epoch 10/10\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.3806\n",
            "21/21 [==============================] - 0s 3ms/step\n",
            "ROC-AUC Score: 0.7033840717586302\n"
          ]
        }
      ],
      "source": [
        "X_test = test_df.drop(['TenYearCHD'], axis = 1)\n",
        "y_test = test_df['TenYearCHD']\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "optimized_model = Sequential()\n",
        "optimized_model.add(Dense(units=236, activation='relu', input_dim=X.shape[1]))\n",
        "optimized_model.add(Dense(units=1, activation='sigmoid'))\n",
        "optimizer = Adam(learning_rate=0.006173770394704579)\n",
        "optimized_model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "\n",
        "\n",
        "# Train the model on the training data\n",
        "optimized_model.fit(X, y, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Make predictions on X_test\n",
        "y_test_pred_probs = optimized_model.predict(X_test)[:, 0]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_test_pred_probs)\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITLy9l7umK-g"
      },
      "source": [
        "### Soft Voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEsqnfxUj8vG",
        "outputId": "9c26eff9-5485-49d0-ff1a-50d636bce629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 1 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4501\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4338\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4534\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4429\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4430\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4553\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4531\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4386\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4596\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4498\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4497\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4479\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4531\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4553\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4417\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 1 ended. Evaluation done at random point.\n",
            "Time taken: 38.7013\n",
            "Function value obtained: -0.6887\n",
            "Current minimum: -0.6887\n",
            "Iteration No: 2 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4544\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4514\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4659\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4470\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4411\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4621\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4473\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4603\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4455\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4547\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4608\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4562\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4609\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4536\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4550\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 2 ended. Evaluation done at random point.\n",
            "Time taken: 29.7364\n",
            "Function value obtained: -0.6934\n",
            "Current minimum: -0.6934\n",
            "Iteration No: 3 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4610\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4486\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4508\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4569\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4501\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4476\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4629\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4540\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4637\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4448\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4570\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4401\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 3s 6ms/step - loss: 0.4476\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4585\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4509\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Iteration No: 3 ended. Evaluation done at random point.\n",
            "Time taken: 37.7741\n",
            "Function value obtained: -0.6894\n",
            "Current minimum: -0.6934\n",
            "Iteration No: 4 started. Evaluating function at random point.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4529\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4512\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4491\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4516\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4488\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4529\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4438\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4512\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4510\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4455\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4638\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4403\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4503\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4546\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4664\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 4 ended. Evaluation done at random point.\n",
            "Time taken: 34.6543\n",
            "Function value obtained: -0.6946\n",
            "Current minimum: -0.6946\n",
            "Iteration No: 5 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4454\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4556\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4437\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4427\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4490\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4556\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4561\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4499\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4450\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4618\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4575\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4433\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4571\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4656\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4511\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 5 ended. Evaluation done at random point.\n",
            "Time taken: 29.5936\n",
            "Function value obtained: -0.6711\n",
            "Current minimum: -0.6946\n",
            "Iteration No: 6 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4569\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4632\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4513\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4474\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4399\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4485\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4443\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4462\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4444\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4585\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4581\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4430\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 4ms/step - loss: 0.4654\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4651\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4503\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 6 ended. Evaluation done at random point.\n",
            "Time taken: 40.1621\n",
            "Function value obtained: -0.6926\n",
            "Current minimum: -0.6946\n",
            "Iteration No: 7 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4475\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4536\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4537\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4520\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4461\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4475\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4573\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4587\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4524\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4490\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4566\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4468\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4594\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4564\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4608\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 7 ended. Evaluation done at random point.\n",
            "Time taken: 30.9212\n",
            "Function value obtained: -0.6899\n",
            "Current minimum: -0.6946\n",
            "Iteration No: 8 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4479\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4497\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4488\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4532\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4622\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4587\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4638\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4474\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4454\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4500\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4534\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4449\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 2s 2ms/step - loss: 0.4636\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4547\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4503\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 8 ended. Evaluation done at random point.\n",
            "Time taken: 35.3473\n",
            "Function value obtained: -0.6977\n",
            "Current minimum: -0.6977\n",
            "Iteration No: 9 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4531\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4508\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4510\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4546\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4518\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4642\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4439\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4452\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4409\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4497\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4508\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4478\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 4ms/step - loss: 0.4541\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4544\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4608\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 9 ended. Evaluation done at random point.\n",
            "Time taken: 31.7006\n",
            "Function value obtained: -0.7024\n",
            "Current minimum: -0.7024\n",
            "Iteration No: 10 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4555\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 3ms/step - loss: 0.4510\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4591\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4441\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4476\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4525\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4600\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4543\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4664\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4582\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4523\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4591\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4427\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4543\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4543\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 10 ended. Evaluation done at random point.\n",
            "Time taken: 30.7707\n",
            "Function value obtained: -0.7008\n",
            "Current minimum: -0.7024\n",
            "Iteration No: 11 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4513\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4443\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4516\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4716\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4527\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4588\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4546\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4515\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 4ms/step - loss: 0.4590\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4553\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4484\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4612\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4534\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4479\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4507\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 11 ended. Evaluation done at random point.\n",
            "Time taken: 35.8815\n",
            "Function value obtained: -0.6922\n",
            "Current minimum: -0.7024\n",
            "Iteration No: 12 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4544\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4518\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4493\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4513\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4494\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4579\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4654\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4516\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4409\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4531\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4622\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4587\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4525\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4467\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4482\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 12 ended. Evaluation done at random point.\n",
            "Time taken: 31.0615\n",
            "Function value obtained: -0.6779\n",
            "Current minimum: -0.7024\n",
            "Iteration No: 13 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4653\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4579\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4491\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4585\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4576\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4521\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4492\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4574\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4500\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4521\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4546\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4566\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4568\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 2s 2ms/step - loss: 0.4644\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4527\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 13 ended. Evaluation done at random point.\n",
            "Time taken: 31.6668\n",
            "Function value obtained: -0.6910\n",
            "Current minimum: -0.7024\n",
            "Iteration No: 14 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4471\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4576\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4617\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4552\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4507\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4510\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4521\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4636\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4572\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4644\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4553\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4538\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4619\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4477\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4362\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 14 ended. Evaluation done at random point.\n",
            "Time taken: 38.7855\n",
            "Function value obtained: -0.6905\n",
            "Current minimum: -0.7024\n",
            "Iteration No: 15 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4359\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4520\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4590\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4484\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 4ms/step - loss: 0.4357\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4625\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4498\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4549\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4548\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4690\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4564\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4713\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4519\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4586\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4539\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 15 ended. Evaluation done at random point.\n",
            "Time taken: 30.3669\n",
            "Function value obtained: -0.6960\n",
            "Current minimum: -0.7024\n",
            "Iteration No: 16 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4469\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4536\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4571\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4513\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4477\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4462\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4581\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4563\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4470\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4540\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4576\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4562\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4456\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4492\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4596\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 16 ended. Evaluation done at random point.\n",
            "Time taken: 37.1108\n",
            "Function value obtained: -0.6911\n",
            "Current minimum: -0.7024\n",
            "Iteration No: 17 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4509\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4552\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4465\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4588\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4554\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4399\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4511\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4781\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4567\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4497\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4598\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4536\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4537\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4574\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4541\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 17 ended. Evaluation done at random point.\n",
            "Time taken: 30.6381\n",
            "Function value obtained: -0.6927\n",
            "Current minimum: -0.7024\n",
            "Iteration No: 18 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4478\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4442\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4507\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4561\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4589\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4484\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4622\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4521\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4513\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4647\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4719\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4516\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4559\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 2s 2ms/step - loss: 0.4562\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4449\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No: 18 ended. Evaluation done at random point.\n",
            "Time taken: 31.8767\n",
            "Function value obtained: -0.6893\n",
            "Current minimum: -0.7024\n",
            "Iteration No: 19 started. Evaluating function at random point.\n",
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4552\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4421\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4522\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4528\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4519\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4551\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4471\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4455\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4514\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4408\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4514\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4555\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4538\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4496\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4485\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Iteration No: 19 ended. Evaluation done at random point.\n",
            "Time taken: 34.4665\n",
            "Function value obtained: -0.6785\n",
            "Current minimum: -0.7024\n",
            "Iteration No: 20 started. Evaluating function at random point.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4458\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4557\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4527\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4520\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 2ms/step - loss: 0.4469\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4446\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4615\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4558\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4524\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4606\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4556\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4455\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4505\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 0s 2ms/step - loss: 0.4624\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "54/54 [==============================] - 1s 3ms/step - loss: 0.4555\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Iteration No: 20 ended. Evaluation done at random point.\n",
            "Time taken: 31.7983\n",
            "Function value obtained: -0.6731\n",
            "Current minimum: -0.7024\n",
            "Optimal weights: [0.7031865096223286, 0.45725464901581847, 0.014723448242867202, 0.27636134692557307, 0.8056712170488267]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from functools import partial\n",
        "from skopt import gp_minimize\n",
        "from skopt import space\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.classifier import EnsembleVoteClassifier\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "# Define the wrapper class for Keras models\n",
        "class KerasWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(X, y)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        predictions = self.model.predict(X)\n",
        "        proba = predictions.flatten()\n",
        "        proba = np.vstack([1 - proba, proba]).T\n",
        "        return proba\n",
        "\n",
        "# Define the base models\n",
        "neural_network = Sequential()\n",
        "neural_network.add(Dense(units=236, activation='relu', input_dim=X_train.shape[1]))\n",
        "neural_network.add(Dense(units=1, activation='sigmoid'))\n",
        "optimizer = Adam(learning_rate=0.006173770394704579)\n",
        "neural_network.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "\n",
        "neural_network_model = KerasWrapper(neural_network)\n",
        "\n",
        "xgboost_model = XGBClassifier(max_depth=4, n_estimators=507,\n",
        "                              learning_rate=0.08488585554676588, subsample=0.7428342774870389,\n",
        "                              colsample_bytree=0.8704321827011343, min_child_weight=7.467421949775829)\n",
        "\n",
        "lightgbm_model = LGBMClassifier(max_depth=3, n_estimators=173,\n",
        "                                learning_rate=0.26658391864937014,\n",
        "                                subsample=0.27417395433428393,\n",
        "                                colsample_bytree=0.37678704205444047,\n",
        "                                min_child_weight=8.827872937189104)\n",
        "\n",
        "random_forest_model = RandomForestClassifier(n_estimators=20, max_depth=7, criterion='gini')\n",
        "lr_model = LogisticRegression(C = 1.0, penalty = 'l2', solver = 'saga')\n",
        "##############################################################################################################\n",
        "from mlxtend.classifier import EnsembleVoteClassifier\n",
        "import numpy as np\n",
        "# Split the data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the voting classifier\n",
        "voting_classifier = EnsembleVoteClassifier(\n",
        "    clfs=[\n",
        "        neural_network_model,\n",
        "        xgboost_model,\n",
        "        lightgbm_model,\n",
        "        random_forest_model,\n",
        "        lr_model\n",
        "    ],\n",
        "    voting='soft',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Define the optimization function\n",
        "def optimize_weights(weights, X, y):\n",
        "    voting_classifier.weights = weights\n",
        "\n",
        "    scores = []\n",
        "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
        "\n",
        "    for train_idx, val_idx in cv.split(X, y):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        voting_classifier.fit(X_train, y_train)\n",
        "        y_pred = voting_classifier.predict_proba(X_val)[:, 1]  # Use predicted probabilities for AUC\n",
        "        score = roc_auc_score(y_val, y_pred)\n",
        "        scores.append(score)\n",
        "\n",
        "    return -np.mean(scores)\n",
        "\n",
        "# Example usage\n",
        "num_classifiers = 5\n",
        "weights = np.full(num_classifiers, 0.5)  # Initial weights\n",
        "\n",
        "# Define the parameter space for weights\n",
        "param_space = [space.Real(0, 1, name='weight_{}'.format(i)) for i in range(num_classifiers)]\n",
        "\n",
        "# Define the optimization function with partial arguments\n",
        "optimization_function = partial(optimize_weights, X=X_train, y=y_train)\n",
        "\n",
        "# Run the optimization using gp_minimize\n",
        "result = gp_minimize(\n",
        "    optimization_function,\n",
        "    dimensions=param_space,\n",
        "    n_calls=20,\n",
        "    n_random_starts=20,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Get the optimal weights\n",
        "optimal_weights = result.x\n",
        "\n",
        "print(\"Optimal weights:\", optimal_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewLUSseyoqs7",
        "outputId": "548d6417-465b-460f-8f8b-69d8382ccf9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 0s 2ms/step\n",
            "Optimized weights: [0.7031865096223286, 0.45725464901581847, 0.014723448242867202, 0.27636134692557307, 0.8056712170488267]\n",
            "Test ROC-AUC score: 0.6808915466159282\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Set the optimal weights in the voting classifier\n",
        "voting_classifier.weights = optimal_weights\n",
        "\n",
        "# Evaluate the performance on the test set\n",
        "y_test_pred_probs = voting_classifier.predict_proba(X_test)[:, 1]\n",
        "test_score = roc_auc_score(y_test, y_test_pred_probs)\n",
        "\n",
        "print(\"Optimized weights:\", optimal_weights)\n",
        "\n",
        "print(\"Test ROC-AUC score:\", test_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCJIehehotzx",
        "outputId": "535536fe-5ca2-41f5-f8a8-147acef9084e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.257197170855414"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "np.sum([0.7031865096223286, 0.45725464901581847, 0.014723448242867202, 0.27636134692557307, 0.8056712170488267])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUzlKiuIo-el",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44ff279-faf5-4fcc-a40f-eef850eb171d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7031865096223286,\n",
              " 0.45725464901581847,\n",
              " 0.014723448242867202,\n",
              " 0.27636134692557307,\n",
              " 0.8056712170488267]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "optimal_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the optimal weights in the voting classifier\n",
        "voting_classifier.weights = [0.9031865096223286, 0.45725464901581847, 0.014723448242867202, 0.27636134692557307, 0.8056712170488267]\n",
        "\n",
        "# Evaluate the performance on the test set\n",
        "y_test_pred_probs = voting_classifier.predict_proba(X_test)[:, 1]\n",
        "test_score = roc_auc_score(y_test, y_test_pred_probs)\n",
        "\n",
        "print(\"Optimized weights:\", optimal_weights)\n",
        "\n",
        "print(\"Test ROC-AUC score:\", test_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Azh2ohPb5mMi",
        "outputId": "3ca3abfd-28ef-4425-a35e-c91b2abbe487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 0s 4ms/step\n",
            "Optimized weights: [0.7031865096223286, 0.45725464901581847, 0.014723448242867202, 0.27636134692557307, 0.8056712170488267]\n",
            "Test ROC-AUC score: 0.6757440880674096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacking"
      ],
      "metadata": {
        "id": "ctimrYbv7RwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.classifier import StackingClassifier\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from mlxtend.classifier import StackingCVClassifier\n",
        "\n",
        "X = train_df.drop(['TenYearCHD'], axis = 1)\n",
        "y = train_df['TenYearCHD']\n",
        "X = X.reset_index(drop=True)\n",
        "y = y.reset_index(drop=True)\n",
        "\n",
        "# Define the wrapper class for Keras models\n",
        "class KerasWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(X, y)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        predictions = self.model.predict(X)\n",
        "        proba = predictions.flatten()\n",
        "        proba = np.vstack([1 - proba, proba]).T\n",
        "        return proba\n",
        "\n",
        "# Define the base models\n",
        "neural_network = Sequential()\n",
        "neural_network.add(Dense(units=236, activation='relu', input_dim=X_train.shape[1]))\n",
        "neural_network.add(Dense(units=1, activation='sigmoid'))\n",
        "optimizer = Adam(learning_rate=0.006173770394704579)\n",
        "neural_network.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "\n",
        "neural_network_model = KerasWrapper(neural_network)\n",
        "\n",
        "xgboost_model = XGBClassifier(max_depth=4, n_estimators=507,\n",
        "                              learning_rate=0.08488585554676588, subsample=0.7428342774870389,\n",
        "                              colsample_bytree=0.8704321827011343, min_child_weight=7.467421949775829)\n",
        "\n",
        "lightgbm_model = LGBMClassifier(max_depth=3, n_estimators=173,\n",
        "                                learning_rate=0.26658391864937014,\n",
        "                                subsample=0.27417395433428393,\n",
        "                                colsample_bytree=0.37678704205444047,\n",
        "                                min_child_weight=8.827872937189104)\n",
        "\n",
        "random_forest_model = RandomForestClassifier(n_estimators=20, max_depth=7, criterion='gini')\n",
        "lr_model = LogisticRegression(C = 1.0, penalty = 'l2', solver = 'saga')\n",
        "\n",
        "# Define meta model\n",
        "meta_model = XGBClassifier(max_depth= 3, n_estimators= 100, learning_rate= 0.06, subsample= 0.7, colsample_bytree= 0.4, reg_alpha=0.3, reg_lambda=0.3,min_child_weight= 10)\n",
        "\n",
        "# Define stratified k-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define stacking classifier with base models and meta model\n",
        "stacking_clf_xgb4 = StackingCVClassifier(classifiers=[neural_network_model, xgboost_model, lightgbm_model, random_forest_model, lr_model],\n",
        "                                    meta_classifier=meta_model,\n",
        "                                    cv=5,\n",
        "                                    stratify=True,\n",
        "                                    shuffle=True,\n",
        "                                    use_probas=True,\n",
        "                                    use_features_in_secondary=True,\n",
        "                                    verbose=2)\n",
        "\n",
        "# Define metric as ROC\n",
        "metric = roc_auc_score\n",
        "\n",
        "# Fit the stacking classifier\n",
        "stacking_clf_xgb4.fit(X.values, y.values)\n",
        "\n",
        "# Calculate the score\n",
        "score = metric(y, stacking_clf_xgb4.predict_proba(X)[:, 1])\n",
        "\n",
        "\n",
        "# Print the score\n",
        "print(f\"ROC AUC score: {score}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwFM0qTo5sY7",
        "outputId": "83814bef-7ce3-4350-9a49-7f00fcdb1693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting classifier1: keraswrapper (1/5)\n",
            "KerasWrapper(model=<keras.engine.sequential.Sequential object at 0x7fe9f4c48f10>)\n",
            "Training and fitting fold 1 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.4452\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 2 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3993\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "Training and fitting fold 3 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3883\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "Training and fitting fold 4 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3801\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Training and fitting fold 5 of 5...\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.3883\n",
            "17/17 [==============================] - 0s 1ms/step\n",
            "Fitting classifier2: xgbclassifier (2/5)\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.8704321827011343, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.08488585554676588,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
            "              min_child_weight=7.467421949775829, missing=nan,\n",
            "              monotone_constraints=None, n_estimators=507, n_jobs=None,\n",
            "              num_parallel_tree=None, objective='binary:logistic',\n",
            "              predictor=None, ...)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier3: lgbmclassifier (3/5)\n",
            "LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
            "               colsample_bytree=0.37678704205444047, importance_type='split',\n",
            "               learning_rate=0.26658391864937014, max_depth=3,\n",
            "               min_child_samples=20, min_child_weight=8.827872937189104,\n",
            "               min_split_gain=0.0, n_estimators=173, n_jobs=-1, num_leaves=31,\n",
            "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
            "               silent='warn', subsample=0.27417395433428393,\n",
            "               subsample_for_bin=200000, subsample_freq=0)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier4: randomforestclassifier (4/5)\n",
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=7, max_features='sqrt',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_samples_leaf=1,\n",
            "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                       n_estimators=20, n_jobs=None, oob_score=False,\n",
            "                       random_state=None, verbose=0, warm_start=False)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "Fitting classifier5: logisticregression (5/5)\n",
            "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Training and fitting fold 1 of 5...\n",
            "Training and fitting fold 2 of 5...\n",
            "Training and fitting fold 3 of 5...\n",
            "Training and fitting fold 4 of 5...\n",
            "Training and fitting fold 5 of 5...\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.3859\n",
            "84/84 [==============================] - 0s 1ms/step\n",
            "ROC AUC score: 0.782939367311072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the performance on the test set\n",
        "y_test_pred_probs = stacking_clf_xgb4.predict_proba(X_test)[:, 1]\n",
        "test_score = roc_auc_score(y_test, y_test_pred_probs)\n",
        "\n",
        "\n",
        "print(\"Test ROC-AUC score:\", test_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNF9RuIq8Kxh",
        "outputId": "05998693-b0cc-44af-f7f9-0f0cb59222a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 0s 2ms/step\n",
            "Test ROC-AUC score: 0.7047431367219353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Old Soft Voting"
      ],
      "metadata": {
        "id": "fgKcghIB9-7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.classifier import EnsembleVoteClassifier\n",
        "\n",
        "\n",
        "# Define the wrapper class for Keras models\n",
        "class KerasWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(X, y)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        predictions = self.model.predict(X)\n",
        "        proba = predictions.flatten()\n",
        "        proba = np.vstack([1 - proba, proba]).T\n",
        "        return proba\n",
        "\n",
        "# Define the base models\n",
        "neural_network = Sequential()\n",
        "neural_network.add(Dense(units=236, activation='relu', input_dim=X_train.shape[1]))\n",
        "neural_network.add(Dense(units=1, activation='sigmoid'))\n",
        "optimizer = Adam(learning_rate=0.006173770394704579)\n",
        "neural_network.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "\n",
        "neural_network_model = KerasWrapper(neural_network)\n",
        "\n",
        "xgboost_model = XGBClassifier(max_depth=4, n_estimators=507,\n",
        "                              learning_rate=0.08488585554676588, subsample=0.7428342774870389,\n",
        "                              colsample_bytree=0.8704321827011343, min_child_weight=7.467421949775829)\n",
        "\n",
        "lightgbm_model = LGBMClassifier(max_depth=3, n_estimators=173,\n",
        "                                learning_rate=0.26658391864937014,\n",
        "                                subsample=0.27417395433428393,\n",
        "                                colsample_bytree=0.37678704205444047,\n",
        "                                min_child_weight=8.827872937189104)\n",
        "\n",
        "random_forest_model = RandomForestClassifier(n_estimators=20, max_depth=7, criterion='gini')\n",
        "lr_model = LogisticRegression(C = 1.0, penalty = 'l2', solver = 'saga')\n",
        "\n",
        "\n",
        "# # set the weights for each classifier\n",
        "# weights = [0.99, 0.993, 1.004, 1.006, 1.007]\n",
        "weights = [1.5, 1, 1, 1, 1.5]\n",
        "#0.121\n",
        "\n",
        "\n",
        "classifiers = [neural_network_model, xgboost_model, lightgbm_model, random_forest_model, lr_model]\n",
        "\n",
        "# create the ensemble classifier\n",
        "ensemble_SVW_O = EnsembleVoteClassifier(clfs=classifiers, voting='soft', weights = weights, verbose=1)\n",
        "\n",
        "# fit the ensemble classifier on the training data\n",
        "ensemble_SVW_O.fit(X, y)\n",
        "\n",
        "\n",
        "# Calculate the score\n",
        "score = metric(y, stacking_clf_xgb4.predict_proba(X)[:, 1])\n",
        "\n",
        "\n",
        "# Print the score\n",
        "print(f\"ROC AUC score: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJRKWrxD9P-d",
        "outputId": "91ef2d56-c6da-4a3d-8a70-d28a0489a5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 classifiers...\n",
            "Fitting clf1: keraswrapper (1/5)\n",
            "84/84 [==============================] - 1s 4ms/step - loss: 0.4364\n",
            "Fitting clf2: xgbclassifier (2/5)\n",
            "Fitting clf3: lgbmclassifier (3/5)\n",
            "Fitting clf4: randomforestclassifier (4/5)\n",
            "Fitting clf5: logisticregression (5/5)\n",
            "84/84 [==============================] - 0s 1ms/step\n",
            "ROC AUC score: 0.782939367311072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the performance on the test set\n",
        "y_test_pred_probs = ensemble_SVW_O.predict_proba(X_test)[:, 1]\n",
        "test_score = roc_auc_score(y_test, y_test_pred_probs)\n",
        "\n",
        "\n",
        "print(\"Test ROC-AUC score:\", test_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz9uKIce-qar",
        "outputId": "3d3adcbf-612c-4989-a403-40718c0a4108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 0s 1ms/step\n",
            "Test ROC-AUC score: 0.6903880130470237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TbWfDI_a-0kD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}